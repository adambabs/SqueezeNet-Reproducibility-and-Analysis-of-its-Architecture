{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_exps_ablations_squeezeratio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu83Jf5qHY_l",
        "colab_type": "code",
        "outputId": "43832b8f-e686-4f59-adb1-dc15241fe4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import torchvision\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYTXiSckMwDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%cd /content/\n",
        "#!/usr/bin/python\n",
        "# essential imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.ion() \n",
        "import numpy as np\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import pickle\n",
        "#tensorflow 1.15\n",
        "import tensorflow as tf\n",
        "#print(tf.__version__)\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# default seeding for reproducability\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MXcpFQSOI50",
        "colab_type": "code",
        "outputId": "877a149f-2e33-401b-b961-1309cbb5ddb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Import torch Libraries\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, MSELoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "#from torch.autograd import Variable\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "#adam sgd combined optimizer\n",
        "!pip install adabound\n",
        "import adabound\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "\n",
        "#defaults\n",
        "PIXEL_LENGTH_MODIFIED = 128\n",
        "FEATURE_SIZE_MODIFIED = PIXEL_LENGTH_MODIFIED*PIXEL_LENGTH_MODIFIED\n",
        "PIXEL_LENGTH_MNIST = 28\n",
        "FEATURE_SIZE_MNIST = PIXEL_LENGTH_MNIST*PIXEL_LENGTH_MNIST"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/44/0c2c414effb3d9750d780b230dbb67ea48ddc5d9a6d7a9b7e6fcc6bdcff9/adabound-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.17.4)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Tb-VZDOU5Q",
        "colab_type": "text"
      },
      "source": [
        "## helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLScvSjuOWFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "# def normalize_max(images):\n",
        "#   images_normalize=[]\n",
        "#   for i in images:\n",
        "#     i -= i.min()\n",
        "#     denom = i.max()-i.min()\n",
        "#     images_normalize.append(np.divide(i, denom))\n",
        "#   return np.asarray(images_normalize) \n",
        "\n",
        "# def standardize_mean(images):\n",
        "#   images_standardize=[]\n",
        "#   for i in images:\n",
        "#     mean, std = i.mean(), i.std()\n",
        "#     i = (i - mean) / std\n",
        "#     images_standardize.append(i)\n",
        "#   return np.asarray(images_standardize) \n",
        "  \n",
        "# training/testing functions\n",
        "def train(epoch, train_loader, model, error, optimizer, batch_size):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    #loss_total = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        #peak at tensor details\n",
        "        # if batch_idx==1:\n",
        "        #   # visualize one of the images in data set\n",
        "        #   plt.imshow(np.squeeze(images[0].numpy()), cmap='Greys')\n",
        "        #   plt.axis(\"off\")\n",
        "        #   plt.title(str(labels[0].numpy()))\n",
        "        #   #plt.savefig('graph.png')\n",
        "        #   plt.show()\n",
        "        #reshape for training\n",
        "        train = images.view(batch_size,3,32,32).to(device=device, dtype=torch.float)\n",
        "        labels = labels.to(device=device, dtype=torch.long)\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "        # Calculate softmax and cross entropy loss\n",
        "        loss = error(outputs, labels)\n",
        "        #print(loss)\n",
        "        #loss_total += loss.item()\n",
        "\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        #print every 100 batches\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
        "            # plt.imshow(images[0].numpy().reshape(128,128))\n",
        "            # plt.axis('off')\n",
        "            # plt.show()\n",
        "            #Print Loss\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(images), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), running_loss/100))\n",
        "            running_loss = 0.0\n",
        "    #loss_total /= len(train_loader)\n",
        "    #print('Test set: Test loss: {:.4f}'.format(loss_total))\n",
        "    #return last loss\n",
        "    return loss.item()\n",
        "\n",
        "def test(test_loader, model, error, batch_size):\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Calculate Accuracy         \n",
        "        correct = 0\n",
        "        loss_test = 0.0\n",
        "        # Iterate through test dataset\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            # if batch_idx==1:\n",
        "            #   # visualize one of the images in data set\n",
        "            #   plt.imshow(np.squeeze(images[0].numpy()), cmap='Greys')\n",
        "            #   plt.axis(\"off\")\n",
        "            #   plt.title(str(labels[0].numpy()))\n",
        "            #   #plt.savefig('graph.png')\n",
        "            #   plt.show()\n",
        "\n",
        "            test = images.view(batch_size,3,32,32).to(device=device, dtype=torch.float)\n",
        "            labels = labels.to(device=device, dtype=torch.long)\n",
        "            # Forward propagation\n",
        "            outputs = model(test)\n",
        "            \n",
        "\n",
        "            # sum up batch loss\n",
        "            loss_test += error(outputs, labels).item()\n",
        "            # get the index of the max log-probability\n",
        "            predicted = outputs.max(1, keepdim=True)[1]\n",
        "            # if batch_idx==1:\n",
        "            #   print(predicted[0].item())\n",
        "            correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "        loss_test /= len(test_loader)\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "        print('\\nTest set: Test loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "              .format(loss_test, correct, len(test_loader.dataset), accuracy))\n",
        "        #return loss and accuracy\n",
        "        return loss_test, accuracy\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK6XR7HuS4xp",
        "colab_type": "code",
        "outputId": "bfe7a4c1-c310-44fb-ad37-52d256932e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# batch_size = 50\n",
        "# workers = os.cpu_count()\n",
        "\n",
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# lengths = [60000*0.8, 60000*0.1, 60000*0.1]\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "# train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=workers)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=workers)\n",
        "\n",
        "# #check for gpu/cpu\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 50\n",
        "workers = os.cpu_count()\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "lengths = [5000, 5000]\n",
        "\n",
        "fullset_train = torchvision.datasets.CIFAR10(root='./data', download=True, train=True,\n",
        "                                       transform=transform)\n",
        "\n",
        "fullset_test = torchvision.datasets.CIFAR10(root='./data', download=True, train=False,\n",
        "                                       transform=transform)\n",
        "\n",
        "testset, finaltestset = torch.utils.data.random_split(fullset_test, lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(fullset_train, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "final_test_loader = torch.utils.data.DataLoader(finaltestset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "#check for gpu/cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 26929928.55it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXhP2GvLy-pw",
        "colab_type": "code",
        "outputId": "cb796762-3caa-4446-ae9a-1987b7527c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "fullset_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7NZIrYFL1GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# # load mnist dataset\n",
        "# (x_train_stack, y_train_stack), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# # split train into train-val set\n",
        "# #x_train_stack_normalized = standardize_mean(x_train_stack)\n",
        "# features_train, features_test, targets_train, targets_test = train_test_split(x_train_stack,\n",
        "#                                                       y_train_stack,\n",
        "#                                                       test_size = 5000,\n",
        "#                                                       random_state = 42) \n",
        "\n",
        "\n",
        "# print(features_train.shape)\n",
        "# print(features_test.shape)\n",
        "# print(targets_train.shape)\n",
        "# print(targets_test.shape)\n",
        "# # visualize one of the images in data set\n",
        "# plt.imshow(x_train_stack[10], cmap='Greys')\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(str(y_train_stack[10]))\n",
        "# #plt.savefig('graph.png')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4FXgWmMgqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # set batch_size, epoch and iteration\n",
        "# batch_size = 50\n",
        "# workers = os.cpu_count()\n",
        "# # n_iters = 10000\n",
        "# # num_epochs = n_iters / (len(features_train) / batch_size)\n",
        "# # num_epochs = int(num_epochs)\n",
        "\n",
        "# # create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
        "# featuresTrain = torch.from_numpy(features_train).type(torch.LongTensor)\n",
        "# targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# # create feature and targets tensor for test set.\n",
        "# featuresTest = torch.from_numpy(features_test).type(torch.LongTensor)\n",
        "# targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# # Pytorch train and test sets\n",
        "# train_data = torch.utils.data.TensorDataset(featuresTrain,targetsTrain, transform=transform)\n",
        "# test_data = torch.utils.data.TensorDataset(featuresTest,targetsTest, transform=transform)\n",
        "\n",
        "# # prepare data loaders (combine dataset and sampler)\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=False, num_workers=workers)\n",
        "\n",
        "# #check for gpu/cpu\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uUybJbjN6W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I38j-LzMN8_g",
        "colab_type": "text"
      },
      "source": [
        "## this guy alex..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoGYjfQ6N90P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['AlexNet', 'alexnet']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(pretrained=False, **kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hhLKkJrN-e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "model = alexnet(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.1\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_KS7Rk0OAJP",
        "colab_type": "code",
        "outputId": "05d55df5-b9da-4e88-c319-0f3b438c799a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/alexnet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/alexnet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.293136\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.014728\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.889440\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.773362\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.721718\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.664595\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.612807\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.580618\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.559560\n",
            "\n",
            "Test set: Test loss: 1.4471, Accuracy: 2265/5000 (45%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 45.3%\n",
            "Better loss at Epoch 0: loss = 1.44712819814682%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.484247\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.440978\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.393117\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.423753\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.393297\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.343575\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.323909\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.290417\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.287544\n",
            "\n",
            "Test set: Test loss: 1.1781, Accuracy: 2875/5000 (58%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 57.5%\n",
            "Better loss at Epoch 1: loss = 1.1780840069055558%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.183384\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.172043\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.147634\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.132466\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.145960\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.098322\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.136004\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.097184\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.087452\n",
            "\n",
            "Test set: Test loss: 1.0184, Accuracy: 3200/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 64.0%\n",
            "Better loss at Epoch 2: loss = 1.0184080082178115%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 0.993341\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.019708\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 0.985655\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.008987\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.975413\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 0.954575\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 0.950305\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 0.964914\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.950027\n",
            "\n",
            "Test set: Test loss: 1.1101, Accuracy: 3068/5000 (61%)\n",
            "\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.865279\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.875965\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.850768\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.838983\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.845044\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.840495\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.848480\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.862641\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.849395\n",
            "\n",
            "Test set: Test loss: 0.9009, Accuracy: 3470/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 69.4%\n",
            "Better loss at Epoch 4: loss = 0.9008629962801933%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.711274\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.722851\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.790607\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.756436\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.760998\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.754076\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.732441\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.770684\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.744399\n",
            "\n",
            "Test set: Test loss: 0.8623, Accuracy: 3569/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 71.38%\n",
            "Better loss at Epoch 5: loss = 0.862334297299385%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.658906\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.623183\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.630551\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.656202\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.670642\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.679595\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.685746\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.642508\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.648098\n",
            "\n",
            "Test set: Test loss: 0.8783, Accuracy: 3552/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.524652\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.568918\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.555639\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.559207\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.567143\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.560405\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.578190\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.567862\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.571902\n",
            "\n",
            "Test set: Test loss: 0.8574, Accuracy: 3566/5000 (71%)\n",
            "\n",
            "Better loss at Epoch 7: loss = 0.8573857283592224%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.451270\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.459910\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.444802\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.469724\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.537403\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.501822\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.497845\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.547439\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.480775\n",
            "\n",
            "Test set: Test loss: 0.9427, Accuracy: 3502/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.386764\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.393958\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.402887\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.409379\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.439597\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.431238\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.443910\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.428010\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.456881\n",
            "\n",
            "Test set: Test loss: 0.8554, Accuracy: 3648/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 72.96%\n",
            "Better loss at Epoch 9: loss = 0.8554293078184128%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.298416\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.344596\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.338543\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.370834\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.366562\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.382582\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.369893\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.390420\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.409374\n",
            "\n",
            "Test set: Test loss: 0.9697, Accuracy: 3612/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.256481\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.285680\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.273319\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.294731\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.328346\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.305082\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.315375\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.338854\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.338145\n",
            "\n",
            "Test set: Test loss: 0.9368, Accuracy: 3649/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 72.98%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.224559\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.224654\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.251448\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.260003\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.266709\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.270103\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.289540\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.246632\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.279048\n",
            "\n",
            "Test set: Test loss: 1.0334, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 73.68%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.174363\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.180245\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.229009\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.217307\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.210331\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.248660\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.221617\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.231315\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.250077\n",
            "\n",
            "Test set: Test loss: 1.0241, Accuracy: 3689/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 73.78%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.153772\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.165199\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.183539\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.203842\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.199421\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.206558\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.200211\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.211794\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.229203\n",
            "\n",
            "Test set: Test loss: 1.1096, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.115676\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.142334\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.157605\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.162603\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.159220\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.144466\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.209252\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.187724\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.166162\n",
            "\n",
            "Test set: Test loss: 1.1992, Accuracy: 3665/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.123671\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.109287\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.151969\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.165625\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.143416\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.162722\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.162604\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.147265\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.156242\n",
            "\n",
            "Test set: Test loss: 1.2212, Accuracy: 3636/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.110855\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.109087\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.122635\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.137793\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.107142\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.113866\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.124507\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.125127\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.123406\n",
            "\n",
            "Test set: Test loss: 1.2665, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.072278\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.099537\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.090712\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.101663\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.119118\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.124073\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.127811\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.124681\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.115455\n",
            "\n",
            "Test set: Test loss: 1.2231, Accuracy: 3695/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 73.9%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.066058\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.087999\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.093879\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.072318\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.085181\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.095085\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.116541\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.119393\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.099623\n",
            "\n",
            "Test set: Test loss: 1.2377, Accuracy: 3663/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.058035\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.055320\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.081222\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.079684\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.105010\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.091768\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.087520\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.092301\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.113989\n",
            "\n",
            "Test set: Test loss: 1.4116, Accuracy: 3677/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.073665\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.057535\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.070426\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.061078\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.073943\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.086569\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.095135\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.092263\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.071517\n",
            "\n",
            "Test set: Test loss: 1.3712, Accuracy: 3695/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.037835\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.041157\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.045987\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.067751\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.062075\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.062574\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.075218\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.072903\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.054752\n",
            "\n",
            "Test set: Test loss: 1.5349, Accuracy: 3663/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.070131\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.047217\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.038575\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.070046\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.054258\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.050213\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.075025\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.068185\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.072157\n",
            "\n",
            "Test set: Test loss: 1.5186, Accuracy: 3710/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 23: accuracy = 74.2%\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.065230\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.042527\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.057553\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.050741\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.041472\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.050317\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.079859\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.045884\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.050235\n",
            "\n",
            "Test set: Test loss: 1.4701, Accuracy: 3694/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.040201\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.052655\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.048189\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.056534\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.038015\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.051332\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.061447\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.051849\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.049427\n",
            "\n",
            "Test set: Test loss: 1.7114, Accuracy: 3649/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.035182\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.024492\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.056296\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.075357\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.055855\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.046137\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.059144\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.054614\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.066655\n",
            "\n",
            "Test set: Test loss: 1.5703, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.038593\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.052199\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.031264\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.045684\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.043805\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.033608\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.034305\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.039703\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.029011\n",
            "\n",
            "Test set: Test loss: 1.7138, Accuracy: 3689/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.041806\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.050394\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.038607\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.030450\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.036299\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.055571\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.060859\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.040558\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.034977\n",
            "\n",
            "Test set: Test loss: 1.5406, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.037202\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.019779\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.039141\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.030352\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.034383\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.039980\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.057904\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.053017\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.040544\n",
            "\n",
            "Test set: Test loss: 1.7167, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.024984\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.031576\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.027698\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.030113\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.026741\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.021360\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.030549\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.038196\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.038472\n",
            "\n",
            "Test set: Test loss: 1.7881, Accuracy: 3708/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.024230\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.018755\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.033493\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.028029\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.062848\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.050564\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.043176\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.050782\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.040413\n",
            "\n",
            "Test set: Test loss: 1.8192, Accuracy: 3708/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.036076\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.025790\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.018124\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.025819\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.041656\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.042698\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.031682\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.027409\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.032202\n",
            "\n",
            "Test set: Test loss: 1.7703, Accuracy: 3742/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 32: accuracy = 74.84%\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.026912\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.019695\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.013555\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.012990\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.024813\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.032600\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.036405\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.047131\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.035506\n",
            "\n",
            "Test set: Test loss: 1.7931, Accuracy: 3745/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 33: accuracy = 74.9%\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.024997\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.033388\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.029109\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.036651\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.021533\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.027427\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.022218\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.024750\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.035974\n",
            "\n",
            "Test set: Test loss: 1.7311, Accuracy: 3722/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.015145\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.010016\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.011319\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.021146\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.019687\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.032407\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.017762\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.018608\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.021317\n",
            "\n",
            "Test set: Test loss: 1.9328, Accuracy: 3671/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.017205\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.013736\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.018880\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.016900\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.016805\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.019041\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.027605\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.038731\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.032327\n",
            "\n",
            "Test set: Test loss: 1.7994, Accuracy: 3654/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.032652\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.017983\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.021816\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.022410\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.019442\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.034640\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.033195\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.025736\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.028364\n",
            "\n",
            "Test set: Test loss: 1.8290, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.023210\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.014931\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.009820\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.012391\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.015358\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.025298\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.031619\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.020569\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.017800\n",
            "\n",
            "Test set: Test loss: 1.8475, Accuracy: 3721/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.020902\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.022516\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.027037\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.033483\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.028593\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.026932\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.039605\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.027108\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.019468\n",
            "\n",
            "Test set: Test loss: 1.8548, Accuracy: 3726/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.019583\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.021031\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.024321\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.038354\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.023119\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.019679\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.022673\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.030248\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.033943\n",
            "\n",
            "Test set: Test loss: 1.7719, Accuracy: 3736/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.017130\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.017303\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.018328\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.034757\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.020323\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.012561\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.012614\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.015359\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.024581\n",
            "\n",
            "Test set: Test loss: 1.8315, Accuracy: 3711/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.009700\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.009968\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.010557\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.012740\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.024483\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.013082\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.013058\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.024419\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.027340\n",
            "\n",
            "Test set: Test loss: 1.9962, Accuracy: 3724/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.011033\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.024265\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.020571\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.018980\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.015804\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.012549\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.009831\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.014438\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.023662\n",
            "\n",
            "Test set: Test loss: 1.9749, Accuracy: 3661/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.020782\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.011269\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.009677\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.015993\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.026170\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.017379\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.015692\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.022626\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.025690\n",
            "\n",
            "Test set: Test loss: 1.8632, Accuracy: 3719/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.010109\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.008814\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.013121\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.030209\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.021040\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.015247\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.024188\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.017912\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.012205\n",
            "\n",
            "Test set: Test loss: 1.9594, Accuracy: 3757/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 45: accuracy = 75.14%\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.010250\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.008303\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.009340\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.024520\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.010826\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.013809\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.020384\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.019101\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.031423\n",
            "\n",
            "Test set: Test loss: 1.7592, Accuracy: 3707/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.013961\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.014245\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.012006\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.008031\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.016149\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.015201\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.014061\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.010034\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.013948\n",
            "\n",
            "Test set: Test loss: 2.0277, Accuracy: 3731/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.011119\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.018607\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.013880\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.009636\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.010210\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.023588\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.040084\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.023441\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.018817\n",
            "\n",
            "Test set: Test loss: 1.9075, Accuracy: 3725/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.008786\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.004856\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.007750\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.020532\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.009930\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.020738\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.015966\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.017843\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.014966\n",
            "\n",
            "Test set: Test loss: 1.9474, Accuracy: 3744/5000 (75%)\n",
            "\n",
            "CPU times: user 10min 9s, sys: 2min 32s, total: 12min 42s\n",
            "Wall time: 13min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp5SzXncOO39",
        "colab_type": "code",
        "outputId": "041ec453-3a7a-448b-c0e4-9702493360b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRhumQBhGIZhuPU/f56JyN5X9ZsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 16.60127462081999 seconds\n",
            "Best accuracy: 75.14  Best training loss: 0.00020103454880882055  Best validation loss: 0.8554293078184128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EApzFYGfLjw",
        "colab_type": "code",
        "outputId": "d81dd3df-e18b-4013-bcec-0f3e44b036ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "[1.6835790872573853, 1.175603985786438, 0.9885087609291077, 1.0834705829620361, 0.5205501317977905, 0.6725502014160156, 0.5350584387779236, 0.6497542858123779, 0.6653600335121155, 0.4189870059490204, 0.36256447434425354, 0.25640812516212463, 0.39531826972961426, 0.14752794802188873, 0.103270024061203, 0.210580974817276, 0.17804880440235138, 0.01845608651638031, 0.09959878772497177, 0.039013996720314026, 0.09906764328479767, 0.20886491239070892, 0.030130835250020027, 0.1711345613002777, 0.15228445827960968, 0.03762003779411316, 0.015244321897625923, 0.045352753251791, 0.019627027213573456, 0.21302169561386108, 0.07005283236503601, 0.016617432236671448, 0.07222004234790802, 0.0075284577906131744, 0.05671592801809311, 0.0012827396858483553, 0.017310313880443573, 0.003903617849573493, 0.14320771396160126, 0.00020103454880882055, 0.005680846981704235, 0.06716543436050415, 0.005466203670948744, 0.0903606042265892, 0.00386638636700809, 0.0038905907422304153, 0.002135429298505187, 0.0002275848382851109, 0.057308949530124664, 0.0003503417829051614]\n",
            "[1.44712819814682, 1.1780840069055558, 1.0184080082178115, 1.1100626921653747, 0.9008629962801933, 0.862334297299385, 0.878264063000679, 0.8573857283592224, 0.9426943066716195, 0.8554293078184128, 0.9697082796692849, 0.936833711862564, 1.0334056901931763, 1.0240645429491997, 1.1095976746082306, 1.1992230933904648, 1.2212348321080209, 1.2664743837714196, 1.2230980008840562, 1.2377275025844574, 1.4115734595060347, 1.3711766177415847, 1.534936579465866, 1.5186029469966889, 1.4700755324959756, 1.7114304494857788, 1.570284583568573, 1.713808557987213, 1.5406410855054855, 1.7166801816225052, 1.788138062953949, 1.8191793447732925, 1.7703304362297059, 1.7931327319145203, 1.7310948312282561, 1.9327742475271226, 1.7993641477823257, 1.8290344268083571, 1.8475029343366622, 1.8547963827848435, 1.7719349592924118, 1.8315067499876023, 1.9961856889724732, 1.9748841798305512, 1.863212181329727, 1.95939208984375, 1.759235202074051, 2.027719659805298, 1.907529857158661, 1.9473622387647629]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFC4-mQZMse",
        "colab_type": "text"
      },
      "source": [
        "## squeeze dat net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2C4MQMcQbAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaz9jOeSZOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCt69lxiZXZI",
        "colab_type": "code",
        "outputId": "df416a64-5d4a-477f-cd22-c4d2c6e05bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-23b1f9abbaa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/squeezenet_loss.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/squeezenet_loss.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT6xPT7cZZPY",
        "colab_type": "code",
        "outputId": "b38a7c06-84f9-4b24-fdca-03b4fd62a5b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 250\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.262885\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.009205\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.889318\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.827036\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.795875\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.752193\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.695312\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.671312\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.634310\n",
            "\n",
            "Test set: Test loss: 1.5376, Accuracy: 2271/5000 (45%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 45.42%\n",
            "Better loss at Epoch 0: loss = 1.5375925695896149%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.571716\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.536547\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.520602\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.465033\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.460390\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.442775\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.444041\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.425001\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.406522\n",
            "\n",
            "Test set: Test loss: 1.3009, Accuracy: 2714/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 54.28%\n",
            "Better loss at Epoch 1: loss = 1.3008760565519333%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.338548\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.310083\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.303653\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.291659\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.291660\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.247594\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.274670\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.246250\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.225842\n",
            "\n",
            "Test set: Test loss: 1.2141, Accuracy: 2893/5000 (58%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 57.86%\n",
            "Better loss at Epoch 2: loss = 1.2141004276275635%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.210341\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.179330\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.157165\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.124956\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.154487\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.145200\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.151474\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.117143\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.166774\n",
            "\n",
            "Test set: Test loss: 1.0965, Accuracy: 3093/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 61.86%\n",
            "Better loss at Epoch 3: loss = 1.0964593732357024%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.077548\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.073074\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.058791\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.052285\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.068416\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.053910\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.053458\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.031073\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.044898\n",
            "\n",
            "Test set: Test loss: 1.0320, Accuracy: 3211/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 64.22%\n",
            "Better loss at Epoch 4: loss = 1.0320316684246063%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.984841\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.969402\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.979700\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.986129\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.987574\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.974029\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.964371\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.990432\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.977712\n",
            "\n",
            "Test set: Test loss: 0.9566, Accuracy: 3338/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 66.76%\n",
            "Better loss at Epoch 5: loss = 0.9565583503246308%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.911341\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.931157\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.901124\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.891991\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.934538\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.910594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5f83fe6b3ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 250\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#loss_total = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#peak at tensor details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# if batch_idx==1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject_list\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mare\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreadable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         '''\n\u001b[0;32m--> 903\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_WaitSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVENT_READ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fd_to_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# read-only mapping returned by get_map()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SelectorMapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fileobj_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEpPEK4ZZbPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhTWsbg_ejfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XUt6oJNnkHO",
        "colab_type": "text"
      },
      "source": [
        "### testing adding layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB5dEKcS1AOa",
        "colab_type": "text"
      },
      "source": [
        "#### adding more fire layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogEv-s47np-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "            # self.features = nn.Sequential(\n",
        "            #     nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            #     nn.ReLU(inplace=True),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(96, 16, 64, 64),\n",
        "            #     Fire(128, 16, 64, 64),\n",
        "            #     Fire(128, 32, 128, 128),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(256, 32, 128, 128),\n",
        "            #     Fire(256, 48, 192, 192),\n",
        "            #     Fire(384, 48, 192, 192),\n",
        "            #     Fire(384, 64, 256, 256),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "                # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),t\n",
        "            # )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYCYDYWOgFoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik8CPhgtgJHz",
        "colab_type": "code",
        "outputId": "ca9d0a26-6792-4246-c50b-11cc13bd2276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.311227\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.218915\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.213744\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.190053\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.172718\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.163886\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.099876\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.091815\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 2.058563\n",
            "\n",
            "Test set: Test loss: 2.0117, Accuracy: 1196/5000 (24%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 23.92%\n",
            "Better loss at Epoch 0: loss = 2.011711308956146%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 2.055048\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 2.020186\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 2.015145\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 2.003264\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.990417\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.950319\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.935024\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.947427\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.935252\n",
            "\n",
            "Test set: Test loss: 1.9510, Accuracy: 1442/5000 (29%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 28.84%\n",
            "Better loss at Epoch 1: loss = 1.9510031151771545%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.916074\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.910533\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.874671\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.859949\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.868767\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.877301\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.855109\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.871784\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.853339\n",
            "\n",
            "Test set: Test loss: 1.8027, Accuracy: 1641/5000 (33%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 32.82%\n",
            "Better loss at Epoch 2: loss = 1.8027160727977753%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.836488\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.835665\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.805682\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.829661\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.819615\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.828080\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.775828\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.809370\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.770017\n",
            "\n",
            "Test set: Test loss: 1.7527, Accuracy: 1923/5000 (38%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 38.46%\n",
            "Better loss at Epoch 3: loss = 1.7526912105083465%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.759409\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.738696\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.747590\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.729206\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.760996\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.744524\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.707428\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.743356\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.705991\n",
            "\n",
            "Test set: Test loss: 1.7060, Accuracy: 2033/5000 (41%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 40.66%\n",
            "Better loss at Epoch 4: loss = 1.7060320341587067%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.691423\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 1.684731\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.687953\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.676633\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.684406\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 1.690455\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 1.651903\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.691631\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 1.668236\n",
            "\n",
            "Test set: Test loss: 1.6839, Accuracy: 2014/5000 (40%)\n",
            "\n",
            "Better loss at Epoch 5: loss = 1.6839301323890685%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 1.669302\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 1.647044\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 1.654761\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 1.640196\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 1.653255\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 1.633276\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 1.636781\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 1.626931\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 1.621923\n",
            "\n",
            "Test set: Test loss: 1.5937, Accuracy: 2283/5000 (46%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 45.66%\n",
            "Better loss at Epoch 6: loss = 1.5937085151672363%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 1.644549\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 1.566249\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 1.607117\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 1.582553\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 1.589591\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 1.579697\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 1.583016\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 1.561686\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 1.579626\n",
            "\n",
            "Test set: Test loss: 1.6190, Accuracy: 2236/5000 (45%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 1.594952\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 1.522353\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 1.558674\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 1.547576\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 1.576012\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 1.559524\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 1.558891\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 1.570926\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 1.563949\n",
            "\n",
            "Test set: Test loss: 1.5527, Accuracy: 2377/5000 (48%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 47.54%\n",
            "Better loss at Epoch 8: loss = 1.552677448987961%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 1.531992\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 1.522430\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 1.538192\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 1.542098\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 1.505524\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 1.507937\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 1.545532\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 1.503959\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 1.541457\n",
            "\n",
            "Test set: Test loss: 1.4922, Accuracy: 2485/5000 (50%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 49.7%\n",
            "Better loss at Epoch 9: loss = 1.4921533167362213%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 1.537474\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 1.472516\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 1.476276\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 1.509770\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 1.514928\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 1.494651\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 1.497858\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 1.504872\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 1.481036\n",
            "\n",
            "Test set: Test loss: 1.5182, Accuracy: 2439/5000 (49%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 1.501842\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 1.476661\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 1.460829\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 1.473473\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 1.433158\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 1.465815\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 1.464387\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 1.458229\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 1.467170\n",
            "\n",
            "Test set: Test loss: 1.4895, Accuracy: 2531/5000 (51%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 50.62%\n",
            "Better loss at Epoch 11: loss = 1.4894848263263702%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 1.418749\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 1.436380\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 1.444709\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 1.456886\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 1.441553\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 1.444194\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 1.460927\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 1.486462\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 1.429987\n",
            "\n",
            "Test set: Test loss: 1.4942, Accuracy: 2467/5000 (49%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 1.412650\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 1.402713\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 1.441270\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 1.430536\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 1.424682\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 1.413314\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 1.456319\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 1.428315\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 1.405211\n",
            "\n",
            "Test set: Test loss: 1.5272, Accuracy: 2517/5000 (50%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 1.392726\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 1.380289\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 1.398742\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 1.429095\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 1.390656\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 1.419242\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 1.440421\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 1.413455\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 1.405286\n",
            "\n",
            "Test set: Test loss: 1.4247, Accuracy: 2599/5000 (52%)\n",
            "\n",
            "Better accuracy at Epoch 14: accuracy = 51.98%\n",
            "Better loss at Epoch 14: loss = 1.424692423939705%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 1.367519\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 1.374212\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 1.395336\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 1.383555\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 1.396386\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 1.393799\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 1.362394\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 1.392740\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 1.386203\n",
            "\n",
            "Test set: Test loss: 1.3988, Accuracy: 2711/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 54.22%\n",
            "Better loss at Epoch 15: loss = 1.3988451874256134%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 1.352693\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 1.314752\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 1.275204\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 1.297832\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 1.334520\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 1.276529\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 1.282520\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 1.294184\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 1.285760\n",
            "\n",
            "Test set: Test loss: 1.3070, Accuracy: 2875/5000 (58%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 57.5%\n",
            "Better loss at Epoch 16: loss = 1.307012152671814%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 1.279667\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 1.266384\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 1.263400\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 1.258567\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 1.282098\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 1.269668\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 1.252282\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 1.261480\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 1.241256\n",
            "\n",
            "Test set: Test loss: 1.3204, Accuracy: 2851/5000 (57%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 1.218989\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 1.214346\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 1.236314\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 1.260129\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 1.207666\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 1.231321\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 1.250275\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 1.272565\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 1.250451\n",
            "\n",
            "Test set: Test loss: 1.2668, Accuracy: 2948/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 58.96%\n",
            "Better loss at Epoch 18: loss = 1.2667602264881135%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 1.219151\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 1.210506\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 1.238607\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 1.205369\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 1.232263\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 1.175947\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 1.256594\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 1.210853\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 1.203385\n",
            "\n",
            "Test set: Test loss: 1.2569, Accuracy: 2965/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 19: accuracy = 59.3%\n",
            "Better loss at Epoch 19: loss = 1.2568857431411744%\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 1.178049\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 1.161167\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 1.176494\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 1.210237\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 1.167999\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 1.221644\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 1.239010\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 1.185496\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 1.166120\n",
            "\n",
            "Test set: Test loss: 1.2254, Accuracy: 3044/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 20: accuracy = 60.88%\n",
            "Better loss at Epoch 20: loss = 1.2254088199138642%\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 1.155921\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 1.122395\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 1.194459\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 1.164448\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 1.195413\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 1.176726\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 1.156255\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 1.187687\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 1.187760\n",
            "\n",
            "Test set: Test loss: 1.2809, Accuracy: 2904/5000 (58%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 1.149993\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 1.189318\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 1.136478\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 1.147185\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 1.153860\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 1.156277\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 1.156418\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 1.151169\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 1.154261\n",
            "\n",
            "Test set: Test loss: 1.2299, Accuracy: 3042/5000 (61%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 1.133208\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 1.128016\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 1.160088\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 1.148552\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 1.138534\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 1.189278\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 1.129779\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 1.169081\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 1.148776\n",
            "\n",
            "Test set: Test loss: 1.2546, Accuracy: 2986/5000 (60%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 1.139975\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 1.075573\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 1.135095\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 1.122941\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 1.114842\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 1.146263\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 1.119761\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 1.147218\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 1.135734\n",
            "\n",
            "Test set: Test loss: 1.2365, Accuracy: 3063/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 24: accuracy = 61.26%\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 1.111090\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 1.103892\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 1.074834\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 1.099837\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 1.088342\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 1.143292\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 1.129310\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 1.113095\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 1.127987\n",
            "\n",
            "Test set: Test loss: 1.2224, Accuracy: 3051/5000 (61%)\n",
            "\n",
            "Better loss at Epoch 25: loss = 1.2223666357994079%\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 1.087911\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 1.086090\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 1.112034\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 1.103809\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 1.088943\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 1.117769\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 1.109897\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 1.072734\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 1.075385\n",
            "\n",
            "Test set: Test loss: 1.2324, Accuracy: 2988/5000 (60%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 1.080710\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 1.082303\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 1.087426\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 1.062225\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 1.094753\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 1.129632\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 1.105917\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 1.067050\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 1.113909\n",
            "\n",
            "Test set: Test loss: 1.1764, Accuracy: 3104/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 27: accuracy = 62.08%\n",
            "Better loss at Epoch 27: loss = 1.176350443959236%\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 1.073856\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 1.045576\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 1.100151\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 1.097767\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 1.067574\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 1.073725\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 1.066964\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 1.099652\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 1.031895\n",
            "\n",
            "Test set: Test loss: 1.1061, Accuracy: 3210/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 28: accuracy = 64.2%\n",
            "Better loss at Epoch 28: loss = 1.1061074781417846%\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.932599\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.948599\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.929190\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.941532\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.950722\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.968816\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.991714\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.945679\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.968756\n",
            "\n",
            "Test set: Test loss: 1.1147, Accuracy: 3133/5000 (63%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.987893\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.927532\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.947616\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.899257\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.955448\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.921222\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.920381\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.963125\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.954307\n",
            "\n",
            "Test set: Test loss: 1.0870, Accuracy: 3200/5000 (64%)\n",
            "\n",
            "Better loss at Epoch 30: loss = 1.086987242102623%\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.909699\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.903742\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.911661\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.918203\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.928204\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.942510\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.921856\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.971089\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.921695\n",
            "\n",
            "Test set: Test loss: 1.0328, Accuracy: 3329/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 31: accuracy = 66.58%\n",
            "Better loss at Epoch 31: loss = 1.032847262620926%\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.911861\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.915387\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.927320\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.894187\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.909106\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.935615\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.912662\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.926600\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.932554\n",
            "\n",
            "Test set: Test loss: 1.0391, Accuracy: 3289/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.871241\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.878490\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.879677\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.906929\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.928121\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.924603\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.910941\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.914849\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.902367\n",
            "\n",
            "Test set: Test loss: 1.0440, Accuracy: 3315/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.872735\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.865684\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.880256\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.897726\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.892417\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.902336\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.939102\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.926864\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.932400\n",
            "\n",
            "Test set: Test loss: 1.0447, Accuracy: 3248/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.865065\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.891953\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.873170\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.878516\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.880796\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.923709\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.902208\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.904360\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.902244\n",
            "\n",
            "Test set: Test loss: 1.0120, Accuracy: 3356/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 35: accuracy = 67.12%\n",
            "Better loss at Epoch 35: loss = 1.0119995754957198%\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.838250\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.884863\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.889791\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.863508\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.833058\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.864139\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.854421\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.867089\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.893450\n",
            "\n",
            "Test set: Test loss: 1.0515, Accuracy: 3285/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.848163\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.862380\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.829155\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.861151\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.850731\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.872590\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.916749\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.869956\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.902334\n",
            "\n",
            "Test set: Test loss: 1.0447, Accuracy: 3279/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.830059\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.811451\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.854698\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.886000\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.847099\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.854728\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.837105\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.835075\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.862628\n",
            "\n",
            "Test set: Test loss: 1.0643, Accuracy: 3296/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.847291\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.854879\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.844162\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.871240\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.854110\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.864278\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.826152\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.802847\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.905134\n",
            "\n",
            "Test set: Test loss: 1.0334, Accuracy: 3348/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.845353\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.822257\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.836672\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.836286\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.856469\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.848245\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.870064\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.838964\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.862316\n",
            "\n",
            "Test set: Test loss: 1.0088, Accuracy: 3407/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 40: accuracy = 68.14%\n",
            "Better loss at Epoch 40: loss = 1.0088230687379838%\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.830458\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.797319\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.810537\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.844826\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.864691\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.843898\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.875405\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.852122\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.857958\n",
            "\n",
            "Test set: Test loss: 1.1482, Accuracy: 3243/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.853048\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.821544\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.835299\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.829032\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.836435\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.816118\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.862094\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.862520\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.831966\n",
            "\n",
            "Test set: Test loss: 0.9932, Accuracy: 3429/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 42: accuracy = 68.58%\n",
            "Better loss at Epoch 42: loss = 0.9931567043066025%\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.813242\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.777872\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.829362\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.843550\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.863927\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.845369\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.829495\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.847964\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.867290\n",
            "\n",
            "Test set: Test loss: 1.0184, Accuracy: 3376/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.825093\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.789066\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.821484\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.832074\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.825721\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.827906\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.822893\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.867740\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.847735\n",
            "\n",
            "Test set: Test loss: 0.9700, Accuracy: 3452/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 44: accuracy = 69.04%\n",
            "Better loss at Epoch 44: loss = 0.9700033265352249%\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.831687\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.827835\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.799404\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.808558\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.805255\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.844181\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.825468\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.895716\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.838342\n",
            "\n",
            "Test set: Test loss: 0.9992, Accuracy: 3365/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.828490\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.800404\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.814445\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.813633\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.862889\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.859447\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.865726\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.848004\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.849555\n",
            "\n",
            "Test set: Test loss: 1.0150, Accuracy: 3423/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.838575\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.798771\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.775627\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.830349\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.818434\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.799796\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.859390\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.816416\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.811928\n",
            "\n",
            "Test set: Test loss: 1.0339, Accuracy: 3396/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.821630\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.821410\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.840037\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.818203\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.837688\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.808020\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.796185\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.808595\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.818135\n",
            "\n",
            "Test set: Test loss: 0.9977, Accuracy: 3432/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.770765\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.786135\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.795346\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.788422\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.811377\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.810187\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.843691\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.831156\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.794543\n",
            "\n",
            "Test set: Test loss: 1.0519, Accuracy: 3303/5000 (66%)\n",
            "\n",
            "CPU times: user 19min 11s, sys: 27 s, total: 19min 38s\n",
            "Wall time: 20min 43s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l84zzYKS2bGS",
        "colab_type": "code",
        "outputId": "40c4c232-493f-45cc-c964-fccaed34657d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd5xU1f3/8dcHEAFBqRKUaoldSUBB\nsYFKsGKsKEY0RqJR1FixIypRY0PjN4ktoiJIUJQYLLCCIP4U6YiIIgEVaVKkSN39/P44d2VYZndn\ny+zdnXk/H4957Nw755753GX5zJlzzz3H3B0REcke1eIOQEREKpYSv4hIllHiFxHJMkr8IiJZRolf\nRCTLKPGLiGQZJX6RCmRmE83sV2Wso6WZrTOz6uVZNoW6XjCz+6Lnh5rZR2WtU+KhxC9lYmZHm9lH\nZvajma2MEtvhccdV0cxsnJn9oZgypwNr3X1awr4DzWxk9Ptba2Zjzeyooupx92/cva675xYXV0nK\nloS7zwRWR+ckVYwSv5Same0KvAU8CTQE9gTuATbFGVcldgXwUv6Gme0NTARmAW2APYARwHtmdmSy\nCsysRgXEmarBwB/jDkJKwd310KNUD6A9sLqI16sDDwM/APOBqwAHakSvLwBOTCjfD3g5Ybsj8BGw\nGpgBHJ/w2m7Ac8BiYBFwH1A9em0GsC7h4fnHFlPnOOBeQjJeC7wHNC4uHuB+IBfYGL3f35L8LmoC\nG4DmCfteAkYlKft3YHz0vHUU/2XAN8D4hH35v8c20f61wBjgqfzfY5KyxZ3jv4ElwI9RnQclvPYC\ncF/C9p7ROe0c99+iHiV7qMUvZfElkGtmg8zsZDNrUOD1y4HTgF8RPiTOSbViM9sT+C8hoTcEbgRe\nM7MmUZEXgK3APlH9XYE/ALj7YR66N+oC1wNzgakp1AlwIXApsDshWd9YXDzufjswAbg6et+rk5zS\nvkCeu3+XsO8kQqItaBjQycxqJ+w7DjgA+E2S8q8Ak4BGhA/P3yUpkyjpOUbejmLdHZhKaNUn5e6L\ngC3AfsW8n1QySvxSau6+Bjia0KJ8Blge9Vc3jYqcBzzu7t+6+0rgLyWo/iJCa3iUu+e5+2hgMnBK\nVP8pwHXuvt7dlwGPAT0SKzCzowmJ+owo1kLrTDjsX+7+pbtvICTgtsXFk+L51Ce0sBM1JnxjKWgx\n4f9mw4R9/aJz3VDgHFsChwN3uftmd/8QGFlMLIWdI+7+vLuvdfdNhA+Rw8xstyLqWhudm1QhSvxS\nJu4+x90vcffmwMGEfurHo5f3AL5NKL6wBFW3As41s9X5D8KHTLPotZ2AxQmv/ZPQSgXAzFoQklov\nd/8yhTrzLUl4/hNQtwTHFmUVUK/Avh8KOb4ZkBcdk+/bJOUg/I5XuvtPKZTNl/Qczay6mT1gZl+b\n2RpCVxyED6jC1CN0fUkVUpkuFEkV5+5fmNkLbLvgtxhokVCkZYFD1gN1ErZ/kfD8W+Ald7+84PuY\nWTPCBeTG7r41yeu1gTcI3zbeTqXOFBR3bHHT3M4LodmeURcJhP74c4F/FSh7HvD/3P0nMyuu/sVA\nQzOrk5D8WxRStjgXAt2BEwlJfzfCh48lKxx1f9UkdKVJFaIWv5Same1vZjeYWfNouwVwAfBxVGQY\ncI2ZNY/6//sWqGI60MPMdjKzgtcAXgZON7PfRC3RWmZ2vJk1d/fFhIuSj5jZrmZWzcz2NrPjomOf\nB75w94cKvF+hdaZwusUduxTYq7CD3X0zIdEfl7D7HuAoM7vfzBqaWT0z6wNcDNySQky4+0JCl1M/\nM6sZjQYq7RDLeoQP1BWED+QBxZQ/Dng/6haSKkSJX8piLdAB+MTM1hMS/mfADdHrzwDvEkbATAVe\nL3D8ncDehFblPYSLlAC4+7eE1udtwHJCi/smtv3NXkxobX4eHT+cbd0mPYDfRjcu5T+OSaHOQqVw\n7EDgHDNbZWZPFFLNP0m48OruXxG6iw4jtLAXA2cDv3H3icXFlKAncCQhYd8HvErphtS+SOiOW0T4\nvX5cdHF6Av8oxftIzMxdC7FIxTCz1sD/gJ2SddFkAzObSBj9M63YwqV/j1cJ33juTuN7HAr8092T\n3m8glZsSv1QYJf70iO6UXkn43XYlXN84Mp0fLlK16eKuSNX3C0I3WiPgO+BKJX0pilr8IiJZRhd3\nRUSyTJXo6mncuLG3bt067jBERKqUKVOm/ODuTQrurxKJv3Xr1kyePDnuMEREqhQzS3q3vLp6RESy\nTNoSv5ntZ2bTEx5rzOy66A7F0Wb2VfSz4IyOIiKSRmlL/O4+193buntboB1hMqgRhNv2c9x9XyCH\nHW/jFxGRNKqorp4TgK+jeUW6A4Oi/YOAMysoBhERoeISfw9gSPS8aTTJFoTpYZsmO8DMepvZZDOb\nvHz58oqIUUQkK6Q98ZtZTeAMkqw05OHusaR3kLn70+7e3t3bN2myw2gkEREppYpo8Z8MTHX3pdH2\n0mg+9fx51ZdVQAwiIhKpiMR/Adu6eSAsC9cret4LeLMCYhARKbnFi+H55yE3N+5IylVab+Ays10I\nC0r/MWH3A8AwM7uMMPf3eemMQUSkVJYvh86dYe5cmDcPBhS3Lk3VkdbE7+7rCTMGJu5bQRjlIyJS\nsSZMgJ12go4diy63Zg106wYLF8Ipp8Bf/gKHHgo9epRvPD/+CE88AStWwGOPgSVd5bLc6c5dEckO\nGzbA6afDUUfB3XcX3n2TX27mTHjtNRgxAo45Bn7/e5g6tej3mDYNfv3r8E3hpZfgp5+Sl1u7NnyD\naNMG7roLBg6E1wsuUJc+SvwiUjUtWwaPPALr16dWfsSI0MI+7jjo3x9OOin04SfasgXOPTd8M3jx\nxdDar1kThg+Hxo2he3dYujR5/cOGQadOIa5vvoGLL4Zf/AJ694aPPwb3EOtDD4WEf/vtcPTRMGkS\nHHQQ9O0LmzeX7XeSKnev9I927dq5iMjPZsxwb9XKHdz/+tfUjjnxRPfWrd1zc91feMG9Th333Xd3\nf++98HpurvuFF4Y6//73HY+fOtW9dm33Tp3cN23atj831/3228NxRx3lvmRJ2DdunHuvXuF9wH3/\n/d2bNAnPTz7ZfdKkbXX8979h/xNPlPY3khQw2ZPk1NiTeioPJX4R+dmbb7rXreu+xx7uhx4aPgC2\nbCn6mP/9z93M/Z57tu2bPdv9wAPD/jvucL/yypASBwwovJ6hQ0OZyy93z8tzX7PGvXv3sO+yy9w3\nbtzxmDVr3J991v2449xPOcV94sQdy+TluXfp4t6okfvq1Sn8ElKjxC8iVVtenvuDD4ZE3b69+6JF\n7q+/HtLYa68VfWy/fuG4BQu2379unfull4Y6wP2mm8L7FOXWW0PZO+5wP+gg9+rVQ0u9uOOKM2VK\nqPfWW8tWT4LCEn+VWHqxffv2rvn4RSqpLVvCSJl02rQp9JW/+CKcf34YW1+nTrhAu/fe0Lo1jBuX\n/Ni8vFBmn31g9OjkZYYODf3yN91U/MiavLzQ1//WW9CgAfz733BCOQ1UvOiicEH5yy+hRYsyV2dm\nU9y9fcH9urgrIqU3bBjstlsYwZIuS5ZAly4h6d9zDwwZEpI+QPXqcPXV8MEHMH168uPHjYMFC8Ko\nnML06AE335zacMpq1WDwYLjzTvj00/JL+gD33x++e9x5Z/nVmUyyrwGV7aGuHpFKaMkS94YN3WvU\nCN0oL71U/u/x3nvuTZuGi6rDhiUvs3JluIB66aXJX7/oIvfddnP/6afyjy8dbrop/D6nTy9zVRTS\n1aMWv0imWrMmtHKHDQutyPLWpw+sWxeGKnbuHIYvvvxy+dS9ZUsY3ti1KzRqFIY8nntu8rINGoT3\nfuWVcLdtoh9/DEMxL7wQatcun9jS7dZboX798A0kXZJ9GlS2h1r8IqWQfxEyf5jhJ58UXX7tWveX\nX3Z/553i6x4+3LcbAbN+fRiVUq1a2Vv+8+e7d+wY6u/dO9RdnM8/D+Xvu2/7/f/8Z9j/6adli6mi\nPfpoiPvdd8tUDRrVI5JFvvnGvVYt9wsucH/mmdBdAu49e7ovXLit3Nat7mPGuF98sfsuu4QyZu5P\nP1143T/8EOr79a/dN2/etn/9evfOnUPyf/nl0sU9bFjoltl118K7dgrTtWsY4pkYU4cO7gcfXPYR\nNxVt40b3Nm3CcNWtW0tdjRK/SDa55BL3mjW3DV9cs8b9ttvcd945fCDcdpt7377uzZuHNLDrru5/\n+IN7Tk64uQjcH3kked2/+13o10/WB13S5L9hg/sHH7jfe2/4xgAhWc+fX/JzfuutcPyQIWF79uyw\n/eijJa+rMhgyJPwbJt7oVUJK/CLZYvr00Gq/8cYdX1uwIHwLgDD+/NRT3V99dfsLn5s2uZ9zTihz\n993bt5bzk+tddxX+/uvWbUv+bduGlvhFF7n/+c/uf/lL+AZy663uRx8dElt+d9Qhh7j37799i70k\ncnPd9903dBO5h/OvUcN92bLS1Re3vLztv52VQmGJX+P4RTJNt27hYujXX4cLn8l89RXsuis0Tbry\nKWzdCpdfDi+8ANdfDw8/HC6UHnQQNGwIU6aEOWwKs359mAht7twwd83y5eFn/rw6NWpAu3Zh8rNj\njw1z3DRsWKbTBuDJJ+Gaa+DDD+Gss0K9FTj5WWVT2Dj+tE7LLCIVbPRoePfdMHlZYUkfYN99i66n\nRg147rnw4fDoo2GEUF5eGFP/xhtFJ32AXXYJHxYF/fRT+BBo3DiUKW+9eoXJzy66KHzQXHpp+b9H\nBlDiF8kUubnhztM2beCqq8peX7Vq8PjjIfnfd1/Yd/PNcPjhpa+zTh1o1arssRVm113DENaBA8PM\nmCefnL73qsKU+EUyxeDBMGNGuLN1553Lp04zuPfe0ELPyYF+/cqn3nTq0yd0+fTqFb65yA7Uxy+S\nCTZsgP32C332n3wSWuvZbNasMDdPVblpK03Uxy9SVSxaFCY923331I8ZOBC+/TbMmZPtSR/gkEPi\njqBS01+ISGWyciW0bw9t28L8+akds3x5WBP29NPD6lIixVDiF6lMrrkGfvgBNm4M89QsWVJ0+WXL\nQrmNG+GBByomRqnylPhFKos33ggXaO+4A95+OyT9bt1g9erk5b/7LoyBnzsXRo6EAw+s2HilylLi\nF6kMVqyAK64IXTy33QYdOoQbjz7/PHTh/PTT9uW//jos1L14cRi3/5vfxBO3VElK/CKVQZ8+oX9/\n0KBtq1l17RqmOZ44Maw6tWVL2D97drjjdd06eP/98FykBJT4ReL2+uth7P2dd8Khh27/2nnnwVNP\nhWX+LrssrPh07LHhtfHjw7QHIiWkcfwicfrhhzD/TfPmYUGTwtauve++8MFQvXpYizUnB/baq2Jj\nlSpH4/hFKqOrr4ZVq2DMmKIXLL/99tDPP3ZsWFFqzz0rLkbJOEr8InEZPhxefTVMiVDcDUdmMGBA\nxcQlGU99/CJxWLkS/vQn+PWv4ZZb4o5Gsoxa/CJxuO22kPxHjy66i0ckDdTiF6lon3wCTz8d7tI9\n7LC4o5EspBa/SEn873/w0Uew//5wwAFhfvmS2LoVrrwSmjWDe+5JT4wixVDiF0nVl1+Gu2WXLw/b\nZmHRk4MOgoMPhiOPhNNOC/sL83//B9OmwbBhUK9excQtUoASv2SOd9+FN9+EJ55IbQGOvLywYMfx\nxxff5fLdd3DSSeH5+++H/vnZs7c93n57W2v+ySfDePuCFi8O8/B07QrnnFPi0xMpL0r8khkmToQz\nzwyzVHboEFZfKs6IEXDddVCrVuhz/93vkpdbsSLMhbNqFYwbF0biAJx99rYymzfDXXfBgw+GBP/K\nKzsuAnL99aHcU08V/a1AJM3SenHXzOqb2XAz+8LM5pjZkWbW0MxGm9lX0c8iVoQWSUH+RGYtW4bx\n8Pfeu21em8Lk5sLdd8MvfwkdO8LFF4f5cjZv3r7cunVw6qlhUrSRI7cl/YJq1gzTIj/xRPjWccIJ\n4QMj35gxMHQo9O0bVoYSiZO7p+0BDAL+ED2vCdQHHgL6Rvv6Ag8WV0+7du1cJKlvv3Vv0cL9F79w\nnz/ffeRId3B/7rmijxs8OJQbOtR9yxb3G24I2506uX//fSizaZN7167u1aq5jxiRekzDh7vvvLP7\nL38ZYtq4MTzfe2/3DRtKf64iJQRM9mS5OdnO8ngAuwH/I5oPKGH/XKBZ9LwZMLe4upT4JalVq9wP\nPti9Xj33adPCvrw89/bt3Vu3dt+8OflxW7a477uv+yGHuOfmbts/ZIh7nTrhQ2T8ePfzzw//RZ5/\nvuSxTZjg3qCBe9Om7pdcEup5552S1yNSBnEk/rbAJOAFYBrwLLALsDqhjCVuFzi+NzAZmNyyZct0\n/36kqtmwwf3YY9132sk9J2f71/773/Cn/fTTyY99/vnwerJW/IwZoWUO4fHQQ6WP8fPP3Vu2DPWc\ne27p6xEppcISf9pm5zSz9sDHQCd3/8TMBgJrgD7uXj+h3Cp3L7KfX7NzynZyc8P89K+9FqYz7tFj\n+9fdw9DKxYvhq69C/3u+zZthv/2gUaMwxXGyi6yrVoWbqw44INxhWxbffw+PPw433ABNm5atLpES\nimN2zu+A79z9k2h7OKFPf6mZNXP3xWbWDFiWxhgkU2zYAJMmwYQJYejkRx/Bo4/umPQhJPP+/cNI\nnOefDytb5fvXv2DBgjCevrCRNQ0awEsvlU/ce+wBDz1UPnWJlJO0zsdvZhMIF3fnmlk/QlcPwAp3\nf8DM+gIN3f3moupRiz9LTZkSWvXjx4fWef6Im0MOgUsvhT//ufBj3cPNVt98A/Pmwc47h6Ge++wD\nrVrBhx9qSKVkvLjm4+8DDDazmsB84FLCENJhZnYZsBA4L80xSFU0YwZ06hS6ddq1g2uvDUsMduoE\nDRsWf3x+q//EE+HZZ+Gqq8JY/UWL4MUXlfQlq2kFLql81q8PyX7NmjC9QWn7xt3DXbnz5sHMmWFq\nhQMPDHfeimQBrcAlVcc114R5ccaMKdsFUbMwEVrnzuGGqqVLQ9eRSJbTtMxSuQwdGi7I3nordOlS\n9vqOPz4k/hkzwsXeTp3KXqdIFafEL5XH/PnQu3cYitmvX/nVO2AANGmipQtFIurqkcphyxa44AKo\nVi1McFaeq1J17AjLNGpYJJ9a/FIxPv4YWrcOk6H99787ToZ2xx1hnP6zz4ZyIpI2avFLxRgwIMxW\n+Z//hJuj6teH3/423IG7dWu4yal3b81TL1IB1OKX9Pv6a3jrrXDD1dKlIfmffjoMHw7duoVVqw46\nCB57LO5IRbKCWvySfn/7W1iR6sorw7w5p50WHhs3hlWz3nsvDOEs6fq1IlIqSvySXmvXhuGZ550X\nFhhPVKsWdO8eHiJSYdTVI+n1wgvhDtxrr407EhGJKPFL+uQvZt6xIxxxRNzRiEhEiV/S5513wnz4\n11wTdyQikkCJX9Jn4MAwH72GaIpUKkr8kh5z5oTROn/6U/nehSsiZabEL+nx5JNh8ZPeveOOREQK\nUOKX8rd6NQwaBBdeGCZHE5FKRYlfyt9zz8FPP+mirkglpcQv5Ss3N9ype+yx0LZt3NGISBJK/FK+\nRo6EBQt0w5ZIJabEL+Vnwwa4+WbYZx8444y4oxGRQmiuHik/998fFjYfMwZq6E9LpLJSi1/Kx2ef\nwYMPQq9eYWFzEam0lPil7PLy4PLLw+IqDz8cdzQiUgx9H5ey+8c/wtKKL70EjRvHHY2IFEMtfimb\nRYugb1846STo2TPuaEQkBUr8UjZ9+oQ1c//xDzCLOxoRSYG6eqT0RowIjwcfhL32ijsaEUmRWvxS\nOj/+CFdfDYcdFhZRF5EqQy1+KTl3uOEGWLwY3nhD0y6LVDFq8UvJDRwYJmK75RY4/PC4oxGRElLi\nl5IZMQKuvx7OOivcqSsiVY4Sv6Ru0qQwZPPww8OY/Wr68xGpivQ/V1KzYAGcfjo0bRpm4KxTJ+6I\nRKSUik38ZtbHzBpURDBSSa1eDaecAps3w6hRIfmLSJWVSou/KfCpmQ0zs25mqd+lY2YLzGyWmU03\ns8nRvoZmNtrMvop+6kOlonz4YZgu+euvUz9m82Y4++ww6+aIEXDAAemLT0QqRLGJ393vAPYFngMu\nAb4yswFmtneK79HZ3du6e/touy+Q4+77AjnRtqTb99+HBP6f/8BRR8HUqcUfs2kT/P738P778Oyz\ncPzxaQ9TRNIvpT5+d3dgSfTYCjQAhpvZQ6V4z+7AoOj5IODMUtQhJbFlC5x3HqxfH1rttWvDcceF\nefMLM3s2HHEEDB4M990HF19ccfGKSFql0sd/rZlNAR4CJgKHuPuVQDvg7GIOd+A9M5tiZr2jfU3d\nfXH0fAmhK0nSqW9fmDgRnnkGzjwTPvoI2rQJ/fZDh25f1h2eeALatQs3aP3nP3D77fHELSJpkcqd\nuw2Bs9x9YeJOd88zs9OKOfZod19kZrsDo83siwJ1uJl5sgOjD4reAC1btkwhzCx0992wcSPceSfU\nrZu8zPDh8OijYXqFCy4I+/bYA8aPh+7dw74lS+C660Kiv/RSePddOPXUcJOWLuSKZB53L/IBdATq\nJWzvCnQo7rgk9fQDbgTmAs2ifc2AucUd265dO5cCRo50D+1z99at3d97b8cyc+e616vn3qGD+6ZN\nO76+YYP7WWeFOnr2dG/UyL12bfe//909Ly/95yAiaQVM9iQ5NZU+/r8D6xK210X7imRmu5hZvfzn\nQFfgM2Ak0Csq1gt4M4UYJNHq1XDFFXDIIZCTAzVrQteu4ULsqlWhzPr14WJuzZowbFj4WVCtWuG1\nK68MffmtWoWLvldcoSmWRTJYKl09Fn1yAD938aRyXFNgRDT6swbwiru/Y2afAsPM7DJgIXBeKeLO\nbjfeCEuXhhup2rWDGTOgf3946CF4+2146qlwEXf2bHjnHSiqq6x69VD+0kvDTJvJPiBEJKOkksDn\nm9k1bGvl/wmYX9xB7j4fOCzJ/hWAVuMurdGjQ997374h6UNouQ8YAOeeG1r9Z0fX3O+5J3wTKI6Z\nJlsTySKW0JhPXiBcmH0C6EIYpZMDXOfuy9IfXtC+fXufPHlyRb1d5bV2bejeqVULpk8PPwvasgUe\nfzwsifjoo5pPRySLmdkU33YP1c+KbfFHCb5HWqKSkrn1Vvjmm3AHbrKkD2Fu/Jtuqti4RKRKKTbx\nm1kt4DLgIODnbOPuv09jXFLQ+PGhL/6668KdtyIipZRKP8BLwC+A3wAfAM2BtekMSgr46afQd7/X\nXuEuWhGRMkjl4u4+7n6umXV390Fm9gowId2BSYI77wwTq73/PuyyS9zRiEgVl0qLf0v0c7WZHQzs\nBuyevpBkO3Pnhou1f/wjdO4cdzQikgFSafE/HU2dfAfh5qu6wJ1pjUq2uffecCG3f/+4IxGRDFFk\n4jezasAad18FjAf2qpCoJPjiCxgyBG64AXbXlywRKR9FdvW4ex5wcwXFIgX17x+mUNbwTBEpR6n0\n8Y8xsxvNrEW0elZDM2uY9siy3Zw5Ycrkq6+GJk3ijkZEMkgqffznRz+vStjnqNsnvfr3Dwua33hj\n3JGISIZJ5c7dNhURiCSYPRtefRVuuQUaN447GhHJMKncuZt0zT13f7H8w8kS7kVPe3zvvWG8/g03\nVFxMIpI1UunjPzzhcQxhQZUz0hhTZhsyBOrVg8ceg9zcHV+fPTvMkd+nj1r7IpIWqXT19EncNrP6\nwNBCiktxXn0VNmyA668PyyI+/zzst9+21++5JyyjqNa+iKRJaebsXQ+o3780tm6FcePCvDsvvRRG\n7rRtCw8/HFr/s2bBv/8N11wDjRrFHa2IZKhU+vj/QxjFA+GD4kBgWDqDylhTp8KPP8KJJ8L558MJ\nJ4RlD2+6CV57LfTr16sXvg2IiKRJKsM5H054vhVY6O7fpSmezDZmTPjZpUv42axZWCJx6NDQp79i\nBdxxBzTUbRIikj6pJP5vgMXuvhHAzGqbWWt3X5DWyDJRTk5Y1zbxhiwzuOCC8GEwZAj84Q/xxSci\nWSGVPv5/A3kJ27nRPimJDRtg4sTQvZNM06ZhkZW6dSs2LhHJOqkk/hruvjl/I3peM30hZaiJE2HT\nptC/LyISo1QS/3Iz+3ncvpl1B35IX0gZaswYqFEDjjkm7khEJMul0sd/BTDYzP4WbX8HJL2bV4qQ\nkwNHHqmuHBGJXSo3cH0NdDSzutH2urRHlWlWroQpU+Duu+OORESk+K4eMxtgZvXdfZ27rzOzBmam\nFb9LYty4MD+P+vdFpBJIpY//ZHdfnb8RrcZ1SvpCykBjxoQuniOOiDsSEZGUEn91M9s5f8PMagM7\nF1FeCsrJgeOOg512ijsSEZGULu4OBnLM7F+AAZcAg9IZVEb59lv48ku44oq4IxERAVK7uPugmc0A\nTiTM2fMu0CrdgWWMnJzws7Abt0REKliqs3MuJST9c4EuwJy0RZRpcnJg993h4IPjjkREBCiixW9m\nvwQuiB4/AK8C5u6dKyi2qs89XNjt0gWqlWYGbBGR8ldUV88XwATgNHefB2Bmf66QqDLFnDmwZImG\ncYpIpVJUM/QsYDEw1syeMbMTCBd3JVX50zCrf19EKpFCE7+7v+HuPYD9gbHAdcDuZvZ3M+taUQFW\naTk5sPfe0Lp13JGIiPys2I5nd1/v7q+4++lAc2AacEvaI6sqVq+GNWt23J+/zKJa+yJSyZToiqO7\nr3L3p9095WxmZtXNbJqZvRVttzGzT8xsnpm9amZVd4rn1avhkEPCilmdOkG/fvDhh7BlC0yeHD4Q\n1L8vIpVMRQw1uZbth38+CDzm7vsAq4DLKiCG9Lj5Zvj+e7jqqpDs+/cP0y43agS9eoUynTUISkQq\nl7QmfjNrDpwKPBttG+E+gOFRkUHAmemMIW3GjYNnngkLow8cCJMmwQ8/wPDh0LMn5ObC6adD48Zx\nRyoish1z9/RVbjYc+AtQD7iRMN3Dx1FrHzNrAbzt7jvc3WRmvYHeAC1btmy3cOHCtMVZYhs2wKGH\nhnH6M2dCnTpxRyQisgMzm+Lu7QvuT1uL38xOA5a5+5TSHB9dS2jv7u2bJC5OXhn06wfz5sHTTyvp\ni0iVk8okbaXVCTjDzE4BavyHOI8AAAzkSURBVAG7AgOB+mZWw923EkYJLUpjDOVv6lR45BG47LJw\nR66ISBWTtha/u9/q7s3dvTXQA3jf3XsS7gk4JyrWC3gzXTGUuy1bQsJv0gT++te4oxERKZU4JpC5\nBbjezOYBjYDnYoihdB55BKZPh6eeggYN4o5GRKRU0tnV8zN3HweMi57PB6reUlRffhn69s86KzxE\nRKooTRmZCnfo3Rtq14a//S3uaEREyqRCWvxV3uzZ8MEH8Pjj0KxZ3NGIiJSJWvypGDcu/DzjjFjD\nEBEpD0r8qRg7Flq1gjZt4o5ERKTMlPiLk5cXunmOPz7uSEREyoUSf3Fmz4YVK5T4RSRjKPEXZ+zY\n8FOJX0QyhBJ/ccaNCytoaRUtEckQSvxFUf++iGQgJf6izJoFK1dqMRURyShK/EXJH79/3HGxhiEi\nUp6U+IsyblwYu9+qVdyRiIiUGyX+wuT376ubR0QyjBJ/YWbOhFWrdGFXRDKOEn9h8vv3lfhFJMMo\n8Rdm7FjYe29o0SLuSEREypUSfzK5uTB+vFr7IpKRlPiTmTEDVq9W4heRjKTEn4z690UkgynxJzNu\nHOyzDzRvHnckIiLlTom/IPXvi0iGU+IvaPp0+PFH3bglIhlLib8gzc8jIhlOib+gsWNh331hzz3j\njkREJC2U+BNt3QoTJqibR0QymhJ/oqlTYc0aXdgVkYymxJ9o+HCoUQNOOinuSERE0kaJP19eHrzy\nCnTrBo0bxx2NiEjaKPHn++ADWLQIevaMOxIRkbRS4s83eDDUrQtnnBF3JCIiaaXED7BxY+jf/+1v\noU6duKMREUkrJX6AUaPC3boXXRR3JCIiaafED6Gbp2lT6NIl7khERNJOiX/VKnjrLejRIwzlFBHJ\ncEr8r70GmzdrNI+IZI20JX4zq2Vmk8xshpnNNrN7ov1tzOwTM5tnZq+aWc10xZCSwYPD3Dzt28ca\nhohIRUlni38T0MXdDwPaAt3MrCPwIPCYu+8DrAIuS2MMRfvuuzB+/6KLwCy2MEREKlLaEr8H66LN\nnaKHA12A4dH+QcCZ6YqhWEOGgDtceGFsIYiIVLS09vGbWXUzmw4sA0YDXwOr3X1rVOQ7IOn8x2bW\n28wmm9nk5cuXpyfAwYOhQ4ewzKKISJZIa+J391x3bws0B44A9i/BsU+7e3t3b9+kSZPyD+6zz2DG\nDF3UFZGsUyGjetx9NTAWOBKob2b54yabA4sqIoYdDB4M1avD+efH8vYiInFJ56ieJmZWP3peGzgJ\nmEP4ADgnKtYLeDNdMRQqfybOrl1h990r/O1FROKUzhZ/M2Csmc0EPgVGu/tbwC3A9WY2D2gEPJfG\nGJKbOBG++UbdPCKSldJ2q6q7zwR+lWT/fEJ/f3yGD4dataB791jDEBGJQ3beuTtqFJxwQpiGWUQk\ny2Rf4v/qK5g3D04+Oe5IRERikX2Jf9So8FOJX0SyVHYm/v33h732ijsSEZFYZFfiX78exo2DU06J\nOxIRkdhkV+IfOzZMwaxuHhHJYtmV+EeNgl12gWOOiTsSEZHYZE/idw+J/8QTYeed445GRCQ22ZP4\n58yBhQvVvy8iWS97Ev/bb4ef6t8XkSyXPYl/1Cg4+GBo0SLuSEREYpUdiX/NGpgwQd08IiJkS+LP\nyYEtW5T4RUTIlsQ/ahTsuiscdVTckYiIxC7zE797uLDbtSvstFPc0YiIxC7zE/+sWbBokUbziIhE\nMj/x58/G2a1bvHGIiFQS2ZH4f/Ur2GOPuCMREakUMjvxr14NH32k0TwiIgkyO/GPHg25uerfFxFJ\nkNmJf9QoaNAAOnSIOxIRkUojsxP/fvvBH/8INWrEHYmISKWR2Rmxb9+4IxARqXQyu8UvIiI7UOIX\nEckySvwiIllGiV9EJMso8YuIZBklfhGRLKPELyKSZZT4RUSyjLl73DEUy8yWAwtLeXhj4IdyDKeq\n0Hlnl2w9b8jec0/lvFu5e5OCO6tE4i8LM5vs7u3jjqOi6byzS7aeN2TvuZflvNXVIyKSZZT4RUSy\nTDYk/qfjDiAmOu/skq3nDdl77qU+74zv4xcRke1lQ4tfREQSKPGLiGSZjE78ZtbNzOaa2Twzy9hV\nWczseTNbZmafJexraGajzeyr6GeDOGNMBzNrYWZjzexzM5ttZtdG+zP63M2slplNMrMZ0XnfE+1v\nY2afRH/vr5pZzbhjTQczq25m08zsrWg748/bzBaY2Swzm25mk6N9pf47z9jEb2bVgaeAk4EDgQvM\n7MB4o0qbF4BuBfb1BXLcfV8gJ9rONFuBG9z9QKAjcFX0b5zp574J6OLuhwFtgW5m1hF4EHjM3fcB\nVgGXxRhjOl0LzEnYzpbz7uzubRPG7pf67zxjEz9wBDDP3ee7+2ZgKNA95pjSwt3HAysL7O4ODIqe\nDwLOrNCgKoC7L3b3qdHztYRksCcZfu4erIs2d4oeDnQBhkf7M+68AcysOXAq8Gy0bWTBeRei1H/n\nmZz49wS+Tdj+LtqXLZq6++Lo+RKgaZzBpJuZtQZ+BXxCFpx71N0xHVgGjAa+Bla7+9aoSKb+vT8O\n3AzkRduNyI7zduA9M5tiZr2jfaX+O8/sxdYFCC1EM8vYcbtmVhd4DbjO3deERmCQqefu7rlAWzOr\nD4wA9o85pLQzs9OAZe4+xcyOjzueCna0uy8ys92B0Wb2ReKLJf07z+QW/yKgRcJ282hftlhqZs0A\nop/LYo4nLcxsJ0LSH+zur0e7s+LcAdx9NTAWOBKob2b5jblM/HvvBJxhZgsIXbddgIFk/nnj7oui\nn8sIH/RHUIa/80xO/J8C+0ZX/GsCPYCRMcdUkUYCvaLnvYA3Y4wlLaL+3eeAOe7+aMJLGX3uZtYk\nauljZrWBkwjXN8YC50TFMu683f1Wd2/u7q0J/5/fd/eeZPh5m9kuZlYv/znQFfiMMvydZ/Sdu2Z2\nCqFPsDrwvLvfH3NIaWFmQ4DjCdO0LgXuBt4AhgEtCVNan+fuBS8AV2lmdjQwAZjFtj7f2wj9/Bl7\n7mZ2KOFiXnVC422Yu/c3s70ILeGGwDTgInffFF+k6RN19dzo7qdl+nlH5zci2qwBvOLu95tZI0r5\nd57RiV9ERHaUyV09IiKShBK/iEiWUeIXEckySvwiIllGiV9EJMso8UuszMzN7JGE7RvNrF851f2C\nmZ1TfMkyv8+5ZjbHzMYW2L+HmQ2PnreNhheX13vWN7M/JXsvkeIo8UvcNgFnmVnjuANJlHAnaCou\nAy53986JO939e3fP/+BpC5Qo8RcTQ33g58Rf4L1EiqTEL3HbSlg79M8FXyjYYjezddHP483sAzN7\n08zmm9kDZtYzmqN+lpntnVDNiWY22cy+jOZ6yZ/g7K9m9qmZzTSzPybUO8HMRgKfJ4nngqj+z8zs\nwWjfXcDRwHNm9tcC5VtHZWsC/YHzo/nUz4/uxnw+inmamXWPjrnEzEaa2ftAjpnVNbMcM5savXf+\nDLMPAHtH9f01/72iOmqZ2b+i8tPMrHNC3a+b2TsW5nB/qMT/WpIRNEmbVAZPATNLmIgOAw4gTEc9\nH3jW3Y+wsBhLH+C6qFxrwrwmewNjzWwf4GLgR3c/3Mx2Biaa2XtR+V8DB7v7/xLfzMz2IMz73o4w\n5/t7ZnZmdMdsF8JdpJOTBerum6MPiPbufnVU3wDClAO/j6ZfmGRmYxJiONTdV0at/t9Gk881Bj6O\nPpj6RnG2jeprnfCWV4W39UPMbP8o1l9Gr7UlzGK6CZhrZk+6e+IstpIF1OKX2Ln7GuBF4JoSHPZp\nNB//JsKUxPmJexYh2ecb5u557v4V4QNif8JcJxdbmNb4E8LUvvtG5ScVTPqRw4Fx7r48mgJ4MHBs\nCeItqCvQN4phHFCLcOs9wOiEW+8NGGBmM4ExhCmHi5t+92jgZQB3/4JwO39+4s9x9x/dfSPhW02r\nMpyDVFFq8Utl8TgwFfhXwr6tRI0TM6sGJC6plzgXS17Cdh7b/10XnJPECcm0j7u/m/hCNP/L+tKF\nX2IGnO3ucwvE0KFADD2BJkA7d99iYWbKWmV438TfWy7KAVlJLX6pFKIW7jC2XzZvAaFrBeAMwkpT\nJXWumVWL+v33AuYC7wJXWpjSGTP7ZTTrYVEmAceZWWMLy3peAHxQgjjWAvUStt8F+piFxQPM7FeF\nHLcbYQ76LVFffX4LvWB9iSYQPjCIunhaEs5bBFDil8rlEcIMo/meISTbGYT55kvTGv+GkLTfBq6I\nujieJXRzTI0uiP6TYlq+0UpHfQlTAM8Aprh7Sab/HQscmH9xF7iX8EE208xmR9vJDAbam9kswrWJ\nL6J4VhCuTXxW8KIy8H9AteiYV4FLMmm2Sik7zc4pIpJl1OIXEckySvwiIllGiV9EJMso8YuIZBkl\nfhGRLKPELyKSZZT4RUSyzP8HbQAJEf9Qo2YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 24.850373377199986 seconds\n",
            "Best accuracy: 69.04  Best training loss: 0.5774543285369873  Best validation loss: 0.9700033265352249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wv1zFW8OLMSl",
        "colab_type": "text"
      },
      "source": [
        "#### adding layers 128 384"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm6fNo5xFQt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 24, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 56, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "            # self.features = nn.Sequential(\n",
        "            #     nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            #     nn.ReLU(inplace=True),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(96, 16, 64, 64),\n",
        "            #     Fire(128, 16, 64, 64),\n",
        "            #     Fire(128, 32, 128, 128),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(256, 32, 128, 128),\n",
        "            #     Fire(256, 48, 192, 192),\n",
        "            #     Fire(384, 48, 192, 192),\n",
        "            #     Fire(384, 64, 256, 256),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "                # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),t\n",
        "            # )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpSXAq-HF7ve",
        "colab_type": "code",
        "outputId": "1c40cd8c-0a06-4c0e-b0ab-b3ba78a167bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.325768\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.297450\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.302597\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.302585\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 10.24%\n",
            "Better loss at Epoch 0: loss = 2.302584409713745%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 512/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 2.302584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-34a24e5c9987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmodel = squeezenet1_0(num_classes=10)\\nmodel = model.to(device=device, dtype=torch.float)\\n\\n# Cross Entropy Loss \\nerror = CrossEntropyLoss().to(device=device, dtype=torch.float)\\n\\n#Optimizer\\nlearning_rate = 0.1\\noptimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\\n\\n#Optimizer adam\\n# learning_rate = 0.04\\n# optimizer = Adam(model.parameters(), lr=learning_rate)\\n# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\\n# optimizer = SGD(model.parameters(), lr=learning_rate)\\n\\n#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    sta...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#loss_total = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#peak at tensor details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# if batch_idx==1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8yfW9VlGDZL",
        "colab_type": "code",
        "outputId": "e555c4b3-3f35-48b4-e5ff-1ce324920e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njSNaQWg0Go3Gkf8PJaZiUEnql+gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njBK/iEiZUeIXESkzSvwiImXm/wOMM9We9xJXawAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 25.150628093634158 seconds\n",
            "Best accuracy: 70.64  Best training loss: 0.4841686189174652  Best validation loss: 0.9113111990690231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0S-DRYNITzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyRpVWnuLdsJ",
        "colab_type": "text"
      },
      "source": [
        "#### adding layers 128 384 with batchnorm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dfSj2sULiuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 24, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 56, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "            # self.features = nn.Sequential(\n",
        "            #     nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            #     nn.ReLU(inplace=True),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(96, 16, 64, 64),\n",
        "            #     Fire(128, 16, 64, 64),\n",
        "            #     Fire(128, 32, 128, 128),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(256, 32, 128, 128),\n",
        "            #     Fire(256, 48, 192, 192),\n",
        "            #     Fire(384, 48, 192, 192),\n",
        "            #     Fire(384, 64, 256, 256),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "                # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),t\n",
        "            # )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcsbXurSNRjA",
        "colab_type": "code",
        "outputId": "e6120276-0ba0-4c18-9c75-098f742ba2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.324384\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.245353\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.199630\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.162025\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.129273\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.106871\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.109345\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.021423\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.969163\n",
            "\n",
            "Test set: Test loss: 1.8752, Accuracy: 1118/5000 (22%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 22.36%\n",
            "Better loss at Epoch 0: loss = 1.8752297365665436%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.943239\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.908922\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.875375\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.864101\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.873302\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.854273\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.842745\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.844497\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.842782\n",
            "\n",
            "Test set: Test loss: 1.7965, Accuracy: 1496/5000 (30%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 29.92%\n",
            "Better loss at Epoch 1: loss = 1.7965250253677367%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.837471\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.828294\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.799930\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.799889\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.809414\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.796984\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.784202\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.775042\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.758737\n",
            "\n",
            "Test set: Test loss: 1.7917, Accuracy: 1655/5000 (33%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 33.1%\n",
            "Better loss at Epoch 2: loss = 1.791737016439438%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.751940\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.752705\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.751751\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.730328\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.733294\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.715361\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.713208\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.679736\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.672035\n",
            "\n",
            "Test set: Test loss: 1.6572, Accuracy: 1946/5000 (39%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 38.92%\n",
            "Better loss at Epoch 3: loss = 1.657247017621994%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.698605\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.674177\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.657347\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.666264\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.641016\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.649492\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.627147\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.610739\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.625384\n",
            "\n",
            "Test set: Test loss: 1.5609, Accuracy: 2184/5000 (44%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 43.68%\n",
            "Better loss at Epoch 4: loss = 1.5608537662029267%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.636332\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 1.602316\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.596065\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.600441\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.547310\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 1.602502\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 1.559242\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.577404\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 1.545269\n",
            "\n",
            "Test set: Test loss: 1.5012, Accuracy: 2340/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 46.8%\n",
            "Better loss at Epoch 5: loss = 1.5011513710021973%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 1.524583\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 1.537999\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 1.552155\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 1.538116\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 1.562182\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 1.558941\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 1.550579\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 1.508819\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 1.531628\n",
            "\n",
            "Test set: Test loss: 1.4733, Accuracy: 2423/5000 (48%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 48.46%\n",
            "Better loss at Epoch 6: loss = 1.4733422088623047%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 1.539575\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 1.526286\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 1.491319\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 1.477614\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 1.491201\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 1.465527\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 1.503379\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 1.507392\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 1.422643\n",
            "\n",
            "Test set: Test loss: 1.5306, Accuracy: 2342/5000 (47%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 1.480256\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 1.440094\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 1.460080\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 1.489049\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 1.469511\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 1.451602\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 1.446196\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 1.416273\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 1.444792\n",
            "\n",
            "Test set: Test loss: 1.3869, Accuracy: 2573/5000 (51%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 51.46%\n",
            "Better loss at Epoch 8: loss = 1.3868926763534546%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 1.420885\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 1.426587\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 1.405329\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 1.410350\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 1.432129\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 1.411968\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 1.348737\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 1.415581\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 1.444181\n",
            "\n",
            "Test set: Test loss: 1.4388, Accuracy: 2478/5000 (50%)\n",
            "\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 1.424254\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 1.368837\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 1.409733\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 1.365518\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 1.391174\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 1.412346\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 1.392452\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 1.368804\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 1.351823\n",
            "\n",
            "Test set: Test loss: 1.3554, Accuracy: 2590/5000 (52%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 51.8%\n",
            "Better loss at Epoch 10: loss = 1.3554437482357025%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 1.375420\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 1.349839\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 1.360714\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 1.362233\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 1.347185\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 1.378967\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 1.375409\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 1.375320\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 1.348671\n",
            "\n",
            "Test set: Test loss: 1.3465, Accuracy: 2677/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 53.54%\n",
            "Better loss at Epoch 11: loss = 1.3465025568008422%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 1.322995\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 1.341323\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 1.324019\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 1.373101\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 1.323810\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 1.338537\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 1.337958\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 1.338466\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 1.324957\n",
            "\n",
            "Test set: Test loss: 1.3606, Accuracy: 2665/5000 (53%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 1.318284\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 1.296589\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 1.299712\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 1.312558\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 1.346547\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 1.320447\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 1.306570\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 1.339622\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 1.326996\n",
            "\n",
            "Test set: Test loss: 1.3100, Accuracy: 2718/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 54.36%\n",
            "Better loss at Epoch 13: loss = 1.3099944245815278%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 1.292799\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 1.278496\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 1.333404\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 1.306284\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 1.293914\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 1.284197\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 1.285721\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 1.265214\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 1.286531\n",
            "\n",
            "Test set: Test loss: 1.3338, Accuracy: 2666/5000 (53%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 1.269578\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 1.283108\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 1.268715\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 1.270821\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 1.280295\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 1.254205\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 1.269722\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 1.261980\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 1.288477\n",
            "\n",
            "Test set: Test loss: 1.2878, Accuracy: 2765/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 55.3%\n",
            "Better loss at Epoch 15: loss = 1.287791999578476%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 1.263036\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 1.223527\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 1.241874\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 1.239670\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 1.266840\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 1.253479\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 1.277583\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 1.273555\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 1.242695\n",
            "\n",
            "Test set: Test loss: 1.3039, Accuracy: 2776/5000 (56%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 55.52%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 1.236080\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 1.236967\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 1.225278\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 1.213618\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 1.259190\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 1.240632\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 1.231860\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 1.263463\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 1.272252\n",
            "\n",
            "Test set: Test loss: 1.3116, Accuracy: 2736/5000 (55%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 1.241679\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 1.131449\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 1.081717\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 1.106757\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 1.101526\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 1.104917\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 1.116534\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 1.095603\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 1.076049\n",
            "\n",
            "Test set: Test loss: 1.0689, Accuracy: 3189/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 63.78%\n",
            "Better loss at Epoch 18: loss = 1.068924612402916%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 1.083925\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 1.051902\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 1.049760\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 1.071723\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 1.073241\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 1.095148\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 1.065174\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 1.041570\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 1.054776\n",
            "\n",
            "Test set: Test loss: 1.1070, Accuracy: 3086/5000 (62%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 1.028274\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 1.083800\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 1.069030\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 1.047890\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 1.006646\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 1.022149\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 1.054249\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 1.056051\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 1.030770\n",
            "\n",
            "Test set: Test loss: 1.0845, Accuracy: 3153/5000 (63%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.989606\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 1.014221\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 1.000630\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 1.056191\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.996440\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 1.087944\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.998277\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 1.014909\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 1.039730\n",
            "\n",
            "Test set: Test loss: 1.1032, Accuracy: 3160/5000 (63%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 1.051855\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 1.031353\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.998254\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.975485\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 1.018709\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.977841\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 1.001973\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 1.023797\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 1.023711\n",
            "\n",
            "Test set: Test loss: 1.0865, Accuracy: 3172/5000 (63%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 1.007863\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.978970\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.972109\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 1.036606\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.974541\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.994153\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.971773\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.969387\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.987468\n",
            "\n",
            "Test set: Test loss: 1.0379, Accuracy: 3272/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 23: accuracy = 65.44%\n",
            "Better loss at Epoch 23: loss = 1.037909415960312%\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.948093\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.979991\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.984559\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.968187\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.991985\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.978851\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.945418\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.990906\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.945766\n",
            "\n",
            "Test set: Test loss: 1.0698, Accuracy: 3203/5000 (64%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.947454\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.955695\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.947292\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.955835\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.972504\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.984058\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.978470\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.953072\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.946791\n",
            "\n",
            "Test set: Test loss: 1.0134, Accuracy: 3252/5000 (65%)\n",
            "\n",
            "Better loss at Epoch 25: loss = 1.0134114450216294%\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.969339\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.948115\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.938414\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.937863\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.964330\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.948152\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.973652\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.970029\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.982044\n",
            "\n",
            "Test set: Test loss: 1.0187, Accuracy: 3299/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 26: accuracy = 65.98%\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.932795\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.886883\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.947367\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.918541\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.917068\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 1.008748\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.960157\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.929597\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.952886\n",
            "\n",
            "Test set: Test loss: 1.0614, Accuracy: 3200/5000 (64%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.910757\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.903114\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.904542\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.915810\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.890773\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.911196\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.953638\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.964540\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.931134\n",
            "\n",
            "Test set: Test loss: 1.0120, Accuracy: 3288/5000 (66%)\n",
            "\n",
            "Better loss at Epoch 28: loss = 1.0120019340515136%\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.888432\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.861200\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.917849\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.903157\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.933583\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.906862\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.912384\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.910914\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.939341\n",
            "\n",
            "Test set: Test loss: 0.9782, Accuracy: 3378/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 29: accuracy = 67.56%\n",
            "Better loss at Epoch 29: loss = 0.9782250815629959%\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.885930\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.888231\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.874947\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.926596\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.927983\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.910873\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.936673\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.885304\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.879435\n",
            "\n",
            "Test set: Test loss: 1.0284, Accuracy: 3263/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.878963\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.884411\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.838280\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.882806\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.893243\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.924062\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.873776\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.915685\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.913734\n",
            "\n",
            "Test set: Test loss: 1.0317, Accuracy: 3298/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.876913\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.884588\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.863227\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.877664\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.929970\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.888236\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.873726\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.877355\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.904053\n",
            "\n",
            "Test set: Test loss: 1.0582, Accuracy: 3217/5000 (64%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.890005\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.844449\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.834814\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.877351\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.871555\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.908146\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.876354\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.858594\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.851766\n",
            "\n",
            "Test set: Test loss: 0.9948, Accuracy: 3327/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.850876\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.855261\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.852409\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.880619\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.896726\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.870728\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.863844\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.864997\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.872593\n",
            "\n",
            "Test set: Test loss: 1.0220, Accuracy: 3349/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.808390\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.864712\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.858104\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.854889\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.850064\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.844521\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.866354\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.875461\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.853636\n",
            "\n",
            "Test set: Test loss: 0.9876, Accuracy: 3308/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.815629\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.853476\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.896042\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.886740\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.915066\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.844439\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.835373\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.885086\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.837403\n",
            "\n",
            "Test set: Test loss: 0.9949, Accuracy: 3363/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.828414\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.821775\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.836521\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.852773\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.834946\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.860470\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.824752\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.831487\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.851293\n",
            "\n",
            "Test set: Test loss: 0.9879, Accuracy: 3373/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.824592\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.840919\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.796110\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.824131\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.831275\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.826693\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.864116\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.848467\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.826713\n",
            "\n",
            "Test set: Test loss: 0.9948, Accuracy: 3380/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 38: accuracy = 67.6%\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.826948\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.821380\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.825163\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.808070\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.862409\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.830699\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.817311\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.824913\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.842712\n",
            "\n",
            "Test set: Test loss: 0.9662, Accuracy: 3356/5000 (67%)\n",
            "\n",
            "Better loss at Epoch 39: loss = 0.9662342005968094%\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.783496\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.799596\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.804598\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.792593\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.772480\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.830491\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.836639\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.840489\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.854891\n",
            "\n",
            "Test set: Test loss: 0.9765, Accuracy: 3415/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 40: accuracy = 68.3%\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.798422\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.830619\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.833209\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.822999\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.789253\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.788769\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.792331\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.820563\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.823169\n",
            "\n",
            "Test set: Test loss: 0.9465, Accuracy: 3434/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 41: accuracy = 68.68%\n",
            "Better loss at Epoch 41: loss = 0.9464777815341949%\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.795500\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.830243\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.795279\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.782892\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.824823\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.858840\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.820913\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.790272\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.810216\n",
            "\n",
            "Test set: Test loss: 0.9923, Accuracy: 3365/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.815308\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.808653\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.780192\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.771529\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.783157\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.788288\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.818999\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.833139\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.825210\n",
            "\n",
            "Test set: Test loss: 0.9481, Accuracy: 3423/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.769689\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.755293\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.809721\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.811643\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.799513\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.790166\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.800528\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.788587\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.866227\n",
            "\n",
            "Test set: Test loss: 0.9538, Accuracy: 3374/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.758291\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.782573\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.791358\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.822767\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.827474\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.803082\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.815948\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.829899\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.796950\n",
            "\n",
            "Test set: Test loss: 1.0084, Accuracy: 3330/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.776621\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.792681\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.783802\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.791119\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.762355\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.772445\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.776082\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.798721\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.778934\n",
            "\n",
            "Test set: Test loss: 0.9635, Accuracy: 3398/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.749001\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.793848\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.811030\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.754780\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.747866\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.780009\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.780320\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.791471\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.770649\n",
            "\n",
            "Test set: Test loss: 0.9990, Accuracy: 3366/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.729091\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.746805\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.740002\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.776770\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.785500\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.792158\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.798699\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.796306\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.769555\n",
            "\n",
            "Test set: Test loss: 0.9511, Accuracy: 3405/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.771738\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.781229\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.750796\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.779352\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.781012\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.748323\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.799725\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.773283\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.772275\n",
            "\n",
            "Test set: Test loss: 0.9469, Accuracy: 3453/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 49: accuracy = 69.06%\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.716987\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.751237\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.744183\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.769628\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.786651\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.802041\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.782676\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.748374\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.827732\n",
            "\n",
            "Test set: Test loss: 0.9314, Accuracy: 3447/5000 (69%)\n",
            "\n",
            "Better loss at Epoch 50: loss = 0.9314239335060119%\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.779845\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.713819\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.777901\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.801398\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.758426\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.765728\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.760217\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.771992\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.818050\n",
            "\n",
            "Test set: Test loss: 0.9555, Accuracy: 3494/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 51: accuracy = 69.88%\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.766391\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.774411\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.778368\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.762343\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.775095\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.771975\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.808849\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.829098\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.785904\n",
            "\n",
            "Test set: Test loss: 0.9118, Accuracy: 3484/5000 (70%)\n",
            "\n",
            "Better loss at Epoch 52: loss = 0.9118266981840134%\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.706354\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.765304\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.733055\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.746260\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.786115\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.810391\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.781544\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.769645\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.773810\n",
            "\n",
            "Test set: Test loss: 0.9204, Accuracy: 3506/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 53: accuracy = 70.12%\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.742575\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.746947\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.769336\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.760645\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.737509\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.724920\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.782805\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.771177\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.802537\n",
            "\n",
            "Test set: Test loss: 1.0155, Accuracy: 3359/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.738297\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.767803\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.747455\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.749988\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.763574\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.772442\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.792482\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.761224\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.776699\n",
            "\n",
            "Test set: Test loss: 0.9480, Accuracy: 3463/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.745692\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.743533\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.715214\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.742597\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.759096\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.717176\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.799555\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.754879\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.745518\n",
            "\n",
            "Test set: Test loss: 0.9565, Accuracy: 3415/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.711315\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.727000\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.717684\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.737046\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.763459\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.758564\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.739747\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.762120\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.760440\n",
            "\n",
            "Test set: Test loss: 0.9941, Accuracy: 3437/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.738698\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.744908\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.729312\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.774467\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.703049\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.754463\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.775054\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.782009\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.787777\n",
            "\n",
            "Test set: Test loss: 0.9601, Accuracy: 3414/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.733858\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.763905\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.754957\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.729750\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.781839\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.715051\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.794980\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.769426\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.797631\n",
            "\n",
            "Test set: Test loss: 0.9799, Accuracy: 3408/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.750016\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.736041\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.759489\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.782741\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.746164\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.721993\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.756336\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.756811\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.767900\n",
            "\n",
            "Test set: Test loss: 0.9604, Accuracy: 3464/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.725297\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.710798\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.824209\n",
            "Train Epoch: 61 [20000/50000 (40%)]\tTrain Loss: 0.797261\n",
            "Train Epoch: 61 [25000/50000 (50%)]\tTrain Loss: 0.782305\n",
            "Train Epoch: 61 [30000/50000 (60%)]\tTrain Loss: 0.758631\n",
            "Train Epoch: 61 [35000/50000 (70%)]\tTrain Loss: 0.821772\n",
            "Train Epoch: 61 [40000/50000 (80%)]\tTrain Loss: 0.823842\n",
            "Train Epoch: 61 [45000/50000 (90%)]\tTrain Loss: 0.812779\n",
            "\n",
            "Test set: Test loss: 1.0199, Accuracy: 3421/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 62: lr = 0.1\n",
            "Train Epoch: 62 [5000/50000 (10%)]\tTrain Loss: 0.743120\n",
            "Train Epoch: 62 [10000/50000 (20%)]\tTrain Loss: 0.724213\n",
            "Train Epoch: 62 [15000/50000 (30%)]\tTrain Loss: 0.746341\n",
            "Train Epoch: 62 [20000/50000 (40%)]\tTrain Loss: 0.811385\n",
            "Train Epoch: 62 [25000/50000 (50%)]\tTrain Loss: 0.796433\n",
            "Train Epoch: 62 [30000/50000 (60%)]\tTrain Loss: 0.773268\n",
            "Train Epoch: 62 [35000/50000 (70%)]\tTrain Loss: 0.786412\n",
            "Train Epoch: 62 [40000/50000 (80%)]\tTrain Loss: 0.708197\n",
            "Train Epoch: 62 [45000/50000 (90%)]\tTrain Loss: 0.755735\n",
            "\n",
            "Test set: Test loss: 0.9747, Accuracy: 3500/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 63: lr = 0.1\n",
            "Train Epoch: 63 [5000/50000 (10%)]\tTrain Loss: 0.750049\n",
            "Train Epoch: 63 [10000/50000 (20%)]\tTrain Loss: 0.720656\n",
            "Train Epoch: 63 [15000/50000 (30%)]\tTrain Loss: 0.751768\n",
            "Train Epoch: 63 [20000/50000 (40%)]\tTrain Loss: 0.729539\n",
            "Train Epoch: 63 [25000/50000 (50%)]\tTrain Loss: 0.760697\n",
            "Train Epoch: 63 [30000/50000 (60%)]\tTrain Loss: 0.761934\n",
            "Train Epoch: 63 [35000/50000 (70%)]\tTrain Loss: 0.757666\n",
            "Train Epoch: 63 [40000/50000 (80%)]\tTrain Loss: 0.774408\n",
            "Train Epoch: 63 [45000/50000 (90%)]\tTrain Loss: 0.733567\n",
            "\n",
            "Test set: Test loss: 0.9420, Accuracy: 3495/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 64: lr = 0.1\n",
            "Train Epoch: 64 [5000/50000 (10%)]\tTrain Loss: 0.746204\n",
            "Train Epoch: 64 [10000/50000 (20%)]\tTrain Loss: 0.737483\n",
            "Train Epoch: 64 [15000/50000 (30%)]\tTrain Loss: 0.747116\n",
            "Train Epoch: 64 [20000/50000 (40%)]\tTrain Loss: 0.710557\n",
            "Train Epoch: 64 [25000/50000 (50%)]\tTrain Loss: 0.746141\n",
            "Train Epoch: 64 [30000/50000 (60%)]\tTrain Loss: 0.755673\n",
            "Train Epoch: 64 [35000/50000 (70%)]\tTrain Loss: 0.732126\n",
            "Train Epoch: 64 [40000/50000 (80%)]\tTrain Loss: 0.743204\n",
            "Train Epoch: 64 [45000/50000 (90%)]\tTrain Loss: 0.769646\n",
            "\n",
            "Test set: Test loss: 0.9381, Accuracy: 3451/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 65: lr = 0.1\n",
            "Train Epoch: 65 [5000/50000 (10%)]\tTrain Loss: 0.750680\n",
            "Train Epoch: 65 [10000/50000 (20%)]\tTrain Loss: 0.762042\n",
            "Train Epoch: 65 [15000/50000 (30%)]\tTrain Loss: 0.711136\n",
            "Train Epoch: 65 [20000/50000 (40%)]\tTrain Loss: 0.775050\n",
            "Train Epoch: 65 [25000/50000 (50%)]\tTrain Loss: 0.746339\n",
            "Train Epoch: 65 [30000/50000 (60%)]\tTrain Loss: 0.754145\n",
            "Train Epoch: 65 [35000/50000 (70%)]\tTrain Loss: 0.711987\n",
            "Train Epoch: 65 [40000/50000 (80%)]\tTrain Loss: 0.792285\n",
            "Train Epoch: 65 [45000/50000 (90%)]\tTrain Loss: 0.780766\n",
            "\n",
            "Test set: Test loss: 0.9589, Accuracy: 3417/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 66: lr = 0.1\n",
            "Train Epoch: 66 [5000/50000 (10%)]\tTrain Loss: 0.751598\n",
            "Train Epoch: 66 [10000/50000 (20%)]\tTrain Loss: 0.716750\n",
            "Train Epoch: 66 [15000/50000 (30%)]\tTrain Loss: 0.757424\n",
            "Train Epoch: 66 [20000/50000 (40%)]\tTrain Loss: 0.763508\n",
            "Train Epoch: 66 [25000/50000 (50%)]\tTrain Loss: 0.798108\n",
            "Train Epoch: 66 [30000/50000 (60%)]\tTrain Loss: 0.825634\n",
            "Train Epoch: 66 [35000/50000 (70%)]\tTrain Loss: 0.758503\n",
            "Train Epoch: 66 [40000/50000 (80%)]\tTrain Loss: 0.800987\n",
            "Train Epoch: 66 [45000/50000 (90%)]\tTrain Loss: 0.813844\n",
            "\n",
            "Test set: Test loss: 0.9775, Accuracy: 3422/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 67: lr = 0.1\n",
            "Train Epoch: 67 [5000/50000 (10%)]\tTrain Loss: 0.847371\n",
            "Train Epoch: 67 [10000/50000 (20%)]\tTrain Loss: 0.755859\n",
            "Train Epoch: 67 [15000/50000 (30%)]\tTrain Loss: 0.839460\n",
            "Train Epoch: 67 [20000/50000 (40%)]\tTrain Loss: 0.865800\n",
            "Train Epoch: 67 [25000/50000 (50%)]\tTrain Loss: 0.853901\n",
            "Train Epoch: 67 [30000/50000 (60%)]\tTrain Loss: 0.845821\n",
            "Train Epoch: 67 [35000/50000 (70%)]\tTrain Loss: 0.818316\n",
            "Train Epoch: 67 [40000/50000 (80%)]\tTrain Loss: 0.766559\n",
            "Train Epoch: 67 [45000/50000 (90%)]\tTrain Loss: 0.802059\n",
            "\n",
            "Test set: Test loss: 0.9576, Accuracy: 3460/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 68: lr = 0.1\n",
            "Train Epoch: 68 [5000/50000 (10%)]\tTrain Loss: 0.752549\n",
            "Train Epoch: 68 [10000/50000 (20%)]\tTrain Loss: 0.778297\n",
            "Train Epoch: 68 [15000/50000 (30%)]\tTrain Loss: 0.785139\n",
            "Train Epoch: 68 [20000/50000 (40%)]\tTrain Loss: 0.771610\n",
            "Train Epoch: 68 [25000/50000 (50%)]\tTrain Loss: 0.768591\n",
            "Train Epoch: 68 [30000/50000 (60%)]\tTrain Loss: 0.764206\n",
            "Train Epoch: 68 [35000/50000 (70%)]\tTrain Loss: 0.765621\n",
            "Train Epoch: 68 [40000/50000 (80%)]\tTrain Loss: 0.802794\n",
            "Train Epoch: 68 [45000/50000 (90%)]\tTrain Loss: 0.783992\n",
            "\n",
            "Test set: Test loss: 0.9741, Accuracy: 3422/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 69: lr = 0.1\n",
            "Train Epoch: 69 [5000/50000 (10%)]\tTrain Loss: 0.738965\n",
            "Train Epoch: 69 [10000/50000 (20%)]\tTrain Loss: 0.757682\n",
            "Train Epoch: 69 [15000/50000 (30%)]\tTrain Loss: 0.790803\n",
            "Train Epoch: 69 [20000/50000 (40%)]\tTrain Loss: 0.818160\n",
            "Train Epoch: 69 [25000/50000 (50%)]\tTrain Loss: 0.758378\n",
            "Train Epoch: 69 [30000/50000 (60%)]\tTrain Loss: 0.820379\n",
            "Train Epoch: 69 [35000/50000 (70%)]\tTrain Loss: 0.803784\n",
            "Train Epoch: 69 [40000/50000 (80%)]\tTrain Loss: 0.753918\n",
            "Train Epoch: 69 [45000/50000 (90%)]\tTrain Loss: 0.737588\n",
            "\n",
            "Test set: Test loss: 0.9509, Accuracy: 3455/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 70: lr = 0.1\n",
            "Train Epoch: 70 [5000/50000 (10%)]\tTrain Loss: 0.736078\n",
            "Train Epoch: 70 [10000/50000 (20%)]\tTrain Loss: 0.733919\n",
            "Train Epoch: 70 [15000/50000 (30%)]\tTrain Loss: 0.762526\n",
            "Train Epoch: 70 [20000/50000 (40%)]\tTrain Loss: 0.819380\n",
            "Train Epoch: 70 [25000/50000 (50%)]\tTrain Loss: 0.701396\n",
            "Train Epoch: 70 [30000/50000 (60%)]\tTrain Loss: 0.771933\n",
            "Train Epoch: 70 [35000/50000 (70%)]\tTrain Loss: 0.810778\n",
            "Train Epoch: 70 [40000/50000 (80%)]\tTrain Loss: 0.762853\n",
            "Train Epoch: 70 [45000/50000 (90%)]\tTrain Loss: 0.731733\n",
            "\n",
            "Test set: Test loss: 0.9638, Accuracy: 3478/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 71: lr = 0.1\n",
            "Train Epoch: 71 [5000/50000 (10%)]\tTrain Loss: 0.759496\n",
            "Train Epoch: 71 [10000/50000 (20%)]\tTrain Loss: 0.765736\n",
            "Train Epoch: 71 [15000/50000 (30%)]\tTrain Loss: 0.772849\n",
            "Train Epoch: 71 [20000/50000 (40%)]\tTrain Loss: 0.785548\n",
            "Train Epoch: 71 [25000/50000 (50%)]\tTrain Loss: 0.779986\n",
            "Train Epoch: 71 [30000/50000 (60%)]\tTrain Loss: 0.730067\n",
            "Train Epoch: 71 [35000/50000 (70%)]\tTrain Loss: 0.748113\n",
            "Train Epoch: 71 [40000/50000 (80%)]\tTrain Loss: 0.791506\n",
            "Train Epoch: 71 [45000/50000 (90%)]\tTrain Loss: 0.765932\n",
            "\n",
            "Test set: Test loss: 0.9636, Accuracy: 3442/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 72: lr = 0.1\n",
            "Train Epoch: 72 [5000/50000 (10%)]\tTrain Loss: 0.741721\n",
            "Train Epoch: 72 [10000/50000 (20%)]\tTrain Loss: 0.792888\n",
            "Train Epoch: 72 [15000/50000 (30%)]\tTrain Loss: 0.820755\n",
            "Train Epoch: 72 [20000/50000 (40%)]\tTrain Loss: 0.813380\n",
            "Train Epoch: 72 [25000/50000 (50%)]\tTrain Loss: 0.799564\n",
            "Train Epoch: 72 [30000/50000 (60%)]\tTrain Loss: 0.862299\n",
            "Train Epoch: 72 [35000/50000 (70%)]\tTrain Loss: 0.801825\n",
            "Train Epoch: 72 [40000/50000 (80%)]\tTrain Loss: 0.791274\n",
            "Train Epoch: 72 [45000/50000 (90%)]\tTrain Loss: 0.763532\n",
            "\n",
            "Test set: Test loss: 1.0675, Accuracy: 3333/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 73: lr = 0.1\n",
            "Train Epoch: 73 [5000/50000 (10%)]\tTrain Loss: 0.754151\n",
            "Train Epoch: 73 [10000/50000 (20%)]\tTrain Loss: 0.722584\n",
            "Train Epoch: 73 [15000/50000 (30%)]\tTrain Loss: 0.795157\n",
            "Train Epoch: 73 [20000/50000 (40%)]\tTrain Loss: 0.855102\n",
            "Train Epoch: 73 [25000/50000 (50%)]\tTrain Loss: 1.184917\n",
            "Train Epoch: 73 [30000/50000 (60%)]\tTrain Loss: 0.946139\n",
            "Train Epoch: 73 [35000/50000 (70%)]\tTrain Loss: 0.884426\n",
            "Train Epoch: 73 [40000/50000 (80%)]\tTrain Loss: 0.850017\n",
            "Train Epoch: 73 [45000/50000 (90%)]\tTrain Loss: 0.794729\n",
            "\n",
            "Test set: Test loss: 0.9714, Accuracy: 3425/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 74: lr = 0.1\n",
            "Train Epoch: 74 [5000/50000 (10%)]\tTrain Loss: 0.745613\n",
            "Train Epoch: 74 [10000/50000 (20%)]\tTrain Loss: 0.775157\n",
            "Train Epoch: 74 [15000/50000 (30%)]\tTrain Loss: 0.769996\n",
            "Train Epoch: 74 [20000/50000 (40%)]\tTrain Loss: 0.818082\n",
            "Train Epoch: 74 [25000/50000 (50%)]\tTrain Loss: 0.813171\n",
            "Train Epoch: 74 [30000/50000 (60%)]\tTrain Loss: 0.759436\n",
            "Train Epoch: 74 [35000/50000 (70%)]\tTrain Loss: 0.790222\n",
            "Train Epoch: 74 [40000/50000 (80%)]\tTrain Loss: 0.802038\n",
            "Train Epoch: 74 [45000/50000 (90%)]\tTrain Loss: 0.823293\n",
            "\n",
            "Test set: Test loss: 0.9839, Accuracy: 3383/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 75: lr = 0.1\n",
            "Train Epoch: 75 [5000/50000 (10%)]\tTrain Loss: 0.791679\n",
            "Train Epoch: 75 [10000/50000 (20%)]\tTrain Loss: 0.797096\n",
            "Train Epoch: 75 [15000/50000 (30%)]\tTrain Loss: 0.789610\n",
            "Train Epoch: 75 [20000/50000 (40%)]\tTrain Loss: 0.889345\n",
            "Train Epoch: 75 [25000/50000 (50%)]\tTrain Loss: 0.880487\n",
            "Train Epoch: 75 [30000/50000 (60%)]\tTrain Loss: 0.908585\n",
            "Train Epoch: 75 [35000/50000 (70%)]\tTrain Loss: 0.878882\n",
            "Train Epoch: 75 [40000/50000 (80%)]\tTrain Loss: 0.852521\n",
            "Train Epoch: 75 [45000/50000 (90%)]\tTrain Loss: 0.813737\n",
            "\n",
            "Test set: Test loss: 1.0203, Accuracy: 3409/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 76: lr = 0.1\n",
            "Train Epoch: 76 [5000/50000 (10%)]\tTrain Loss: 0.848098\n",
            "Train Epoch: 76 [10000/50000 (20%)]\tTrain Loss: 0.801164\n",
            "Train Epoch: 76 [15000/50000 (30%)]\tTrain Loss: 0.881382\n",
            "Train Epoch: 76 [20000/50000 (40%)]\tTrain Loss: 0.914751\n",
            "Train Epoch: 76 [25000/50000 (50%)]\tTrain Loss: 0.874285\n",
            "Train Epoch: 76 [30000/50000 (60%)]\tTrain Loss: 0.883250\n",
            "Train Epoch: 76 [35000/50000 (70%)]\tTrain Loss: 0.839413\n",
            "Train Epoch: 76 [40000/50000 (80%)]\tTrain Loss: 0.852430\n",
            "Train Epoch: 76 [45000/50000 (90%)]\tTrain Loss: 0.842353\n",
            "\n",
            "Test set: Test loss: 0.9794, Accuracy: 3386/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 77: lr = 0.1\n",
            "Train Epoch: 77 [5000/50000 (10%)]\tTrain Loss: 0.900536\n",
            "Train Epoch: 77 [10000/50000 (20%)]\tTrain Loss: 0.854555\n",
            "Train Epoch: 77 [15000/50000 (30%)]\tTrain Loss: 0.903790\n",
            "Train Epoch: 77 [20000/50000 (40%)]\tTrain Loss: 0.851811\n",
            "Train Epoch: 77 [25000/50000 (50%)]\tTrain Loss: 0.859979\n",
            "Train Epoch: 77 [30000/50000 (60%)]\tTrain Loss: 0.933186\n",
            "Train Epoch: 77 [35000/50000 (70%)]\tTrain Loss: 0.878870\n",
            "Train Epoch: 77 [40000/50000 (80%)]\tTrain Loss: 0.885354\n",
            "Train Epoch: 77 [45000/50000 (90%)]\tTrain Loss: 0.916980\n",
            "\n",
            "Test set: Test loss: 1.0465, Accuracy: 3281/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 78: lr = 0.1\n",
            "Train Epoch: 78 [5000/50000 (10%)]\tTrain Loss: 0.923653\n",
            "Train Epoch: 78 [10000/50000 (20%)]\tTrain Loss: 0.945672\n",
            "Train Epoch: 78 [15000/50000 (30%)]\tTrain Loss: 0.886001\n",
            "Train Epoch: 78 [20000/50000 (40%)]\tTrain Loss: 0.926775\n",
            "Train Epoch: 78 [25000/50000 (50%)]\tTrain Loss: 0.973464\n",
            "Train Epoch: 78 [30000/50000 (60%)]\tTrain Loss: 0.970274\n",
            "Train Epoch: 78 [35000/50000 (70%)]\tTrain Loss: 0.933985\n",
            "Train Epoch: 78 [40000/50000 (80%)]\tTrain Loss: 1.018874\n",
            "Train Epoch: 78 [45000/50000 (90%)]\tTrain Loss: 1.069283\n",
            "\n",
            "Test set: Test loss: 1.0461, Accuracy: 3256/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 79: lr = 0.1\n",
            "Train Epoch: 79 [5000/50000 (10%)]\tTrain Loss: 0.931665\n",
            "Train Epoch: 79 [10000/50000 (20%)]\tTrain Loss: 0.942065\n",
            "Train Epoch: 79 [15000/50000 (30%)]\tTrain Loss: 0.931031\n",
            "Train Epoch: 79 [20000/50000 (40%)]\tTrain Loss: 0.887138\n",
            "Train Epoch: 79 [25000/50000 (50%)]\tTrain Loss: 0.846107\n",
            "Train Epoch: 79 [30000/50000 (60%)]\tTrain Loss: 0.864650\n",
            "Train Epoch: 79 [35000/50000 (70%)]\tTrain Loss: 0.881610\n",
            "Train Epoch: 79 [40000/50000 (80%)]\tTrain Loss: 0.835451\n",
            "Train Epoch: 79 [45000/50000 (90%)]\tTrain Loss: 0.877249\n",
            "\n",
            "Test set: Test loss: 0.9791, Accuracy: 3397/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 80: lr = 0.1\n",
            "Train Epoch: 80 [5000/50000 (10%)]\tTrain Loss: 0.878868\n",
            "Train Epoch: 80 [10000/50000 (20%)]\tTrain Loss: 0.874917\n",
            "Train Epoch: 80 [15000/50000 (30%)]\tTrain Loss: 0.963791\n",
            "Train Epoch: 80 [20000/50000 (40%)]\tTrain Loss: 1.001976\n",
            "Train Epoch: 80 [25000/50000 (50%)]\tTrain Loss: 0.997252\n",
            "Train Epoch: 80 [30000/50000 (60%)]\tTrain Loss: 0.983979\n",
            "Train Epoch: 80 [35000/50000 (70%)]\tTrain Loss: 0.935002\n",
            "Train Epoch: 80 [40000/50000 (80%)]\tTrain Loss: 1.009411\n",
            "Train Epoch: 80 [45000/50000 (90%)]\tTrain Loss: 0.963965\n",
            "\n",
            "Test set: Test loss: 1.0717, Accuracy: 3268/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 81: lr = 0.1\n",
            "Train Epoch: 81 [5000/50000 (10%)]\tTrain Loss: 0.950545\n",
            "Train Epoch: 81 [10000/50000 (20%)]\tTrain Loss: 1.007718\n",
            "Train Epoch: 81 [15000/50000 (30%)]\tTrain Loss: 0.960325\n",
            "Train Epoch: 81 [20000/50000 (40%)]\tTrain Loss: 0.945645\n",
            "Train Epoch: 81 [25000/50000 (50%)]\tTrain Loss: 0.866618\n",
            "Train Epoch: 81 [30000/50000 (60%)]\tTrain Loss: 0.864169\n",
            "Train Epoch: 81 [35000/50000 (70%)]\tTrain Loss: 0.928138\n",
            "Train Epoch: 81 [40000/50000 (80%)]\tTrain Loss: 0.990163\n",
            "Train Epoch: 81 [45000/50000 (90%)]\tTrain Loss: 0.984292\n",
            "\n",
            "Test set: Test loss: 1.1072, Accuracy: 3190/5000 (64%)\n",
            "\n",
            "\n",
            "Train Epoch 82: lr = 0.1\n",
            "Train Epoch: 82 [5000/50000 (10%)]\tTrain Loss: 0.973541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-34a24e5c9987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmodel = squeezenet1_0(num_classes=10)\\nmodel = model.to(device=device, dtype=torch.float)\\n\\n# Cross Entropy Loss \\nerror = CrossEntropyLoss().to(device=device, dtype=torch.float)\\n\\n#Optimizer\\nlearning_rate = 0.1\\noptimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\\n\\n#Optimizer adam\\n# learning_rate = 0.04\\n# optimizer = Adam(model.parameters(), lr=learning_rate)\\n# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\\n# optimizer = SGD(model.parameters(), lr=learning_rate)\\n\\n#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    sta...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LtLRFKQNVWA",
        "colab_type": "code",
        "outputId": "91cebf5b-95ed-4115-fb2e-b26d1b1aea19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 26.48614233373168 seconds\n",
            "Best accuracy: 70.12  Best training loss: 0.4895196557044983  Best validation loss: 0.9118266981840134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GL1CfnuOjIQ",
        "colab_type": "text"
      },
      "source": [
        "#### removing fire layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsFTj-VPOliV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        # self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        # self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        # self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                # nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "            # self.features = nn.Sequential(\n",
        "            #     nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            #     nn.ReLU(inplace=True),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(96, 16, 64, 64),\n",
        "            #     Fire(128, 16, 64, 64),\n",
        "            #     Fire(128, 32, 128, 128),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(256, 32, 128, 128),\n",
        "            #     Fire(256, 48, 192, 192),\n",
        "            #     Fire(384, 48, 192, 192),\n",
        "            #     Fire(384, 64, 256, 256),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "                # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),t\n",
        "            # )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q3_fCWDOy6y",
        "colab_type": "code",
        "outputId": "d191df52-fecb-46bd-a7ab-b731dd6f3718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.288738\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.231754\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.222408\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.209078\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.172160\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.168398\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.173353\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.149965\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 2.124120\n",
            "\n",
            "Test set: Test loss: 2.1001, Accuracy: 1268/5000 (25%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 25.36%\n",
            "Better loss at Epoch 0: loss = 2.1001385867595674%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 2.103331\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 2.099842\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 2.107093\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 2.100324\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 2.086656\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 2.084029\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 2.064250\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 2.070258\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 2.125193\n",
            "\n",
            "Test set: Test loss: 2.0409, Accuracy: 1421/5000 (28%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 28.42%\n",
            "Better loss at Epoch 1: loss = 2.040889205932617%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 2.052638\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 2.055627\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 2.042764\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 2.046747\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 2.036827\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 2.023954\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 2.033938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-34a24e5c9987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmodel = squeezenet1_0(num_classes=10)\\nmodel = model.to(device=device, dtype=torch.float)\\n\\n# Cross Entropy Loss \\nerror = CrossEntropyLoss().to(device=device, dtype=torch.float)\\n\\n#Optimizer\\nlearning_rate = 0.1\\noptimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\\n\\n#Optimizer adam\\n# learning_rate = 0.04\\n# optimizer = Adam(model.parameters(), lr=learning_rate)\\n# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\\n# optimizer = SGD(model.parameters(), lr=learning_rate)\\n\\n#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    sta...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsbound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3rj8xsKPJME",
        "colab_type": "code",
        "outputId": "15a6420c-41ee-40be-a761-24cd6c40dec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet Fire Layers Removed (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet Fire Layers Removed (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhvx/Rh15hN+9NYEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 18.847031471520005 seconds\n",
            "Best accuracy: 70.34  Best training loss: 0.43348202109336853  Best validation loss: 0.9575561422109604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6YZ6iTlRKrN",
        "colab_type": "text"
      },
      "source": [
        "#### removing fire modules with batchnorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKt8bsubD1Lw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQgrVnwkRNb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "            # self.features = nn.Sequential(\n",
        "            #     nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            #     nn.ReLU(inplace=True),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(96, 16, 64, 64),\n",
        "            #     Fire(128, 16, 64, 64),\n",
        "            #     Fire(128, 32, 128, 128),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(256, 32, 128, 128),\n",
        "            #     Fire(256, 48, 192, 192),\n",
        "            #     Fire(384, 48, 192, 192),\n",
        "            #     Fire(384, 64, 256, 256),\n",
        "            #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "                # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            #     Fire(512, 64, 256, 256),\n",
        "            #     Fire(512, 64, 256, 256),t\n",
        "            # )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMxmZ48BRQ_f",
        "colab_type": "code",
        "outputId": "00aacc40-bf78-4076-86a7-f294e86250fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.294660\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.225884\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.161388\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.100973\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.085238\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.071102\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.047601\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.004573\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 2.014063\n",
            "\n",
            "Test set: Test loss: 2.0084, Accuracy: 1336/5000 (27%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 26.72%\n",
            "Better loss at Epoch 0: loss = 2.008350933790207%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.989136\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.948203\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.960391\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.941399\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.906229\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.910538\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.861625\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.893260\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.881348\n",
            "\n",
            "Test set: Test loss: 1.8607, Accuracy: 1774/5000 (35%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 35.48%\n",
            "Better loss at Epoch 1: loss = 1.860663239955902%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.868947\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.850857\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.842037\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.845236\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.838359\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.852344\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.843135\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.805966\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.818753\n",
            "\n",
            "Test set: Test loss: 1.8192, Accuracy: 1874/5000 (37%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 37.48%\n",
            "Better loss at Epoch 2: loss = 1.8191663241386413%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.817440\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.785431\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.820899\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.772023\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.788256\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.762686\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.750798\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.686297\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.666843\n",
            "\n",
            "Test set: Test loss: 1.6498, Accuracy: 2081/5000 (42%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 41.62%\n",
            "Better loss at Epoch 3: loss = 1.6498457074165345%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.680625\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.634862\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.650842\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.635108\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.666846\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.621914\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.607870\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.594318\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.586057\n",
            "\n",
            "Test set: Test loss: 1.5882, Accuracy: 2214/5000 (44%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 44.28%\n",
            "Better loss at Epoch 4: loss = 1.588205955028534%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.610793\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 1.595714\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.569447\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.568607\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.589155\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 1.563075\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 1.542215\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.574946\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 1.577836\n",
            "\n",
            "Test set: Test loss: 1.5137, Accuracy: 2495/5000 (50%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 49.9%\n",
            "Better loss at Epoch 5: loss = 1.5136576390266419%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 1.504617\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 1.428324\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 1.428958\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 1.452557\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 1.437895\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 1.452375\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 1.437350\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 1.456518\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 1.429022\n",
            "\n",
            "Test set: Test loss: 1.4696, Accuracy: 2541/5000 (51%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 50.82%\n",
            "Better loss at Epoch 6: loss = 1.469557328224182%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 1.429288\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 1.410370\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 1.445990\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 1.385250\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 1.411377\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 1.380625\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 1.263951\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 1.297150\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 1.292051\n",
            "\n",
            "Test set: Test loss: 1.2732, Accuracy: 2695/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 53.9%\n",
            "Better loss at Epoch 7: loss = 1.2732369887828827%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 1.243907\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 1.257018\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 1.252224\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 1.293839\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 1.215941\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 1.237116\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 1.268250\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 1.186307\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 1.205588\n",
            "\n",
            "Test set: Test loss: 1.2150, Accuracy: 2831/5000 (57%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 56.62%\n",
            "Better loss at Epoch 8: loss = 1.2150118148326874%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 1.202314\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 1.174772\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 1.206768\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 1.208699\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 1.186371\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 1.162319\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 1.170326\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 1.194675\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 1.156105\n",
            "\n",
            "Test set: Test loss: 1.1742, Accuracy: 2964/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 59.28%\n",
            "Better loss at Epoch 9: loss = 1.1741815954446793%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 1.126863\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 1.146755\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 1.160038\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 1.147230\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 1.133786\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 1.171698\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 1.084036\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 1.126016\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 1.123690\n",
            "\n",
            "Test set: Test loss: 1.1601, Accuracy: 2948/5000 (59%)\n",
            "\n",
            "Better loss at Epoch 10: loss = 1.1601122868061067%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 1.120960\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 1.060619\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 1.075392\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 1.105522\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 1.121298\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 1.108851\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 1.089382\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 1.097266\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 1.093559\n",
            "\n",
            "Test set: Test loss: 1.1267, Accuracy: 3070/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 61.4%\n",
            "Better loss at Epoch 11: loss = 1.1267191994190215%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 1.039969\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 1.106347\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 1.097715\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 1.096196\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 1.085346\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 1.075425\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 1.086582\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 1.040888\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 1.060686\n",
            "\n",
            "Test set: Test loss: 1.1090, Accuracy: 3091/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 61.82%\n",
            "Better loss at Epoch 12: loss = 1.1090163880586623%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 1.039156\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 1.026130\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 1.027940\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 1.066027\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 1.046669\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 1.038067\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 1.048720\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 1.040234\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 1.031548\n",
            "\n",
            "Test set: Test loss: 1.1054, Accuracy: 3084/5000 (62%)\n",
            "\n",
            "Better loss at Epoch 13: loss = 1.1053772741556167%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 1.008799\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 1.006867\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 1.037640\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 1.022087\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 1.016465\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 1.008997\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 1.011412\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 1.046418\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 1.006756\n",
            "\n",
            "Test set: Test loss: 1.0932, Accuracy: 3063/5000 (61%)\n",
            "\n",
            "Better loss at Epoch 14: loss = 1.0932427799701692%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.960221\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 1.003160\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 1.023733\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 1.004198\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.983645\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.990034\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 1.007688\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 1.047014\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.989436\n",
            "\n",
            "Test set: Test loss: 1.0433, Accuracy: 3187/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 63.74%\n",
            "Better loss at Epoch 15: loss = 1.0433043491840364%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.980578\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.999537\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.991153\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.964788\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.947009\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.955786\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.961410\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.973266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-34a24e5c9987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmodel = squeezenet1_0(num_classes=10)\\nmodel = model.to(device=device, dtype=torch.float)\\n\\n# Cross Entropy Loss \\nerror = CrossEntropyLoss().to(device=device, dtype=torch.float)\\n\\n#Optimizer\\nlearning_rate = 0.1\\noptimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\\n\\n#Optimizer adam\\n# learning_rate = 0.04\\n# optimizer = Adam(model.parameters(), lr=learning_rate)\\n# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\\n# optimizer = SGD(model.parameters(), lr=learning_rate)\\n\\n#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    sta...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mstep_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-50QTQbDoSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G2_NoHNRTj4",
        "colab_type": "code",
        "outputId": "493a2bb4-7246-4da6-dac5-5f27efc2b0c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet Fire Layers Removed (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet Fire Layers Removed (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYKhwjCEwGAyGCscYAoPBYKhw/j/13qVHfK6fUgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/hBCKDKR+EMIochE4g8hhCLz/0xt8bfBK2csAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 19.29093309547009 seconds\n",
            "Best accuracy: 70.44  Best training loss: 0.35574018955230713  Best validation loss: 0.9584466779232025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zkrPTmAUb6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# smaller network did not overfit as much"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeSW9sn4ahNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6DneZHD6VF",
        "colab_type": "text"
      },
      "source": [
        "### squeeze ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTc7OzLFD9xw",
        "colab_type": "text"
      },
      "source": [
        "#### 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg58_8AWD_wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 64, 64, 64),\n",
        "                Fire(128, 64, 64, 64),\n",
        "                Fire(128, 128, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 128, 128, 128),\n",
        "                Fire(256, 192, 192, 192),\n",
        "                Fire(384, 192, 192, 192),\n",
        "                Fire(384, 256, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 256, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzbz6strEFT3",
        "colab_type": "code",
        "outputId": "376d3a30-26ea-4066-b35a-c095ca715d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.148186\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.885410\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.779477\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.703525\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.632020\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.586779\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.537545\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.470280\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.436872\n",
            "\n",
            "Test set: Test loss: 1.3482, Accuracy: 2711/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 54.22%\n",
            "Better loss at Epoch 0: loss = 1.3482309436798097%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.346823\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.310505\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.333939\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.276143\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.235768\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.217277\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.213055\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.190170\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.178354\n",
            "\n",
            "Test set: Test loss: 1.1210, Accuracy: 3075/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 61.5%\n",
            "Better loss at Epoch 1: loss = 1.121025424003601%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.068819\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.107522\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.063389\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.067849\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.055189\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.025800\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.041585\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.030365\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.032538\n",
            "\n",
            "Test set: Test loss: 0.9692, Accuracy: 3308/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 66.16%\n",
            "Better loss at Epoch 2: loss = 0.9691879099607468%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 0.943951\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 0.931159\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 0.898670\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 0.923416\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.919904\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 0.896034\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 0.915502\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 0.890611\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.913734\n",
            "\n",
            "Test set: Test loss: 0.8912, Accuracy: 3477/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 69.54%\n",
            "Better loss at Epoch 3: loss = 0.8912218642234803%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.789704\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.789273\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.801703\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.840080\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.824164\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.780606\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.802068\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.812122\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.811570\n",
            "\n",
            "Test set: Test loss: 0.8699, Accuracy: 3476/5000 (70%)\n",
            "\n",
            "Better loss at Epoch 4: loss = 0.8699083393812179%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.703677\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.725421\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.694351\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.753368\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.714731\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.733871\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.722283\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.745126\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.729629\n",
            "\n",
            "Test set: Test loss: 0.8107, Accuracy: 3603/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 72.06%\n",
            "Better loss at Epoch 5: loss = 0.8106717318296432%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.612838\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.644165\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.639278\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.651927\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.669173\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.678645\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.675786\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.649041\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.687195\n",
            "\n",
            "Test set: Test loss: 0.7822, Accuracy: 3663/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 73.26%\n",
            "Better loss at Epoch 6: loss = 0.7822117635607719%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.538765\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.575565\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.577049\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.560826\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.594948\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.611730\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.633189\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.600291\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.611257\n",
            "\n",
            "Test set: Test loss: 0.8270, Accuracy: 3631/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.495857\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.501270\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.515784\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.517711\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.554483\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.565018\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.586165\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.554452\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.542503\n",
            "\n",
            "Test set: Test loss: 0.7706, Accuracy: 3704/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 74.08%\n",
            "Better loss at Epoch 8: loss = 0.7705767253041267%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.438965\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.475717\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.484691\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.487987\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.492894\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.487544\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.492172\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.502233\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.502212\n",
            "\n",
            "Test set: Test loss: 0.8196, Accuracy: 3677/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.377551\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.422236\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.433612\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.445088\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.459405\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.454303\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.479457\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.467448\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.463685\n",
            "\n",
            "Test set: Test loss: 0.7999, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.375666\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.368180\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.366367\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.405336\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.385090\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.401486\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.400853\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.407456\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.447358\n",
            "\n",
            "Test set: Test loss: 0.8012, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.308662\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.318447\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.346496\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.344717\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.361649\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.367838\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.371242\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.399790\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.371565\n",
            "\n",
            "Test set: Test loss: 0.8196, Accuracy: 3759/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 75.18%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.296595\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.280574\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.285837\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.336298\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.334816\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.342671\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.333870\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.351417\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.352076\n",
            "\n",
            "Test set: Test loss: 0.8391, Accuracy: 3743/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.245153\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.286846\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.297977\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.281800\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.308092\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.305771\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.318448\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.319382\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.334632\n",
            "\n",
            "Test set: Test loss: 0.8758, Accuracy: 3729/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.235297\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.240065\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.244215\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.255981\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.290720\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.253560\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.320592\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.309126\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.310699\n",
            "\n",
            "Test set: Test loss: 0.9004, Accuracy: 3712/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.209163\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.209925\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.216968\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.269388\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.258945\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.242464\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.288684\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.280269\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.286026\n",
            "\n",
            "Test set: Test loss: 0.8527, Accuracy: 3812/5000 (76%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 76.24%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.193281\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.206608\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.223644\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.226438\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.235999\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.221197\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.237524\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.225310\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.256795\n",
            "\n",
            "Test set: Test loss: 0.8837, Accuracy: 3779/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.194127\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.180223\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.205047\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.191371\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.204375\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.254873\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.232653\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.215734\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.233118\n",
            "\n",
            "Test set: Test loss: 0.9202, Accuracy: 3752/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.158326\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.145532\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.170254\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.186067\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.190326\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.170738\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.207962\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.232816\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.221644\n",
            "\n",
            "Test set: Test loss: 0.9649, Accuracy: 3723/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.146654\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.125856\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.178241\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.173620\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.172524\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.190963\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.177176\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.210061\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.182831\n",
            "\n",
            "Test set: Test loss: 0.9378, Accuracy: 3788/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.137425\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.152322\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.138942\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.154227\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.150461\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.172251\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.163361\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.177439\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.183388\n",
            "\n",
            "Test set: Test loss: 0.9744, Accuracy: 3752/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.129719\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.121363\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.141730\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.156937\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.153044\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.128833\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.150250\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.172014\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.175015\n",
            "\n",
            "Test set: Test loss: 0.9879, Accuracy: 3756/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.121003\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.098336\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.114103\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.123435\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.127912\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.120255\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.113258\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.137613\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.142627\n",
            "\n",
            "Test set: Test loss: 0.9759, Accuracy: 3772/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.110402\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.103358\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.113254\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.118030\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.131853\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.132454\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.142897\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.130592\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.135550\n",
            "\n",
            "Test set: Test loss: 1.0091, Accuracy: 3750/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.098111\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.105695\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.107525\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.115946\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.101059\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.116673\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.132571\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.136653\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.115964\n",
            "\n",
            "Test set: Test loss: 0.9966, Accuracy: 3778/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.111775\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.088546\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.093263\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.085823\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.101346\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.095054\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.115792\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.136350\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.110274\n",
            "\n",
            "Test set: Test loss: 1.0008, Accuracy: 3821/5000 (76%)\n",
            "\n",
            "Better accuracy at Epoch 26: accuracy = 76.42%\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.096624\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.107117\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.087911\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.092718\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.087805\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.099306\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.089812\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.103220\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.118157\n",
            "\n",
            "Test set: Test loss: 0.9786, Accuracy: 3831/5000 (77%)\n",
            "\n",
            "Better accuracy at Epoch 27: accuracy = 76.62%\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.084219\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.081295\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.081555\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.088378\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.094526\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.089630\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.090915\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.106113\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.093567\n",
            "\n",
            "Test set: Test loss: 1.0703, Accuracy: 3760/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.076872\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.088093\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.083160\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.086563\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.103403\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.090030\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.104448\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.086078\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.109873\n",
            "\n",
            "Test set: Test loss: 1.0129, Accuracy: 3820/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.059973\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.077076\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.097161\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.070584\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.078878\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.089693\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.076740\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.090840\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.089964\n",
            "\n",
            "Test set: Test loss: 1.0624, Accuracy: 3794/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.084857\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.061330\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.070262\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.077383\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.073612\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.080190\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.083677\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.082248\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.081600\n",
            "\n",
            "Test set: Test loss: 1.0579, Accuracy: 3819/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.060823\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.048694\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.067645\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.051141\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.080717\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.071817\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.073909\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.067718\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.080913\n",
            "\n",
            "Test set: Test loss: 1.0914, Accuracy: 3755/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.058431\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.066909\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.070049\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.078188\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.076808\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.063857\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.071070\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.077386\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.086941\n",
            "\n",
            "Test set: Test loss: 1.1075, Accuracy: 3774/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.075170\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.063348\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.059701\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.050198\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.060271\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.051325\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.054191\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.065806\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.067370\n",
            "\n",
            "Test set: Test loss: 1.1203, Accuracy: 3769/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.051246\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.040974\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.045453\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.047471\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.055378\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.051676\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.072926\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.057921\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.057653\n",
            "\n",
            "Test set: Test loss: 1.1137, Accuracy: 3759/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.051423\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.047400\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.065716\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.055449\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.050169\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.065820\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.058706\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.068330\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.066379\n",
            "\n",
            "Test set: Test loss: 1.0684, Accuracy: 3848/5000 (77%)\n",
            "\n",
            "Better accuracy at Epoch 36: accuracy = 76.96%\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.039191\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.041799\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.042875\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.062102\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.059083\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.047300\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.050195\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.051333\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.069588\n",
            "\n",
            "Test set: Test loss: 1.1131, Accuracy: 3825/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.042097\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.040858\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.043005\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.050822\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.044265\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.055976\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.063265\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.052610\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.058098\n",
            "\n",
            "Test set: Test loss: 1.1656, Accuracy: 3819/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.049648\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.034118\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.043037\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.040870\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.036442\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.046193\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.053523\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.055214\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.058529\n",
            "\n",
            "Test set: Test loss: 1.1264, Accuracy: 3832/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.039016\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.041641\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.053740\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.044737\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.055775\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.048969\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.052942\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.065581\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.048477\n",
            "\n",
            "Test set: Test loss: 1.1148, Accuracy: 3830/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.037231\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.044228\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.039000\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.035565\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.041083\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.047415\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.042528\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.044874\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.044934\n",
            "\n",
            "Test set: Test loss: 1.1569, Accuracy: 3804/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.027771\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.029503\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.036481\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.028018\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.037936\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.038810\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.042049\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.044907\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.044359\n",
            "\n",
            "Test set: Test loss: 1.1403, Accuracy: 3799/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.031370\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.032648\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.032743\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.041117\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.035790\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.040500\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.038365\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.025486\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.034363\n",
            "\n",
            "Test set: Test loss: 1.1547, Accuracy: 3807/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.038925\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.032866\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.023260\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.028759\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.043183\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.039532\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.046136\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.045526\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.033165\n",
            "\n",
            "Test set: Test loss: 1.1274, Accuracy: 3833/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.026709\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.024485\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.036949\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.023137\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.039687\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.036446\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.033768\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.043180\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.038018\n",
            "\n",
            "Test set: Test loss: 1.1567, Accuracy: 3871/5000 (77%)\n",
            "\n",
            "Better accuracy at Epoch 45: accuracy = 77.42%\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.029238\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.025111\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.026562\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.032942\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.039128\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.043108\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.038497\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.041929\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.044793\n",
            "\n",
            "Test set: Test loss: 1.1413, Accuracy: 3819/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.034696\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.028897\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.040060\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.031771\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.037741\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.033219\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.036970\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.041989\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.038848\n",
            "\n",
            "Test set: Test loss: 1.1567, Accuracy: 3842/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.021316\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.025944\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.023120\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.019921\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.027153\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.023695\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.019822\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.021028\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.023051\n",
            "\n",
            "Test set: Test loss: 1.1958, Accuracy: 3876/5000 (78%)\n",
            "\n",
            "Better accuracy at Epoch 48: accuracy = 77.52%\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.027776\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.039544\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.034558\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.028052\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.021665\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.027337\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.024999\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.033610\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.033355\n",
            "\n",
            "Test set: Test loss: 1.2197, Accuracy: 3834/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.024417\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.022991\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.021881\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.021669\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.021130\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.026310\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.033574\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.040015\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.040411\n",
            "\n",
            "Test set: Test loss: 1.1927, Accuracy: 3830/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.024961\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.025864\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.025834\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.021104\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.026763\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.026613\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.023911\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.038556\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.025151\n",
            "\n",
            "Test set: Test loss: 1.1673, Accuracy: 3859/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.030047\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.025372\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.021343\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.021716\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.025410\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.021721\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.024075\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.022038\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.028280\n",
            "\n",
            "Test set: Test loss: 1.1397, Accuracy: 3869/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.030139\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.017636\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.013650\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.020465\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.024886\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.032132\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.019685\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.029332\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.023786\n",
            "\n",
            "Test set: Test loss: 1.2092, Accuracy: 3850/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.021130\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.016615\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.019067\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.021618\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.021556\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.027448\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.019215\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.031218\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.022094\n",
            "\n",
            "Test set: Test loss: 1.2233, Accuracy: 3836/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.021412\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.014914\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.016004\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.017265\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.023123\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.020962\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.020169\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.027476\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.026243\n",
            "\n",
            "Test set: Test loss: 1.2303, Accuracy: 3819/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.018265\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.016627\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.024112\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.013230\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.026422\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.026370\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.020924\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.026377\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.041102\n",
            "\n",
            "Test set: Test loss: 1.2075, Accuracy: 3817/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.034280\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.022259\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.026120\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.019753\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.026629\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.021187\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.034044\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.026132\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.029653\n",
            "\n",
            "Test set: Test loss: 1.1955, Accuracy: 3833/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.019862\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.021059\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.016516\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.022285\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.013702\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.024349\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.020781\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.022878\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.014892\n",
            "\n",
            "Test set: Test loss: 1.2498, Accuracy: 3788/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.020654\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.017340\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.029685\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.029451\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.028126\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.025944\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.027765\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.033493\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.027274\n",
            "\n",
            "Test set: Test loss: 1.2018, Accuracy: 3830/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.023848\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.017498\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.019812\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.026990\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.021509\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.021795\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.031324\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.033389\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.020692\n",
            "\n",
            "Test set: Test loss: 1.2462, Accuracy: 3816/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.028744\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.012084\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.018419\n",
            "Train Epoch: 61 [20000/50000 (40%)]\tTrain Loss: 0.018461\n",
            "Train Epoch: 61 [25000/50000 (50%)]\tTrain Loss: 0.030184\n",
            "Train Epoch: 61 [30000/50000 (60%)]\tTrain Loss: 0.028286\n",
            "Train Epoch: 61 [35000/50000 (70%)]\tTrain Loss: 0.029352\n",
            "Train Epoch: 61 [40000/50000 (80%)]\tTrain Loss: 0.034327\n",
            "Train Epoch: 61 [45000/50000 (90%)]\tTrain Loss: 0.027420\n",
            "\n",
            "Test set: Test loss: 1.2344, Accuracy: 3836/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 62: lr = 0.1\n",
            "Train Epoch: 62 [5000/50000 (10%)]\tTrain Loss: 0.030932\n",
            "Train Epoch: 62 [10000/50000 (20%)]\tTrain Loss: 0.015249\n",
            "Train Epoch: 62 [15000/50000 (30%)]\tTrain Loss: 0.019029\n",
            "Train Epoch: 62 [20000/50000 (40%)]\tTrain Loss: 0.018365\n",
            "Train Epoch: 62 [25000/50000 (50%)]\tTrain Loss: 0.016336\n",
            "Train Epoch: 62 [30000/50000 (60%)]\tTrain Loss: 0.018360\n",
            "Train Epoch: 62 [35000/50000 (70%)]\tTrain Loss: 0.015864\n",
            "Train Epoch: 62 [40000/50000 (80%)]\tTrain Loss: 0.020988\n",
            "Train Epoch: 62 [45000/50000 (90%)]\tTrain Loss: 0.018820\n",
            "\n",
            "Test set: Test loss: 1.2325, Accuracy: 3851/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 63: lr = 0.1\n",
            "Train Epoch: 63 [5000/50000 (10%)]\tTrain Loss: 0.015887\n",
            "Train Epoch: 63 [10000/50000 (20%)]\tTrain Loss: 0.019885\n",
            "Train Epoch: 63 [15000/50000 (30%)]\tTrain Loss: 0.015536\n",
            "Train Epoch: 63 [20000/50000 (40%)]\tTrain Loss: 0.015296\n",
            "Train Epoch: 63 [25000/50000 (50%)]\tTrain Loss: 0.016819\n",
            "Train Epoch: 63 [30000/50000 (60%)]\tTrain Loss: 0.011392\n",
            "Train Epoch: 63 [35000/50000 (70%)]\tTrain Loss: 0.014214\n",
            "Train Epoch: 63 [40000/50000 (80%)]\tTrain Loss: 0.019552\n",
            "Train Epoch: 63 [45000/50000 (90%)]\tTrain Loss: 0.018884\n",
            "\n",
            "Test set: Test loss: 1.2787, Accuracy: 3851/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 64: lr = 0.1\n",
            "Train Epoch: 64 [5000/50000 (10%)]\tTrain Loss: 0.013780\n",
            "Train Epoch: 64 [10000/50000 (20%)]\tTrain Loss: 0.017207\n",
            "Train Epoch: 64 [15000/50000 (30%)]\tTrain Loss: 0.015884\n",
            "Train Epoch: 64 [20000/50000 (40%)]\tTrain Loss: 0.014793\n",
            "Train Epoch: 64 [25000/50000 (50%)]\tTrain Loss: 0.017776\n",
            "Train Epoch: 64 [30000/50000 (60%)]\tTrain Loss: 0.020795\n",
            "Train Epoch: 64 [35000/50000 (70%)]\tTrain Loss: 0.028796\n",
            "Train Epoch: 64 [40000/50000 (80%)]\tTrain Loss: 0.016927\n",
            "Train Epoch: 64 [45000/50000 (90%)]\tTrain Loss: 0.034777\n",
            "\n",
            "Test set: Test loss: 1.2297, Accuracy: 3862/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 65: lr = 0.1\n",
            "Train Epoch: 65 [5000/50000 (10%)]\tTrain Loss: 0.015389\n",
            "Train Epoch: 65 [10000/50000 (20%)]\tTrain Loss: 0.014850\n",
            "Train Epoch: 65 [15000/50000 (30%)]\tTrain Loss: 0.011487\n",
            "Train Epoch: 65 [20000/50000 (40%)]\tTrain Loss: 0.018712\n",
            "Train Epoch: 65 [25000/50000 (50%)]\tTrain Loss: 0.018008\n",
            "Train Epoch: 65 [30000/50000 (60%)]\tTrain Loss: 0.013449\n",
            "Train Epoch: 65 [35000/50000 (70%)]\tTrain Loss: 0.021270\n",
            "Train Epoch: 65 [40000/50000 (80%)]\tTrain Loss: 0.024658\n",
            "Train Epoch: 65 [45000/50000 (90%)]\tTrain Loss: 0.020207\n",
            "\n",
            "Test set: Test loss: 1.2548, Accuracy: 3845/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 66: lr = 0.1\n",
            "Train Epoch: 66 [5000/50000 (10%)]\tTrain Loss: 0.015235\n",
            "Train Epoch: 66 [10000/50000 (20%)]\tTrain Loss: 0.012853\n",
            "Train Epoch: 66 [15000/50000 (30%)]\tTrain Loss: 0.016670\n",
            "Train Epoch: 66 [20000/50000 (40%)]\tTrain Loss: 0.020161\n",
            "Train Epoch: 66 [25000/50000 (50%)]\tTrain Loss: 0.019192\n",
            "Train Epoch: 66 [30000/50000 (60%)]\tTrain Loss: 0.021807\n",
            "Train Epoch: 66 [35000/50000 (70%)]\tTrain Loss: 0.026093\n",
            "Train Epoch: 66 [40000/50000 (80%)]\tTrain Loss: 0.020705\n",
            "Train Epoch: 66 [45000/50000 (90%)]\tTrain Loss: 0.028374\n",
            "\n",
            "Test set: Test loss: 1.2657, Accuracy: 3845/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 67: lr = 0.1\n",
            "Train Epoch: 67 [5000/50000 (10%)]\tTrain Loss: 0.024214\n",
            "Train Epoch: 67 [10000/50000 (20%)]\tTrain Loss: 0.026610\n",
            "Train Epoch: 67 [15000/50000 (30%)]\tTrain Loss: 0.023304\n",
            "Train Epoch: 67 [20000/50000 (40%)]\tTrain Loss: 0.017187\n",
            "Train Epoch: 67 [25000/50000 (50%)]\tTrain Loss: 0.027606\n",
            "Train Epoch: 67 [30000/50000 (60%)]\tTrain Loss: 0.026834\n",
            "Train Epoch: 67 [35000/50000 (70%)]\tTrain Loss: 0.018268\n",
            "Train Epoch: 67 [40000/50000 (80%)]\tTrain Loss: 0.019802\n",
            "Train Epoch: 67 [45000/50000 (90%)]\tTrain Loss: 0.016911\n",
            "\n",
            "Test set: Test loss: 1.2515, Accuracy: 3831/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 68: lr = 0.1\n",
            "Train Epoch: 68 [5000/50000 (10%)]\tTrain Loss: 0.016266\n",
            "Train Epoch: 68 [10000/50000 (20%)]\tTrain Loss: 0.022070\n",
            "Train Epoch: 68 [15000/50000 (30%)]\tTrain Loss: 0.016496\n",
            "Train Epoch: 68 [20000/50000 (40%)]\tTrain Loss: 0.014322\n",
            "Train Epoch: 68 [25000/50000 (50%)]\tTrain Loss: 0.014767\n",
            "Train Epoch: 68 [30000/50000 (60%)]\tTrain Loss: 0.019758\n",
            "Train Epoch: 68 [35000/50000 (70%)]\tTrain Loss: 0.015470\n",
            "Train Epoch: 68 [40000/50000 (80%)]\tTrain Loss: 0.017563\n",
            "Train Epoch: 68 [45000/50000 (90%)]\tTrain Loss: 0.017124\n",
            "\n",
            "Test set: Test loss: 1.2550, Accuracy: 3836/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 69: lr = 0.1\n",
            "Train Epoch: 69 [5000/50000 (10%)]\tTrain Loss: 0.012042\n",
            "Train Epoch: 69 [10000/50000 (20%)]\tTrain Loss: 0.015042\n",
            "Train Epoch: 69 [15000/50000 (30%)]\tTrain Loss: 0.013714\n",
            "Train Epoch: 69 [20000/50000 (40%)]\tTrain Loss: 0.012708\n",
            "Train Epoch: 69 [25000/50000 (50%)]\tTrain Loss: 0.012953\n",
            "Train Epoch: 69 [30000/50000 (60%)]\tTrain Loss: 0.009987\n",
            "Train Epoch: 69 [35000/50000 (70%)]\tTrain Loss: 0.009611\n",
            "Train Epoch: 69 [40000/50000 (80%)]\tTrain Loss: 0.008326\n",
            "Train Epoch: 69 [45000/50000 (90%)]\tTrain Loss: 0.011983\n",
            "\n",
            "Test set: Test loss: 1.3484, Accuracy: 3806/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 70: lr = 0.1\n",
            "Train Epoch: 70 [5000/50000 (10%)]\tTrain Loss: 0.015490\n",
            "Train Epoch: 70 [10000/50000 (20%)]\tTrain Loss: 0.017760\n",
            "Train Epoch: 70 [15000/50000 (30%)]\tTrain Loss: 0.017732\n",
            "Train Epoch: 70 [20000/50000 (40%)]\tTrain Loss: 0.010919\n",
            "Train Epoch: 70 [25000/50000 (50%)]\tTrain Loss: 0.016112\n",
            "Train Epoch: 70 [30000/50000 (60%)]\tTrain Loss: 0.014156\n",
            "Train Epoch: 70 [35000/50000 (70%)]\tTrain Loss: 0.013433\n",
            "Train Epoch: 70 [40000/50000 (80%)]\tTrain Loss: 0.013230\n",
            "Train Epoch: 70 [45000/50000 (90%)]\tTrain Loss: 0.013686\n",
            "\n",
            "Test set: Test loss: 1.2399, Accuracy: 3858/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 71: lr = 0.1\n",
            "Train Epoch: 71 [5000/50000 (10%)]\tTrain Loss: 0.015620\n",
            "Train Epoch: 71 [10000/50000 (20%)]\tTrain Loss: 0.015599\n",
            "Train Epoch: 71 [15000/50000 (30%)]\tTrain Loss: 0.017196\n",
            "Train Epoch: 71 [20000/50000 (40%)]\tTrain Loss: 0.012838\n",
            "Train Epoch: 71 [25000/50000 (50%)]\tTrain Loss: 0.014529\n",
            "Train Epoch: 71 [30000/50000 (60%)]\tTrain Loss: 0.029365\n",
            "Train Epoch: 71 [35000/50000 (70%)]\tTrain Loss: 0.026313\n",
            "Train Epoch: 71 [40000/50000 (80%)]\tTrain Loss: 0.022694\n",
            "Train Epoch: 71 [45000/50000 (90%)]\tTrain Loss: 0.016458\n",
            "\n",
            "Test set: Test loss: 1.2521, Accuracy: 3846/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 72: lr = 0.1\n",
            "Train Epoch: 72 [5000/50000 (10%)]\tTrain Loss: 0.012309\n",
            "Train Epoch: 72 [10000/50000 (20%)]\tTrain Loss: 0.010659\n",
            "Train Epoch: 72 [15000/50000 (30%)]\tTrain Loss: 0.018172\n",
            "Train Epoch: 72 [20000/50000 (40%)]\tTrain Loss: 0.019114\n",
            "Train Epoch: 72 [25000/50000 (50%)]\tTrain Loss: 0.015861\n",
            "Train Epoch: 72 [30000/50000 (60%)]\tTrain Loss: 0.012573\n",
            "Train Epoch: 72 [35000/50000 (70%)]\tTrain Loss: 0.019986\n",
            "Train Epoch: 72 [40000/50000 (80%)]\tTrain Loss: 0.018438\n",
            "Train Epoch: 72 [45000/50000 (90%)]\tTrain Loss: 0.021207\n",
            "\n",
            "Test set: Test loss: 1.3077, Accuracy: 3815/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 73: lr = 0.1\n",
            "Train Epoch: 73 [5000/50000 (10%)]\tTrain Loss: 0.015827\n",
            "Train Epoch: 73 [10000/50000 (20%)]\tTrain Loss: 0.014839\n",
            "Train Epoch: 73 [15000/50000 (30%)]\tTrain Loss: 0.012506\n",
            "Train Epoch: 73 [20000/50000 (40%)]\tTrain Loss: 0.014489\n",
            "Train Epoch: 73 [25000/50000 (50%)]\tTrain Loss: 0.017306\n",
            "Train Epoch: 73 [30000/50000 (60%)]\tTrain Loss: 0.015512\n",
            "Train Epoch: 73 [35000/50000 (70%)]\tTrain Loss: 0.016999\n",
            "Train Epoch: 73 [40000/50000 (80%)]\tTrain Loss: 0.012138\n",
            "Train Epoch: 73 [45000/50000 (90%)]\tTrain Loss: 0.021594\n",
            "\n",
            "Test set: Test loss: 1.2764, Accuracy: 3846/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 74: lr = 0.1\n",
            "Train Epoch: 74 [5000/50000 (10%)]\tTrain Loss: 0.018032\n",
            "Train Epoch: 74 [10000/50000 (20%)]\tTrain Loss: 0.016273\n",
            "Train Epoch: 74 [15000/50000 (30%)]\tTrain Loss: 0.013304\n",
            "Train Epoch: 74 [20000/50000 (40%)]\tTrain Loss: 0.011323\n",
            "Train Epoch: 74 [25000/50000 (50%)]\tTrain Loss: 0.025378\n",
            "Train Epoch: 74 [30000/50000 (60%)]\tTrain Loss: 0.021709\n",
            "Train Epoch: 74 [35000/50000 (70%)]\tTrain Loss: 0.027434\n",
            "Train Epoch: 74 [40000/50000 (80%)]\tTrain Loss: 0.019880\n",
            "Train Epoch: 74 [45000/50000 (90%)]\tTrain Loss: 0.016757\n",
            "\n",
            "Test set: Test loss: 1.3029, Accuracy: 3826/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 75: lr = 0.1\n",
            "Train Epoch: 75 [5000/50000 (10%)]\tTrain Loss: 0.022618\n",
            "Train Epoch: 75 [10000/50000 (20%)]\tTrain Loss: 0.019386\n",
            "Train Epoch: 75 [15000/50000 (30%)]\tTrain Loss: 0.011723\n",
            "Train Epoch: 75 [20000/50000 (40%)]\tTrain Loss: 0.010458\n",
            "Train Epoch: 75 [25000/50000 (50%)]\tTrain Loss: 0.014121\n",
            "Train Epoch: 75 [30000/50000 (60%)]\tTrain Loss: 0.015138\n",
            "Train Epoch: 75 [35000/50000 (70%)]\tTrain Loss: 0.018225\n",
            "Train Epoch: 75 [40000/50000 (80%)]\tTrain Loss: 0.015674\n",
            "Train Epoch: 75 [45000/50000 (90%)]\tTrain Loss: 0.021459\n",
            "\n",
            "Test set: Test loss: 1.2508, Accuracy: 3856/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 76: lr = 0.1\n",
            "Train Epoch: 76 [5000/50000 (10%)]\tTrain Loss: 0.008390\n",
            "Train Epoch: 76 [10000/50000 (20%)]\tTrain Loss: 0.013113\n",
            "Train Epoch: 76 [15000/50000 (30%)]\tTrain Loss: 0.012162\n",
            "Train Epoch: 76 [20000/50000 (40%)]\tTrain Loss: 0.012839\n",
            "Train Epoch: 76 [25000/50000 (50%)]\tTrain Loss: 0.010808\n",
            "Train Epoch: 76 [30000/50000 (60%)]\tTrain Loss: 0.011319\n",
            "Train Epoch: 76 [35000/50000 (70%)]\tTrain Loss: 0.008486\n",
            "Train Epoch: 76 [40000/50000 (80%)]\tTrain Loss: 0.013615\n",
            "Train Epoch: 76 [45000/50000 (90%)]\tTrain Loss: 0.009444\n",
            "\n",
            "Test set: Test loss: 1.2960, Accuracy: 3865/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 77: lr = 0.1\n",
            "Train Epoch: 77 [5000/50000 (10%)]\tTrain Loss: 0.004225\n",
            "Train Epoch: 77 [10000/50000 (20%)]\tTrain Loss: 0.010594\n",
            "Train Epoch: 77 [15000/50000 (30%)]\tTrain Loss: 0.008741\n",
            "Train Epoch: 77 [20000/50000 (40%)]\tTrain Loss: 0.014361\n",
            "Train Epoch: 77 [25000/50000 (50%)]\tTrain Loss: 0.014817\n",
            "Train Epoch: 77 [30000/50000 (60%)]\tTrain Loss: 0.009956\n",
            "Train Epoch: 77 [35000/50000 (70%)]\tTrain Loss: 0.013047\n",
            "Train Epoch: 77 [40000/50000 (80%)]\tTrain Loss: 0.021658\n",
            "Train Epoch: 77 [45000/50000 (90%)]\tTrain Loss: 0.011004\n",
            "\n",
            "Test set: Test loss: 1.3137, Accuracy: 3824/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 78: lr = 0.1\n",
            "Train Epoch: 78 [5000/50000 (10%)]\tTrain Loss: 0.021165\n",
            "Train Epoch: 78 [10000/50000 (20%)]\tTrain Loss: 0.015422\n",
            "Train Epoch: 78 [15000/50000 (30%)]\tTrain Loss: 0.011796\n",
            "Train Epoch: 78 [20000/50000 (40%)]\tTrain Loss: 0.013621\n",
            "Train Epoch: 78 [25000/50000 (50%)]\tTrain Loss: 0.015344\n",
            "Train Epoch: 78 [30000/50000 (60%)]\tTrain Loss: 0.012721\n",
            "Train Epoch: 78 [35000/50000 (70%)]\tTrain Loss: 0.010006\n",
            "Train Epoch: 78 [40000/50000 (80%)]\tTrain Loss: 0.014009\n",
            "Train Epoch: 78 [45000/50000 (90%)]\tTrain Loss: 0.016082\n",
            "\n",
            "Test set: Test loss: 1.2906, Accuracy: 3851/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 79: lr = 0.1\n",
            "Train Epoch: 79 [5000/50000 (10%)]\tTrain Loss: 0.016902\n",
            "Train Epoch: 79 [10000/50000 (20%)]\tTrain Loss: 0.008680\n",
            "Train Epoch: 79 [15000/50000 (30%)]\tTrain Loss: 0.019031\n",
            "Train Epoch: 79 [20000/50000 (40%)]\tTrain Loss: 0.006687\n",
            "Train Epoch: 79 [25000/50000 (50%)]\tTrain Loss: 0.008957\n",
            "Train Epoch: 79 [30000/50000 (60%)]\tTrain Loss: 0.007378\n",
            "Train Epoch: 79 [35000/50000 (70%)]\tTrain Loss: 0.008333\n",
            "Train Epoch: 79 [40000/50000 (80%)]\tTrain Loss: 0.010405\n",
            "Train Epoch: 79 [45000/50000 (90%)]\tTrain Loss: 0.010116\n",
            "\n",
            "Test set: Test loss: 1.2727, Accuracy: 3877/5000 (78%)\n",
            "\n",
            "Better accuracy at Epoch 79: accuracy = 77.54%\n",
            "\n",
            "Train Epoch 80: lr = 0.1\n",
            "Train Epoch: 80 [5000/50000 (10%)]\tTrain Loss: 0.012199\n",
            "Train Epoch: 80 [10000/50000 (20%)]\tTrain Loss: 0.018126\n",
            "Train Epoch: 80 [15000/50000 (30%)]\tTrain Loss: 0.008809\n",
            "Train Epoch: 80 [20000/50000 (40%)]\tTrain Loss: 0.008183\n",
            "Train Epoch: 80 [25000/50000 (50%)]\tTrain Loss: 0.009823\n",
            "Train Epoch: 80 [30000/50000 (60%)]\tTrain Loss: 0.014140\n",
            "Train Epoch: 80 [35000/50000 (70%)]\tTrain Loss: 0.014790\n",
            "Train Epoch: 80 [40000/50000 (80%)]\tTrain Loss: 0.008454\n",
            "Train Epoch: 80 [45000/50000 (90%)]\tTrain Loss: 0.012684\n",
            "\n",
            "Test set: Test loss: 1.3173, Accuracy: 3850/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 81: lr = 0.1\n",
            "Train Epoch: 81 [5000/50000 (10%)]\tTrain Loss: 0.011369\n",
            "Train Epoch: 81 [10000/50000 (20%)]\tTrain Loss: 0.008279\n",
            "Train Epoch: 81 [15000/50000 (30%)]\tTrain Loss: 0.009175\n",
            "Train Epoch: 81 [20000/50000 (40%)]\tTrain Loss: 0.009981\n",
            "Train Epoch: 81 [25000/50000 (50%)]\tTrain Loss: 0.013398\n",
            "Train Epoch: 81 [30000/50000 (60%)]\tTrain Loss: 0.021257\n",
            "Train Epoch: 81 [35000/50000 (70%)]\tTrain Loss: 0.010321\n",
            "Train Epoch: 81 [40000/50000 (80%)]\tTrain Loss: 0.014148\n",
            "Train Epoch: 81 [45000/50000 (90%)]\tTrain Loss: 0.010789\n",
            "\n",
            "Test set: Test loss: 1.2336, Accuracy: 3876/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 82: lr = 0.1\n",
            "Train Epoch: 82 [5000/50000 (10%)]\tTrain Loss: 0.011065\n",
            "Train Epoch: 82 [10000/50000 (20%)]\tTrain Loss: 0.015275\n",
            "Train Epoch: 82 [15000/50000 (30%)]\tTrain Loss: 0.009933\n",
            "Train Epoch: 82 [20000/50000 (40%)]\tTrain Loss: 0.009445\n",
            "Train Epoch: 82 [25000/50000 (50%)]\tTrain Loss: 0.014206\n",
            "Train Epoch: 82 [30000/50000 (60%)]\tTrain Loss: 0.018512\n",
            "Train Epoch: 82 [35000/50000 (70%)]\tTrain Loss: 0.008933\n",
            "Train Epoch: 82 [40000/50000 (80%)]\tTrain Loss: 0.016988\n",
            "Train Epoch: 82 [45000/50000 (90%)]\tTrain Loss: 0.013614\n",
            "\n",
            "Test set: Test loss: 1.3549, Accuracy: 3830/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 83: lr = 0.1\n",
            "Train Epoch: 83 [5000/50000 (10%)]\tTrain Loss: 0.020064\n",
            "Train Epoch: 83 [10000/50000 (20%)]\tTrain Loss: 0.014333\n",
            "Train Epoch: 83 [15000/50000 (30%)]\tTrain Loss: 0.013715\n",
            "Train Epoch: 83 [20000/50000 (40%)]\tTrain Loss: 0.014702\n",
            "Train Epoch: 83 [25000/50000 (50%)]\tTrain Loss: 0.017360\n",
            "Train Epoch: 83 [30000/50000 (60%)]\tTrain Loss: 0.017414\n",
            "Train Epoch: 83 [35000/50000 (70%)]\tTrain Loss: 0.015172\n",
            "Train Epoch: 83 [40000/50000 (80%)]\tTrain Loss: 0.012798\n",
            "Train Epoch: 83 [45000/50000 (90%)]\tTrain Loss: 0.014287\n",
            "\n",
            "Test set: Test loss: 1.2495, Accuracy: 3852/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 84: lr = 0.1\n",
            "Train Epoch: 84 [5000/50000 (10%)]\tTrain Loss: 0.020977\n",
            "Train Epoch: 84 [10000/50000 (20%)]\tTrain Loss: 0.012792\n",
            "Train Epoch: 84 [15000/50000 (30%)]\tTrain Loss: 0.016333\n",
            "Train Epoch: 84 [20000/50000 (40%)]\tTrain Loss: 0.015447\n",
            "Train Epoch: 84 [25000/50000 (50%)]\tTrain Loss: 0.017715\n",
            "Train Epoch: 84 [30000/50000 (60%)]\tTrain Loss: 0.011826\n",
            "Train Epoch: 84 [35000/50000 (70%)]\tTrain Loss: 0.015207\n",
            "Train Epoch: 84 [40000/50000 (80%)]\tTrain Loss: 0.014333\n",
            "Train Epoch: 84 [45000/50000 (90%)]\tTrain Loss: 0.018864\n",
            "\n",
            "Test set: Test loss: 1.2260, Accuracy: 3879/5000 (78%)\n",
            "\n",
            "Better accuracy at Epoch 84: accuracy = 77.58%\n",
            "\n",
            "Train Epoch 85: lr = 0.1\n",
            "Train Epoch: 85 [5000/50000 (10%)]\tTrain Loss: 0.007938\n",
            "Train Epoch: 85 [10000/50000 (20%)]\tTrain Loss: 0.007820\n",
            "Train Epoch: 85 [15000/50000 (30%)]\tTrain Loss: 0.008613\n",
            "Train Epoch: 85 [20000/50000 (40%)]\tTrain Loss: 0.011398\n",
            "Train Epoch: 85 [25000/50000 (50%)]\tTrain Loss: 0.010542\n",
            "Train Epoch: 85 [30000/50000 (60%)]\tTrain Loss: 0.006664\n",
            "Train Epoch: 85 [35000/50000 (70%)]\tTrain Loss: 0.013876\n",
            "Train Epoch: 85 [40000/50000 (80%)]\tTrain Loss: 0.008888\n",
            "Train Epoch: 85 [45000/50000 (90%)]\tTrain Loss: 0.010158\n",
            "\n",
            "Test set: Test loss: 1.3071, Accuracy: 3869/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 86: lr = 0.1\n",
            "Train Epoch: 86 [5000/50000 (10%)]\tTrain Loss: 0.013038\n",
            "Train Epoch: 86 [10000/50000 (20%)]\tTrain Loss: 0.010046\n",
            "Train Epoch: 86 [15000/50000 (30%)]\tTrain Loss: 0.014577\n",
            "Train Epoch: 86 [20000/50000 (40%)]\tTrain Loss: 0.017698\n",
            "Train Epoch: 86 [25000/50000 (50%)]\tTrain Loss: 0.017311\n",
            "Train Epoch: 86 [30000/50000 (60%)]\tTrain Loss: 0.012448\n",
            "Train Epoch: 86 [35000/50000 (70%)]\tTrain Loss: 0.009739\n",
            "Train Epoch: 86 [40000/50000 (80%)]\tTrain Loss: 0.010561\n",
            "Train Epoch: 86 [45000/50000 (90%)]\tTrain Loss: 0.012470\n",
            "\n",
            "Test set: Test loss: 1.3072, Accuracy: 3855/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 87: lr = 0.1\n",
            "Train Epoch: 87 [5000/50000 (10%)]\tTrain Loss: 0.012301\n",
            "Train Epoch: 87 [10000/50000 (20%)]\tTrain Loss: 0.009589\n",
            "Train Epoch: 87 [15000/50000 (30%)]\tTrain Loss: 0.006941\n",
            "Train Epoch: 87 [20000/50000 (40%)]\tTrain Loss: 0.017665\n",
            "Train Epoch: 87 [25000/50000 (50%)]\tTrain Loss: 0.012585\n",
            "Train Epoch: 87 [30000/50000 (60%)]\tTrain Loss: 0.012550\n",
            "Train Epoch: 87 [35000/50000 (70%)]\tTrain Loss: 0.012181\n",
            "Train Epoch: 87 [40000/50000 (80%)]\tTrain Loss: 0.014043\n",
            "Train Epoch: 87 [45000/50000 (90%)]\tTrain Loss: 0.016470\n",
            "\n",
            "Test set: Test loss: 1.2997, Accuracy: 3854/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 88: lr = 0.1\n",
            "Train Epoch: 88 [5000/50000 (10%)]\tTrain Loss: 0.006400\n",
            "Train Epoch: 88 [10000/50000 (20%)]\tTrain Loss: 0.004848\n",
            "Train Epoch: 88 [15000/50000 (30%)]\tTrain Loss: 0.005708\n",
            "Train Epoch: 88 [20000/50000 (40%)]\tTrain Loss: 0.004240\n",
            "Train Epoch: 88 [25000/50000 (50%)]\tTrain Loss: 0.007186\n",
            "Train Epoch: 88 [30000/50000 (60%)]\tTrain Loss: 0.007613\n",
            "Train Epoch: 88 [35000/50000 (70%)]\tTrain Loss: 0.011995\n",
            "Train Epoch: 88 [40000/50000 (80%)]\tTrain Loss: 0.012495\n",
            "Train Epoch: 88 [45000/50000 (90%)]\tTrain Loss: 0.012022\n",
            "\n",
            "Test set: Test loss: 1.3530, Accuracy: 3864/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 89: lr = 0.1\n",
            "Train Epoch: 89 [5000/50000 (10%)]\tTrain Loss: 0.008505\n",
            "Train Epoch: 89 [10000/50000 (20%)]\tTrain Loss: 0.008901\n",
            "Train Epoch: 89 [15000/50000 (30%)]\tTrain Loss: 0.009426\n",
            "Train Epoch: 89 [20000/50000 (40%)]\tTrain Loss: 0.003974\n",
            "Train Epoch: 89 [25000/50000 (50%)]\tTrain Loss: 0.011905\n",
            "Train Epoch: 89 [30000/50000 (60%)]\tTrain Loss: 0.009148\n",
            "Train Epoch: 89 [35000/50000 (70%)]\tTrain Loss: 0.015611\n",
            "Train Epoch: 89 [40000/50000 (80%)]\tTrain Loss: 0.011246\n",
            "Train Epoch: 89 [45000/50000 (90%)]\tTrain Loss: 0.012420\n",
            "\n",
            "Test set: Test loss: 1.3277, Accuracy: 3831/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 90: lr = 0.1\n",
            "Train Epoch: 90 [5000/50000 (10%)]\tTrain Loss: 0.012903\n",
            "Train Epoch: 90 [10000/50000 (20%)]\tTrain Loss: 0.013249\n",
            "Train Epoch: 90 [15000/50000 (30%)]\tTrain Loss: 0.011102\n",
            "Train Epoch: 90 [20000/50000 (40%)]\tTrain Loss: 0.010417\n",
            "Train Epoch: 90 [25000/50000 (50%)]\tTrain Loss: 0.010603\n",
            "Train Epoch: 90 [30000/50000 (60%)]\tTrain Loss: 0.016158\n",
            "Train Epoch: 90 [35000/50000 (70%)]\tTrain Loss: 0.013172\n",
            "Train Epoch: 90 [40000/50000 (80%)]\tTrain Loss: 0.018470\n",
            "Train Epoch: 90 [45000/50000 (90%)]\tTrain Loss: 0.011772\n",
            "\n",
            "Test set: Test loss: 1.3070, Accuracy: 3854/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 91: lr = 0.1\n",
            "Train Epoch: 91 [5000/50000 (10%)]\tTrain Loss: 0.008339\n",
            "Train Epoch: 91 [10000/50000 (20%)]\tTrain Loss: 0.006603\n",
            "Train Epoch: 91 [15000/50000 (30%)]\tTrain Loss: 0.010326\n",
            "Train Epoch: 91 [20000/50000 (40%)]\tTrain Loss: 0.011074\n",
            "Train Epoch: 91 [25000/50000 (50%)]\tTrain Loss: 0.008372\n",
            "Train Epoch: 91 [30000/50000 (60%)]\tTrain Loss: 0.008507\n",
            "Train Epoch: 91 [35000/50000 (70%)]\tTrain Loss: 0.010742\n",
            "Train Epoch: 91 [40000/50000 (80%)]\tTrain Loss: 0.008817\n",
            "Train Epoch: 91 [45000/50000 (90%)]\tTrain Loss: 0.005112\n",
            "\n",
            "Test set: Test loss: 1.3322, Accuracy: 3850/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 92: lr = 0.1\n",
            "Train Epoch: 92 [5000/50000 (10%)]\tTrain Loss: 0.010421\n",
            "Train Epoch: 92 [10000/50000 (20%)]\tTrain Loss: 0.008496\n",
            "Train Epoch: 92 [15000/50000 (30%)]\tTrain Loss: 0.007418\n",
            "Train Epoch: 92 [20000/50000 (40%)]\tTrain Loss: 0.010157\n",
            "Train Epoch: 92 [25000/50000 (50%)]\tTrain Loss: 0.011677\n",
            "Train Epoch: 92 [30000/50000 (60%)]\tTrain Loss: 0.010358\n",
            "Train Epoch: 92 [35000/50000 (70%)]\tTrain Loss: 0.019817\n",
            "Train Epoch: 92 [40000/50000 (80%)]\tTrain Loss: 0.015502\n",
            "Train Epoch: 92 [45000/50000 (90%)]\tTrain Loss: 0.015007\n",
            "\n",
            "Test set: Test loss: 1.3181, Accuracy: 3862/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 93: lr = 0.1\n",
            "Train Epoch: 93 [5000/50000 (10%)]\tTrain Loss: 0.012328\n",
            "Train Epoch: 93 [10000/50000 (20%)]\tTrain Loss: 0.011325\n",
            "Train Epoch: 93 [15000/50000 (30%)]\tTrain Loss: 0.021248\n",
            "Train Epoch: 93 [20000/50000 (40%)]\tTrain Loss: 0.014695\n",
            "Train Epoch: 93 [25000/50000 (50%)]\tTrain Loss: 0.017351\n",
            "Train Epoch: 93 [30000/50000 (60%)]\tTrain Loss: 0.012988\n",
            "Train Epoch: 93 [35000/50000 (70%)]\tTrain Loss: 0.014174\n",
            "Train Epoch: 93 [40000/50000 (80%)]\tTrain Loss: 0.014371\n",
            "Train Epoch: 93 [45000/50000 (90%)]\tTrain Loss: 0.016032\n",
            "\n",
            "Test set: Test loss: 1.2572, Accuracy: 3864/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 94: lr = 0.1\n",
            "Train Epoch: 94 [5000/50000 (10%)]\tTrain Loss: 0.009220\n",
            "Train Epoch: 94 [10000/50000 (20%)]\tTrain Loss: 0.009751\n",
            "Train Epoch: 94 [15000/50000 (30%)]\tTrain Loss: 0.006188\n",
            "Train Epoch: 94 [20000/50000 (40%)]\tTrain Loss: 0.010038\n",
            "Train Epoch: 94 [25000/50000 (50%)]\tTrain Loss: 0.009230\n",
            "Train Epoch: 94 [30000/50000 (60%)]\tTrain Loss: 0.010174\n",
            "Train Epoch: 94 [35000/50000 (70%)]\tTrain Loss: 0.008921\n",
            "Train Epoch: 94 [40000/50000 (80%)]\tTrain Loss: 0.008183\n",
            "Train Epoch: 94 [45000/50000 (90%)]\tTrain Loss: 0.011144\n",
            "\n",
            "Test set: Test loss: 1.3110, Accuracy: 3880/5000 (78%)\n",
            "\n",
            "Better accuracy at Epoch 94: accuracy = 77.6%\n",
            "\n",
            "Train Epoch 95: lr = 0.1\n",
            "Train Epoch: 95 [5000/50000 (10%)]\tTrain Loss: 0.007911\n",
            "Train Epoch: 95 [10000/50000 (20%)]\tTrain Loss: 0.011443\n",
            "Train Epoch: 95 [15000/50000 (30%)]\tTrain Loss: 0.007906\n",
            "Train Epoch: 95 [20000/50000 (40%)]\tTrain Loss: 0.008381\n",
            "Train Epoch: 95 [25000/50000 (50%)]\tTrain Loss: 0.010284\n",
            "Train Epoch: 95 [30000/50000 (60%)]\tTrain Loss: 0.011454\n",
            "Train Epoch: 95 [35000/50000 (70%)]\tTrain Loss: 0.009447\n",
            "Train Epoch: 95 [40000/50000 (80%)]\tTrain Loss: 0.008643\n",
            "Train Epoch: 95 [45000/50000 (90%)]\tTrain Loss: 0.011296\n",
            "\n",
            "Test set: Test loss: 1.3356, Accuracy: 3875/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 96: lr = 0.1\n",
            "Train Epoch: 96 [5000/50000 (10%)]\tTrain Loss: 0.007311\n",
            "Train Epoch: 96 [10000/50000 (20%)]\tTrain Loss: 0.011246\n",
            "Train Epoch: 96 [15000/50000 (30%)]\tTrain Loss: 0.010135\n",
            "Train Epoch: 96 [20000/50000 (40%)]\tTrain Loss: 0.017094\n",
            "Train Epoch: 96 [25000/50000 (50%)]\tTrain Loss: 0.019348\n",
            "Train Epoch: 96 [30000/50000 (60%)]\tTrain Loss: 0.013059\n",
            "Train Epoch: 96 [35000/50000 (70%)]\tTrain Loss: 0.012799\n",
            "Train Epoch: 96 [40000/50000 (80%)]\tTrain Loss: 0.006171\n",
            "Train Epoch: 96 [45000/50000 (90%)]\tTrain Loss: 0.010092\n",
            "\n",
            "Test set: Test loss: 1.2357, Accuracy: 3880/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 97: lr = 0.1\n",
            "Train Epoch: 97 [5000/50000 (10%)]\tTrain Loss: 0.011747\n",
            "Train Epoch: 97 [10000/50000 (20%)]\tTrain Loss: 0.009904\n",
            "Train Epoch: 97 [15000/50000 (30%)]\tTrain Loss: 0.011278\n",
            "Train Epoch: 97 [20000/50000 (40%)]\tTrain Loss: 0.009516\n",
            "Train Epoch: 97 [25000/50000 (50%)]\tTrain Loss: 0.010817\n",
            "Train Epoch: 97 [30000/50000 (60%)]\tTrain Loss: 0.014048\n",
            "Train Epoch: 97 [35000/50000 (70%)]\tTrain Loss: 0.014827\n",
            "Train Epoch: 97 [40000/50000 (80%)]\tTrain Loss: 0.013914\n",
            "Train Epoch: 97 [45000/50000 (90%)]\tTrain Loss: 0.011745\n",
            "\n",
            "Test set: Test loss: 1.2885, Accuracy: 3873/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 98: lr = 0.1\n",
            "Train Epoch: 98 [5000/50000 (10%)]\tTrain Loss: 0.006267\n",
            "Train Epoch: 98 [10000/50000 (20%)]\tTrain Loss: 0.007423\n",
            "Train Epoch: 98 [15000/50000 (30%)]\tTrain Loss: 0.007606\n",
            "Train Epoch: 98 [20000/50000 (40%)]\tTrain Loss: 0.008742\n",
            "Train Epoch: 98 [25000/50000 (50%)]\tTrain Loss: 0.008419\n",
            "Train Epoch: 98 [30000/50000 (60%)]\tTrain Loss: 0.007640\n",
            "Train Epoch: 98 [35000/50000 (70%)]\tTrain Loss: 0.012393\n",
            "Train Epoch: 98 [40000/50000 (80%)]\tTrain Loss: 0.019306\n",
            "Train Epoch: 98 [45000/50000 (90%)]\tTrain Loss: 0.015440\n",
            "\n",
            "Test set: Test loss: 1.2857, Accuracy: 3873/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 99: lr = 0.1\n",
            "Train Epoch: 99 [5000/50000 (10%)]\tTrain Loss: 0.004946\n",
            "Train Epoch: 99 [10000/50000 (20%)]\tTrain Loss: 0.013530\n",
            "Train Epoch: 99 [15000/50000 (30%)]\tTrain Loss: 0.010938\n",
            "Train Epoch: 99 [20000/50000 (40%)]\tTrain Loss: 0.012837\n",
            "Train Epoch: 99 [25000/50000 (50%)]\tTrain Loss: 0.008846\n",
            "Train Epoch: 99 [30000/50000 (60%)]\tTrain Loss: 0.007091\n",
            "Train Epoch: 99 [35000/50000 (70%)]\tTrain Loss: 0.008549\n",
            "Train Epoch: 99 [40000/50000 (80%)]\tTrain Loss: 0.003632\n",
            "Train Epoch: 99 [45000/50000 (90%)]\tTrain Loss: 0.005870\n",
            "\n",
            "Test set: Test loss: 1.2970, Accuracy: 3860/5000 (77%)\n",
            "\n",
            "CPU times: user 1h 23min 53s, sys: 1min 3s, total: 1h 24min 56s\n",
            "Wall time: 1h 28min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehbODZeAEaPb",
        "colab_type": "code",
        "outputId": "3d49b3f0-b77c-4c25-f27a-fd7b08dc5643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet 0.25 (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet 0.25 (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoMhyvl/XA63lbPAFdEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngV9EJGUU+EVEUkaBX0QkZRT4RURS5v8D77BnNuOEGxEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 53.290620637090015 seconds\n",
            "Best accuracy: 77.6  Best training loss: 0.0002932357892859727  Best validation loss: 0.7705767253041267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2t2VG8wE4_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 96, 64, 64),\n",
        "                Fire(128, 96, 64, 64),\n",
        "                Fire(128, 192, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 192, 128, 128),\n",
        "                Fire(256, 288, 192, 192),\n",
        "                Fire(384, 288, 192, 192),\n",
        "                Fire(384, 384, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 384, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybLLLPcDEbHE",
        "colab_type": "code",
        "outputId": "94599f6c-b8f3-4f72-c5b0-a7dd1ac4dedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.108787\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.818557\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.722555\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.660857\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.589344\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.500019\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.493894\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.430662\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.374311\n",
            "\n",
            "Test set: Test loss: 1.2930, Accuracy: 2784/5000 (56%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 55.68%\n",
            "Better loss at Epoch 0: loss = 1.2929889118671418%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.279610\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.274536\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.254577\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.198966\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.163263\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.214615\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.188174\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.136504\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.114755\n",
            "\n",
            "Test set: Test loss: 1.0513, Accuracy: 3234/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 64.68%\n",
            "Better loss at Epoch 1: loss = 1.0513149803876878%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.025922\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.004837\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.002476\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.041988\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 0.967729\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.009977\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 0.972739\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 0.974854\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 0.977851\n",
            "\n",
            "Test set: Test loss: 0.9083, Accuracy: 3452/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 69.04%\n",
            "Better loss at Epoch 2: loss = 0.9082880353927613%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 0.827352\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 0.870897\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 0.866404\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 0.854276\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.863781\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 0.857560\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 0.862563\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 0.841527\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.837150\n",
            "\n",
            "Test set: Test loss: 0.8717, Accuracy: 3496/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 69.92%\n",
            "Better loss at Epoch 3: loss = 0.8716598296165466%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.712368\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.716627\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.751389\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.758932\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.739924\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.755167\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.761258\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.778663\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.730566\n",
            "\n",
            "Test set: Test loss: 0.8058, Accuracy: 3615/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 72.3%\n",
            "Better loss at Epoch 4: loss = 0.8058347344398499%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.615095\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.634541\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.641820\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.676344\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.655569\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.651969\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.659971\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.678716\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.662623\n",
            "\n",
            "Test set: Test loss: 0.8074, Accuracy: 3636/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 72.72%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.538243\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.573806\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.574450\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.575628\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.596067\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.594150\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.605205\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.609455\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.593405\n",
            "\n",
            "Test set: Test loss: 0.7546, Accuracy: 3760/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 75.2%\n",
            "Better loss at Epoch 6: loss = 0.7545577809214592%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.493301\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.476494\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.519653\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.520422\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.520958\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.555098\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.549905\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.550865\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.554345\n",
            "\n",
            "Test set: Test loss: 0.7514, Accuracy: 3758/5000 (75%)\n",
            "\n",
            "Better loss at Epoch 7: loss = 0.7513559916615487%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.410169\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.438347\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.449643\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.463075\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.470022\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.488964\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.485053\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.477161\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.471372\n",
            "\n",
            "Test set: Test loss: 0.7427, Accuracy: 3808/5000 (76%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 76.16%\n",
            "Better loss at Epoch 8: loss = 0.7426820194721222%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.366718\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.359157\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.393083\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.396222\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.422826\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.417025\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.436902\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.420722\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.441617\n",
            "\n",
            "Test set: Test loss: 0.7877, Accuracy: 3736/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.320388\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.317542\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.330768\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.381921\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.396319\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.364909\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.378741\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.379622\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.383321\n",
            "\n",
            "Test set: Test loss: 0.7759, Accuracy: 3791/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.287092\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.305049\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.307887\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.311866\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.337153\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.325277\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.357443\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.376173\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.361969\n",
            "\n",
            "Test set: Test loss: 0.7884, Accuracy: 3761/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.260494\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.260258\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.248483\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.292349\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.302250\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.288718\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.282053\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.296566\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.326673\n",
            "\n",
            "Test set: Test loss: 0.8509, Accuracy: 3751/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.232620\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.219763\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.238500\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.244379\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.263501\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.253108\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.238611\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.287393\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.299409\n",
            "\n",
            "Test set: Test loss: 0.8211, Accuracy: 3786/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.205362\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.213574\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.228685\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.238816\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.234289\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.244714\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.242796\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.236369\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.259344\n",
            "\n",
            "Test set: Test loss: 0.8497, Accuracy: 3745/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.182506\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.175506\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.184403\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.202160\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.226049\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.227718\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.223060\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.232497\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.208892\n",
            "\n",
            "Test set: Test loss: 0.9075, Accuracy: 3779/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.159556\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.163060\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.166118\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.192381\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.201098\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.205586\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.214468\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.209111\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.192524\n",
            "\n",
            "Test set: Test loss: 0.8833, Accuracy: 3783/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.154415\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.160638\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.160803\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.182628\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.172832\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.188958\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.159936\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.186660\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.200682\n",
            "\n",
            "Test set: Test loss: 0.8940, Accuracy: 3820/5000 (76%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 76.4%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.139693\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.129142\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.130129\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.157498\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.167045\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.142273\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.157801\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.179474\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.171636\n",
            "\n",
            "Test set: Test loss: 0.9035, Accuracy: 3833/5000 (77%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 76.66%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.127869\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.147970\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.115368\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.139981\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.134280\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.174270\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.181554\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.164137\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.153894\n",
            "\n",
            "Test set: Test loss: 0.9390, Accuracy: 3816/5000 (76%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.117874\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.113984\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.101297\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.139792\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.132464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-34a24e5c9987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nmodel = squeezenet1_0(num_classes=10)\\nmodel = model.to(device=device, dtype=torch.float)\\n\\n# Cross Entropy Loss \\nerror = CrossEntropyLoss().to(device=device, dtype=torch.float)\\n\\n#Optimizer\\nlearning_rate = 0.1\\noptimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\\n\\n#Optimizer adam\\n# learning_rate = 0.04\\n# optimizer = Adam(model.parameters(), lr=learning_rate)\\n# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\\n# optimizer = SGD(model.parameters(), lr=learning_rate)\\n\\n#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    sta...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua4c-d-zEgFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet 0.25 (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet 0.25 (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9knH_LHCEi2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}