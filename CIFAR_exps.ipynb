{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_exps.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu83Jf5qHY_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYTXiSckMwDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%cd /content/\n",
        "#!/usr/bin/python\n",
        "# essential imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.ion() \n",
        "import numpy as np\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import pickle\n",
        "#tensorflow 1.15\n",
        "import tensorflow as tf\n",
        "#print(tf.__version__)\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# default seeding for reproducability\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MXcpFQSOI50",
        "colab_type": "code",
        "outputId": "0de174a9-6fa7-4fad-d2e3-5743de4f85b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Import torch Libraries\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, MSELoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "#from torch.autograd import Variable\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "#adam sgd combined optimizer\n",
        "!pip install adabound\n",
        "import adabound\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "\n",
        "#defaults\n",
        "PIXEL_LENGTH_MODIFIED = 128\n",
        "FEATURE_SIZE_MODIFIED = PIXEL_LENGTH_MODIFIED*PIXEL_LENGTH_MODIFIED\n",
        "PIXEL_LENGTH_MNIST = 28\n",
        "FEATURE_SIZE_MNIST = PIXEL_LENGTH_MNIST*PIXEL_LENGTH_MNIST"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: adabound in /usr/local/lib/python3.6/dist-packages (0.0.5)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Tb-VZDOU5Q",
        "colab_type": "text"
      },
      "source": [
        "## helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLScvSjuOWFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "# def normalize_max(images):\n",
        "#   images_normalize=[]\n",
        "#   for i in images:\n",
        "#     i -= i.min()\n",
        "#     denom = i.max()-i.min()\n",
        "#     images_normalize.append(np.divide(i, denom))\n",
        "#   return np.asarray(images_normalize) \n",
        "\n",
        "# def standardize_mean(images):\n",
        "#   images_standardize=[]\n",
        "#   for i in images:\n",
        "#     mean, std = i.mean(), i.std()\n",
        "#     i = (i - mean) / std\n",
        "#     images_standardize.append(i)\n",
        "#   return np.asarray(images_standardize) \n",
        "  \n",
        "# training/testing functions\n",
        "def train(epoch, train_loader, model, error, optimizer, batch_size):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    #loss_total = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        #peak at tensor details\n",
        "        # if batch_idx==1:\n",
        "        #   # visualize one of the images in data set\n",
        "        #   plt.imshow(np.squeeze(images[0].numpy()), cmap='Greys')\n",
        "        #   plt.axis(\"off\")\n",
        "        #   plt.title(str(labels[0].numpy()))\n",
        "        #   #plt.savefig('graph.png')\n",
        "        #   plt.show()\n",
        "        #reshape for training\n",
        "        train = images.view(batch_size,3,32,32).to(device=device, dtype=torch.float)\n",
        "        labels = labels.to(device=device, dtype=torch.long)\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "        # Calculate softmax and cross entropy loss\n",
        "        loss = error(outputs, labels)\n",
        "        #print(loss)\n",
        "        #loss_total += loss.item()\n",
        "\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        #print every 100 batches\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
        "            # plt.imshow(images[0].numpy().reshape(128,128))\n",
        "            # plt.axis('off')\n",
        "            # plt.show()\n",
        "            #Print Loss\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(images), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), running_loss/100))\n",
        "            running_loss = 0.0\n",
        "    #loss_total /= len(train_loader)\n",
        "    #print('Test set: Test loss: {:.4f}'.format(loss_total))\n",
        "    #return last loss\n",
        "    return loss.item()\n",
        "\n",
        "def test(test_loader, model, error, batch_size):\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Calculate Accuracy         \n",
        "        correct = 0\n",
        "        loss_test = 0.0\n",
        "        # Iterate through test dataset\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            # if batch_idx==1:\n",
        "            #   # visualize one of the images in data set\n",
        "            #   plt.imshow(np.squeeze(images[0].numpy()), cmap='Greys')\n",
        "            #   plt.axis(\"off\")\n",
        "            #   plt.title(str(labels[0].numpy()))\n",
        "            #   #plt.savefig('graph.png')\n",
        "            #   plt.show()\n",
        "\n",
        "            test = images.view(batch_size,3,32,32).to(device=device, dtype=torch.float)\n",
        "            labels = labels.to(device=device, dtype=torch.long)\n",
        "            # Forward propagation\n",
        "            outputs = model(test)\n",
        "            \n",
        "\n",
        "            # sum up batch loss\n",
        "            loss_test += error(outputs, labels).item()\n",
        "            # get the index of the max log-probability\n",
        "            predicted = outputs.max(1, keepdim=True)[1]\n",
        "            # if batch_idx==1:\n",
        "            #   print(predicted[0].item())\n",
        "            correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "        loss_test /= len(test_loader)\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "        print('\\nTest set: Test loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "              .format(loss_test, correct, len(test_loader.dataset), accuracy))\n",
        "        #return loss and accuracy\n",
        "        return loss_test, accuracy\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK6XR7HuS4xp",
        "colab_type": "code",
        "outputId": "5ac361da-0c1a-4bbf-d923-a455d395082e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# batch_size = 50\n",
        "# workers = os.cpu_count()\n",
        "\n",
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# lengths = [60000*0.8, 60000*0.1, 60000*0.1]\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "# train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=workers)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=workers)\n",
        "\n",
        "# #check for gpu/cpu\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 50\n",
        "workers = os.cpu_count()\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "lengths = [5000, 5000]\n",
        "\n",
        "fullset_train = torchvision.datasets.CIFAR10(root='./data', download=True, train=True,\n",
        "                                       transform=transform)\n",
        "\n",
        "fullset_test = torchvision.datasets.CIFAR10(root='./data', download=True, train=False,\n",
        "                                       transform=transform)\n",
        "\n",
        "testset, finaltestset = torch.utils.data.random_split(fullset_test, lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(fullset_train, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "final_test_loader = torch.utils.data.DataLoader(finaltestset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "#check for gpu/cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXhP2GvLy-pw",
        "colab_type": "code",
        "outputId": "3ae0484f-8fdd-4564-fe1f-9a00b64fac61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "fullset_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7NZIrYFL1GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# # load mnist dataset\n",
        "# (x_train_stack, y_train_stack), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# # split train into train-val set\n",
        "# #x_train_stack_normalized = standardize_mean(x_train_stack)\n",
        "# features_train, features_test, targets_train, targets_test = train_test_split(x_train_stack,\n",
        "#                                                       y_train_stack,\n",
        "#                                                       test_size = 5000,\n",
        "#                                                       random_state = 42) \n",
        "\n",
        "\n",
        "# print(features_train.shape)\n",
        "# print(features_test.shape)\n",
        "# print(targets_train.shape)\n",
        "# print(targets_test.shape)\n",
        "# # visualize one of the images in data set\n",
        "# plt.imshow(x_train_stack[10], cmap='Greys')\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(str(y_train_stack[10]))\n",
        "# #plt.savefig('graph.png')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4FXgWmMgqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # set batch_size, epoch and iteration\n",
        "# batch_size = 50\n",
        "# workers = os.cpu_count()\n",
        "# # n_iters = 10000\n",
        "# # num_epochs = n_iters / (len(features_train) / batch_size)\n",
        "# # num_epochs = int(num_epochs)\n",
        "\n",
        "# # create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
        "# featuresTrain = torch.from_numpy(features_train).type(torch.LongTensor)\n",
        "# targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# # create feature and targets tensor for test set.\n",
        "# featuresTest = torch.from_numpy(features_test).type(torch.LongTensor)\n",
        "# targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# # Pytorch train and test sets\n",
        "# train_data = torch.utils.data.TensorDataset(featuresTrain,targetsTrain, transform=transform)\n",
        "# test_data = torch.utils.data.TensorDataset(featuresTest,targetsTest, transform=transform)\n",
        "\n",
        "# # prepare data loaders (combine dataset and sampler)\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=False, num_workers=workers)\n",
        "\n",
        "# #check for gpu/cpu\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uUybJbjN6W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I38j-LzMN8_g",
        "colab_type": "text"
      },
      "source": [
        "## this guy alex..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoGYjfQ6N90P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['AlexNet', 'alexnet']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(pretrained=False, **kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hhLKkJrN-e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "model = alexnet(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.1\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_KS7Rk0OAJP",
        "colab_type": "code",
        "outputId": "05d55df5-b9da-4e88-c319-0f3b438c799a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/alexnet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/alexnet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.293136\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.014728\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.889440\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.773362\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.721718\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.664595\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.612807\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.580618\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.559560\n",
            "\n",
            "Test set: Test loss: 1.4471, Accuracy: 2265/5000 (45%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 45.3%\n",
            "Better loss at Epoch 0: loss = 1.44712819814682%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.484247\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.440978\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.393117\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.423753\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.393297\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.343575\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.323909\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.290417\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.287544\n",
            "\n",
            "Test set: Test loss: 1.1781, Accuracy: 2875/5000 (58%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 57.5%\n",
            "Better loss at Epoch 1: loss = 1.1780840069055558%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.183384\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.172043\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.147634\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.132466\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.145960\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.098322\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.136004\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.097184\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.087452\n",
            "\n",
            "Test set: Test loss: 1.0184, Accuracy: 3200/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 64.0%\n",
            "Better loss at Epoch 2: loss = 1.0184080082178115%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 0.993341\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.019708\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 0.985655\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.008987\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.975413\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 0.954575\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 0.950305\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 0.964914\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.950027\n",
            "\n",
            "Test set: Test loss: 1.1101, Accuracy: 3068/5000 (61%)\n",
            "\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.865279\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.875965\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.850768\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.838983\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.845044\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.840495\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.848480\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.862641\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.849395\n",
            "\n",
            "Test set: Test loss: 0.9009, Accuracy: 3470/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 69.4%\n",
            "Better loss at Epoch 4: loss = 0.9008629962801933%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.711274\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.722851\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.790607\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.756436\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.760998\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.754076\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.732441\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.770684\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.744399\n",
            "\n",
            "Test set: Test loss: 0.8623, Accuracy: 3569/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 71.38%\n",
            "Better loss at Epoch 5: loss = 0.862334297299385%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.658906\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.623183\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.630551\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.656202\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.670642\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.679595\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.685746\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.642508\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.648098\n",
            "\n",
            "Test set: Test loss: 0.8783, Accuracy: 3552/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.524652\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.568918\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.555639\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.559207\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.567143\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.560405\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.578190\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.567862\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.571902\n",
            "\n",
            "Test set: Test loss: 0.8574, Accuracy: 3566/5000 (71%)\n",
            "\n",
            "Better loss at Epoch 7: loss = 0.8573857283592224%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.451270\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.459910\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.444802\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.469724\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.537403\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.501822\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.497845\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.547439\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.480775\n",
            "\n",
            "Test set: Test loss: 0.9427, Accuracy: 3502/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.386764\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.393958\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.402887\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.409379\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.439597\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.431238\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.443910\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.428010\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.456881\n",
            "\n",
            "Test set: Test loss: 0.8554, Accuracy: 3648/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 72.96%\n",
            "Better loss at Epoch 9: loss = 0.8554293078184128%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.298416\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.344596\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.338543\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.370834\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.366562\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.382582\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.369893\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.390420\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.409374\n",
            "\n",
            "Test set: Test loss: 0.9697, Accuracy: 3612/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.256481\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.285680\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.273319\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.294731\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.328346\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.305082\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.315375\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.338854\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.338145\n",
            "\n",
            "Test set: Test loss: 0.9368, Accuracy: 3649/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 72.98%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.224559\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.224654\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.251448\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.260003\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.266709\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.270103\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.289540\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.246632\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.279048\n",
            "\n",
            "Test set: Test loss: 1.0334, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 73.68%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.174363\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.180245\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.229009\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.217307\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.210331\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.248660\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.221617\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.231315\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.250077\n",
            "\n",
            "Test set: Test loss: 1.0241, Accuracy: 3689/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 73.78%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.153772\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.165199\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.183539\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.203842\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.199421\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.206558\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.200211\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.211794\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.229203\n",
            "\n",
            "Test set: Test loss: 1.1096, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.115676\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.142334\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.157605\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.162603\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.159220\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.144466\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.209252\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.187724\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.166162\n",
            "\n",
            "Test set: Test loss: 1.1992, Accuracy: 3665/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.123671\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.109287\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.151969\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.165625\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.143416\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.162722\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.162604\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.147265\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.156242\n",
            "\n",
            "Test set: Test loss: 1.2212, Accuracy: 3636/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.110855\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.109087\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.122635\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.137793\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.107142\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.113866\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.124507\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.125127\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.123406\n",
            "\n",
            "Test set: Test loss: 1.2665, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.072278\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.099537\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.090712\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.101663\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.119118\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.124073\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.127811\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.124681\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.115455\n",
            "\n",
            "Test set: Test loss: 1.2231, Accuracy: 3695/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 73.9%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.066058\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.087999\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.093879\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.072318\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.085181\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.095085\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.116541\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.119393\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.099623\n",
            "\n",
            "Test set: Test loss: 1.2377, Accuracy: 3663/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.058035\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.055320\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.081222\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.079684\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.105010\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.091768\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.087520\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.092301\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.113989\n",
            "\n",
            "Test set: Test loss: 1.4116, Accuracy: 3677/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.073665\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.057535\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.070426\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.061078\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.073943\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.086569\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.095135\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.092263\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.071517\n",
            "\n",
            "Test set: Test loss: 1.3712, Accuracy: 3695/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.037835\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.041157\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.045987\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.067751\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.062075\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.062574\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.075218\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.072903\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.054752\n",
            "\n",
            "Test set: Test loss: 1.5349, Accuracy: 3663/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.070131\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.047217\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.038575\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.070046\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.054258\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.050213\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.075025\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.068185\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.072157\n",
            "\n",
            "Test set: Test loss: 1.5186, Accuracy: 3710/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 23: accuracy = 74.2%\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.065230\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.042527\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.057553\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.050741\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.041472\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.050317\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.079859\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.045884\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.050235\n",
            "\n",
            "Test set: Test loss: 1.4701, Accuracy: 3694/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.040201\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.052655\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.048189\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.056534\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.038015\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.051332\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.061447\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.051849\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.049427\n",
            "\n",
            "Test set: Test loss: 1.7114, Accuracy: 3649/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.035182\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.024492\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.056296\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.075357\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.055855\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.046137\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.059144\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.054614\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.066655\n",
            "\n",
            "Test set: Test loss: 1.5703, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.038593\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.052199\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.031264\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.045684\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.043805\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.033608\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.034305\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.039703\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.029011\n",
            "\n",
            "Test set: Test loss: 1.7138, Accuracy: 3689/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.041806\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.050394\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.038607\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.030450\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.036299\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.055571\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.060859\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.040558\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.034977\n",
            "\n",
            "Test set: Test loss: 1.5406, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.037202\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.019779\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.039141\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.030352\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.034383\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.039980\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.057904\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.053017\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.040544\n",
            "\n",
            "Test set: Test loss: 1.7167, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.024984\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.031576\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.027698\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.030113\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.026741\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.021360\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.030549\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.038196\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.038472\n",
            "\n",
            "Test set: Test loss: 1.7881, Accuracy: 3708/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.024230\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.018755\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.033493\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.028029\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.062848\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.050564\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.043176\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.050782\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.040413\n",
            "\n",
            "Test set: Test loss: 1.8192, Accuracy: 3708/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.036076\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.025790\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.018124\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.025819\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.041656\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.042698\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.031682\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.027409\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.032202\n",
            "\n",
            "Test set: Test loss: 1.7703, Accuracy: 3742/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 32: accuracy = 74.84%\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.026912\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.019695\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.013555\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.012990\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.024813\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.032600\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.036405\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.047131\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.035506\n",
            "\n",
            "Test set: Test loss: 1.7931, Accuracy: 3745/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 33: accuracy = 74.9%\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.024997\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.033388\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.029109\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.036651\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.021533\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.027427\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.022218\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.024750\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.035974\n",
            "\n",
            "Test set: Test loss: 1.7311, Accuracy: 3722/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.015145\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.010016\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.011319\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.021146\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.019687\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.032407\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.017762\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.018608\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.021317\n",
            "\n",
            "Test set: Test loss: 1.9328, Accuracy: 3671/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.017205\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.013736\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.018880\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.016900\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.016805\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.019041\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.027605\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.038731\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.032327\n",
            "\n",
            "Test set: Test loss: 1.7994, Accuracy: 3654/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.032652\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.017983\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.021816\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.022410\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.019442\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.034640\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.033195\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.025736\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.028364\n",
            "\n",
            "Test set: Test loss: 1.8290, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.023210\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.014931\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.009820\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.012391\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.015358\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.025298\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.031619\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.020569\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.017800\n",
            "\n",
            "Test set: Test loss: 1.8475, Accuracy: 3721/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.020902\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.022516\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.027037\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.033483\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.028593\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.026932\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.039605\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.027108\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.019468\n",
            "\n",
            "Test set: Test loss: 1.8548, Accuracy: 3726/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.019583\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.021031\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.024321\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.038354\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.023119\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.019679\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.022673\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.030248\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.033943\n",
            "\n",
            "Test set: Test loss: 1.7719, Accuracy: 3736/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.017130\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.017303\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.018328\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.034757\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.020323\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.012561\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.012614\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.015359\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.024581\n",
            "\n",
            "Test set: Test loss: 1.8315, Accuracy: 3711/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.009700\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.009968\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.010557\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.012740\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.024483\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.013082\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.013058\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.024419\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.027340\n",
            "\n",
            "Test set: Test loss: 1.9962, Accuracy: 3724/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.011033\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.024265\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.020571\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.018980\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.015804\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.012549\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.009831\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.014438\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.023662\n",
            "\n",
            "Test set: Test loss: 1.9749, Accuracy: 3661/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.020782\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.011269\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.009677\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.015993\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.026170\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.017379\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.015692\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.022626\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.025690\n",
            "\n",
            "Test set: Test loss: 1.8632, Accuracy: 3719/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.010109\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.008814\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.013121\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.030209\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.021040\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.015247\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.024188\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.017912\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.012205\n",
            "\n",
            "Test set: Test loss: 1.9594, Accuracy: 3757/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 45: accuracy = 75.14%\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.010250\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.008303\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.009340\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.024520\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.010826\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.013809\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.020384\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.019101\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.031423\n",
            "\n",
            "Test set: Test loss: 1.7592, Accuracy: 3707/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.013961\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.014245\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.012006\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.008031\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.016149\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.015201\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.014061\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.010034\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.013948\n",
            "\n",
            "Test set: Test loss: 2.0277, Accuracy: 3731/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.011119\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.018607\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.013880\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.009636\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.010210\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.023588\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.040084\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.023441\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.018817\n",
            "\n",
            "Test set: Test loss: 1.9075, Accuracy: 3725/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.008786\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.004856\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.007750\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.020532\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.009930\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.020738\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.015966\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.017843\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.014966\n",
            "\n",
            "Test set: Test loss: 1.9474, Accuracy: 3744/5000 (75%)\n",
            "\n",
            "CPU times: user 10min 9s, sys: 2min 32s, total: 12min 42s\n",
            "Wall time: 13min 53s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp5SzXncOO39",
        "colab_type": "code",
        "outputId": "041ec453-3a7a-448b-c0e4-9702493360b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYRhumQBhGIZhuPU/f56JyN5X9ZsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 16.60127462081999 seconds\n",
            "Best accuracy: 75.14  Best training loss: 0.00020103454880882055  Best validation loss: 0.8554293078184128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EApzFYGfLjw",
        "colab_type": "code",
        "outputId": "d81dd3df-e18b-4013-bcec-0f3e44b036ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "[1.6835790872573853, 1.175603985786438, 0.9885087609291077, 1.0834705829620361, 0.5205501317977905, 0.6725502014160156, 0.5350584387779236, 0.6497542858123779, 0.6653600335121155, 0.4189870059490204, 0.36256447434425354, 0.25640812516212463, 0.39531826972961426, 0.14752794802188873, 0.103270024061203, 0.210580974817276, 0.17804880440235138, 0.01845608651638031, 0.09959878772497177, 0.039013996720314026, 0.09906764328479767, 0.20886491239070892, 0.030130835250020027, 0.1711345613002777, 0.15228445827960968, 0.03762003779411316, 0.015244321897625923, 0.045352753251791, 0.019627027213573456, 0.21302169561386108, 0.07005283236503601, 0.016617432236671448, 0.07222004234790802, 0.0075284577906131744, 0.05671592801809311, 0.0012827396858483553, 0.017310313880443573, 0.003903617849573493, 0.14320771396160126, 0.00020103454880882055, 0.005680846981704235, 0.06716543436050415, 0.005466203670948744, 0.0903606042265892, 0.00386638636700809, 0.0038905907422304153, 0.002135429298505187, 0.0002275848382851109, 0.057308949530124664, 0.0003503417829051614]\n",
            "[1.44712819814682, 1.1780840069055558, 1.0184080082178115, 1.1100626921653747, 0.9008629962801933, 0.862334297299385, 0.878264063000679, 0.8573857283592224, 0.9426943066716195, 0.8554293078184128, 0.9697082796692849, 0.936833711862564, 1.0334056901931763, 1.0240645429491997, 1.1095976746082306, 1.1992230933904648, 1.2212348321080209, 1.2664743837714196, 1.2230980008840562, 1.2377275025844574, 1.4115734595060347, 1.3711766177415847, 1.534936579465866, 1.5186029469966889, 1.4700755324959756, 1.7114304494857788, 1.570284583568573, 1.713808557987213, 1.5406410855054855, 1.7166801816225052, 1.788138062953949, 1.8191793447732925, 1.7703304362297059, 1.7931327319145203, 1.7310948312282561, 1.9327742475271226, 1.7993641477823257, 1.8290344268083571, 1.8475029343366622, 1.8547963827848435, 1.7719349592924118, 1.8315067499876023, 1.9961856889724732, 1.9748841798305512, 1.863212181329727, 1.95939208984375, 1.759235202074051, 2.027719659805298, 1.907529857158661, 1.9473622387647629]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFC4-mQZMse",
        "colab_type": "text"
      },
      "source": [
        "## squeeze dat net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2C4MQMcQbAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpLLai93DC5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaz9jOeSZOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCt69lxiZXZI",
        "colab_type": "code",
        "outputId": "8f0172e7-181d-4ecb-a358-e4d60ab4ff1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-23b1f9abbaa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/squeezenet_loss.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SqueezeNet:\n\tMissing key(s) in state_dict: \"features.1.weight\", \"features.1.bias\", \"features.1.running_mean\", \"features.1.running_var\", \"features.4.bns1.weight\", \"features.4.bns1.bias\", \"features.4.bns1.running_mean\", \"features.4.bns1.running_var\", \"features.4.bne1.weight\", \"features.4.bne1.bias\", \"features.4.bne1.running_mean\", \"features.4.bne1.running_var\", \"features.4.bne3.weight\", \"features.4.bne3.bias\", \"features.4.bne3.running_mean\", \"features.4.bne3.running_var\", \"features.5.bns1.weight\", \"features.5.bns1.bias\", \"features.5.bns1.running_mean\", \"features.5.bns1.running_var\", \"features.5.bne1.weight\", \"features.5.bne1.bias\", \"features.5.bne1.running_mean\", \"features.5.bne1.running_var\", \"features.5.bne3.weight\", \"features.5.bne3.bias\", \"features.5.bne3.running_mean\", \"features.5.bne3.running_var\", \"features.6.squeeze.weight\", \"features.6.squeeze.bias\", \"features.6.bns1.weight\", \"features.6.bns1.bias\", \"features.6.bns1.running_mean\", \"features.6.bns1.running_var\", \"features.6.expand1x1.weight\", \"features.6.expand1x1.bias\", \"features.6.bne1.weight\", \"features.6.bne1.bias\", \"features.6.bne1.running_mean\", \"features.6.bne1.running_var\", \"features.6.expand3x3.weight\", \"features.6.expand3x3.bias\", \"features.6.bne3.weight\", \"features.6.bne3.bias\", \"features.6.bne3.running_mean\", \"features.6.bne3.running_var\", \"features.8.bns1.weight\", \"features.8.bns1.bias\", \"features.8.bns1.running_mean\", \"features.8.bns1.running_var\", \"features.8.bne1.weight\", \"features.8.bne1.bias\", \"features.8.bne1....\n\tUnexpected key(s) in state_dict: \"features.3.squeeze.weight\", \"features.3.squeeze.bias\", \"features.3.expand1x1.weight\", \"features.3.expand1x1.bias\", \"features.3.expand3x3.weight\", \"features.3.expand3x3.bias\", \"features.7.squeeze.weight\", \"features.7.squeeze.bias\", \"features.7.expand1x1.weight\", \"features.7.expand1x1.bias\", \"features.7.expand3x3.weight\", \"features.7.expand3x3.bias\", \"features.12.squeeze.weight\", \"features.12.squeeze.bias\", \"features.12.expand1x1.weight\", \"features.12.expand1x1.bias\", \"features.12.expand3x3.weight\", \"features.12.expand3x3.bias\". \n\tsize mismatch for features.4.squeeze.weight: copying a param with shape torch.Size([16, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 96, 1, 1]).\n\tsize mismatch for features.5.squeeze.weight: copying a param with shape torch.Size([32, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([16, 128, 1, 1]).\n\tsize mismatch for features.5.squeeze.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for features.5.expand1x1.weight: copying a param with shape torch.Size([128, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 16, 1, 1]).\n\tsize mismatch for features.5.expand1x1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for features.5.expand3x3.weight: copying a param with shape torch.Size([128, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 16, 3, 3]).\n\tsize mismatch for features.5.expand3x3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for features.8.squeeze.weight: copying a param with shape torch.Size([48, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([32, 256, 1, 1]).\n\tsize mismatch for features.8.squeeze.bias: copying a param with shape torch.Size([48]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for features.8.expand1x1.weight: copying a param with shape torch.Size([192, 48, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 32, 1, 1]).\n\tsize mismatch for features.8.expand1x1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features.8.expand3x3.weight: copying a param with shape torch.Size([192, 48, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 32, 3, 3]).\n\tsize mismatch for features.8.expand3x3.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for features.9.squeeze.weight: copying a param with shape torch.Size([48, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 256, 1, 1]).\n\tsize mismatch for features.10.squeeze.weight: copying a param with shape torch.Size([64, 384, 1, 1]) from checkpoint, the shape in current model is torch.Size([48, 384, 1, 1]).\n\tsize mismatch for features.10.squeeze.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([48]).\n\tsize mismatch for features.10.expand1x1.weight: copying a param with shape torch.Size([256, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([192, 48, 1, 1]).\n\tsize mismatch for features.10.expand1x1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192]).\n\tsize mismatch for features.10.expand3x3.weight: copying a param with shape torch.Size([256, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([192, 48, 3, 3]).\n\tsize mismatch for features.10.expand3x3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([192])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT6xPT7cZZPY",
        "colab_type": "code",
        "outputId": "1c08a5f4-4c20-48c0-a4b6-c55584d14e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 250\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.191117\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.932757\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.831139\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.771829\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.753587\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.671224\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.663117\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.595782\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.589745\n",
            "\n",
            "Test set: Test loss: 1.4998, Accuracy: 2349/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 46.98%\n",
            "Better loss at Epoch 0: loss = 1.4997807788848876%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.501928\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.473120\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.461741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5f83fe6b3ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 250\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-7d352974c4b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsbound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEpPEK4ZZbPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhTWsbg_ejfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XUt6oJNnkHO",
        "colab_type": "text"
      },
      "source": [
        "### residual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogEv-s47np-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "        self.features4 = nn.Sequential(\n",
        "            Fire(384, 64, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        \n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "\n",
        "        x = self.features4(x)\n",
        "\n",
        "        residual4 = x\n",
        "        x = self.block4(x)\n",
        "        x += residual4\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGdonSJdEyP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3tGIw3AE0XI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f7f7b6f8-7fc4-4be3-9c3d-8048b7903f86"
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.189702\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.884978\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.783691\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.712836\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.670738\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.587798\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.560818\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.506738\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.471223\n",
            "\n",
            "Test set: Test loss: 1.3605, Accuracy: 2644/5000 (53%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 52.88%\n",
            "Better loss at Epoch 0: loss = 1.3605176353454589%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.403105\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.356187\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.324081\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.343577\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.303486\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.288181\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.270523\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.244260\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.232560\n",
            "\n",
            "Test set: Test loss: 1.1594, Accuracy: 3012/5000 (60%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 60.24%\n",
            "Better loss at Epoch 1: loss = 1.159411541223526%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.169641\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.153968\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.137870\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.140693\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.164830\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.103065\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.129523\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.129038\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.090757\n",
            "\n",
            "Test set: Test loss: 1.0271, Accuracy: 3208/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 64.16%\n",
            "Better loss at Epoch 2: loss = 1.0270540475845338%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.018200\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.000337\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.042468\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.000457\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.998234\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.011985\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.017141\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.022584\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.000189\n",
            "\n",
            "Test set: Test loss: 0.9804, Accuracy: 3273/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 65.46%\n",
            "Better loss at Epoch 3: loss = 0.980360300540924%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.946347\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.942831\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.940985\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.929705\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.905081\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.943282\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.907905\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.907567\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.910361\n",
            "\n",
            "Test set: Test loss: 0.9804, Accuracy: 3298/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 65.96%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.834803\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.832881\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.846979\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.866330\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.869983\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.846416\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.861683\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.855915\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.858983\n",
            "\n",
            "Test set: Test loss: 0.9211, Accuracy: 3406/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 68.12%\n",
            "Better loss at Epoch 5: loss = 0.9210622584819794%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.775616\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.808452\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.801295\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.780675\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.769928\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.805291\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.795287\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.812497\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.779276\n",
            "\n",
            "Test set: Test loss: 0.8652, Accuracy: 3486/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 69.72%\n",
            "Better loss at Epoch 6: loss = 0.8651850792765617%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.710385\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.702410\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.744822\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.730201\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.761893\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.744412\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.733505\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.738848\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.732929\n",
            "\n",
            "Test set: Test loss: 0.8501, Accuracy: 3529/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 70.58%\n",
            "Better loss at Epoch 7: loss = 0.8501198822259903%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.678034\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.657225\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.689104\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.691555\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.711212\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.702131\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.686725\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.692335\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.700880\n",
            "\n",
            "Test set: Test loss: 0.8194, Accuracy: 3583/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 71.66%\n",
            "Better loss at Epoch 8: loss = 0.8194299104809761%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.614360\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.619749\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.652763\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.635027\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.642332\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.674379\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.680789\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.660599\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.649904\n",
            "\n",
            "Test set: Test loss: 0.8153, Accuracy: 3589/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 71.78%\n",
            "Better loss at Epoch 9: loss = 0.8152713599801064%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.558906\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.585321\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.606078\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.625803\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.633984\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.612109\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.604345\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.628350\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.596577\n",
            "\n",
            "Test set: Test loss: 0.8477, Accuracy: 3575/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.515916\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.536853\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.578342\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.582894\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.576088\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.573049\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.559780\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.583068\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.617240\n",
            "\n",
            "Test set: Test loss: 0.7837, Accuracy: 3694/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 73.88%\n",
            "Better loss at Epoch 11: loss = 0.7837129467725754%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.499968\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.504796\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.554102\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.518348\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.545502\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.560460\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.568042\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.552446\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.555238\n",
            "\n",
            "Test set: Test loss: 0.8470, Accuracy: 3617/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.479020\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.498738\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.488968\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.511174\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.519379\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.491442\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.519634\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.533629\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.531031\n",
            "\n",
            "Test set: Test loss: 0.8197, Accuracy: 3648/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.448043\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.436463\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.470591\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.471760\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.480448\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.489100\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.504666\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.488521\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.481349\n",
            "\n",
            "Test set: Test loss: 0.8188, Accuracy: 3640/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.410421\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.450479\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.449583\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.450818\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.482943\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.467814\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.470201\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.481464\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.473582\n",
            "\n",
            "Test set: Test loss: 0.8202, Accuracy: 3682/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.382620\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.396417\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.421261\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.409616\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.452802\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.435192\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.413943\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.470050\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.433382\n",
            "\n",
            "Test set: Test loss: 0.8452, Accuracy: 3672/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.369031\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.372361\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.385002\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.403107\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.409196\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.426091\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.415557\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.423913\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.422140\n",
            "\n",
            "Test set: Test loss: 0.8414, Accuracy: 3698/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 73.96%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.357158\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.395458\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.357428\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.351848\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.385698\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.387401\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.392326\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.388847\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.400715\n",
            "\n",
            "Test set: Test loss: 0.8662, Accuracy: 3659/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.321357\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.336248\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.336260\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.357583\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.391194\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.368956\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.379188\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.383181\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.396818\n",
            "\n",
            "Test set: Test loss: 0.8495, Accuracy: 3693/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.307949\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.318280\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.321765\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.359826\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.357856\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.345771\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.368515\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.344725\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.374419\n",
            "\n",
            "Test set: Test loss: 0.8617, Accuracy: 3666/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.300917\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.285064\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.314622\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.315573\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.340106\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.323470\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.339247\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.319810\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.336731\n",
            "\n",
            "Test set: Test loss: 0.9035, Accuracy: 3681/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.277003\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.278518\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.304701\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.300111\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.336071\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.314411\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.346532\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.317876\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.326868\n",
            "\n",
            "Test set: Test loss: 0.8986, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 22: accuracy = 74.02%\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.250181\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.243336\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.277593\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.284874\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.311315\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.276460\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.306944\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.312999\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.319895\n",
            "\n",
            "Test set: Test loss: 0.9230, Accuracy: 3614/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.255498\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.248755\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.281449\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.269315\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.277553\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.274518\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.293482\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.280470\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.315351\n",
            "\n",
            "Test set: Test loss: 0.9629, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.250626\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.245213\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.257762\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.246369\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.268952\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.286254\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.286005\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.291988\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.299887\n",
            "\n",
            "Test set: Test loss: 0.9573, Accuracy: 3676/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.226415\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.209867\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.219202\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.231208\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.264113\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.250535\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.254933\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.288861\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.269480\n",
            "\n",
            "Test set: Test loss: 1.0078, Accuracy: 3615/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.218387\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.215055\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.224405\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.259055\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.259439\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.243627\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.262830\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.268378\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.273180\n",
            "\n",
            "Test set: Test loss: 0.9520, Accuracy: 3652/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.216043\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.196377\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.191867\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.219402\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.222381\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.239880\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.227526\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.261701\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.262062\n",
            "\n",
            "Test set: Test loss: 0.9866, Accuracy: 3671/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.200146\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.184421\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.195305\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.195491\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.201294\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.224302\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.227359\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.230581\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.246427\n",
            "\n",
            "Test set: Test loss: 1.0251, Accuracy: 3636/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.209732\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.203229\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.224441\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.216900\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.202984\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.193452\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.217368\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.214292\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.243616\n",
            "\n",
            "Test set: Test loss: 0.9785, Accuracy: 3689/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.174413\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.166284\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.172072\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.195843\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.183355\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.210250\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.216595\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.213259\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.237603\n",
            "\n",
            "Test set: Test loss: 0.9910, Accuracy: 3683/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.151303\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.160592\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.178565\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.191092\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.186781\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.193837\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.204696\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.208326\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.200269\n",
            "\n",
            "Test set: Test loss: 1.0013, Accuracy: 3669/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.154694\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.161606\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.169897\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.153970\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.185265\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.206072\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.173436\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.176872\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.174381\n",
            "\n",
            "Test set: Test loss: 1.0521, Accuracy: 3651/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.146977\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.171793\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.162135\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.164236\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.178158\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.168472\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.179568\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.198875\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.191775\n",
            "\n",
            "Test set: Test loss: 1.0221, Accuracy: 3656/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.147497\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.146355\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.163964\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.172245\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.175763\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.188472\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.182382\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.179349\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.203732\n",
            "\n",
            "Test set: Test loss: 1.0415, Accuracy: 3669/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.145144\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.143075\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.134713\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.133921\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.154448\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.163034\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.175858\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.160068\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.185646\n",
            "\n",
            "Test set: Test loss: 1.0674, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.128712\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.137316\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.138232\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.154612\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.163195\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.169111\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.134293\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.153297\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.163335\n",
            "\n",
            "Test set: Test loss: 1.0520, Accuracy: 3698/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.125180\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.115609\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.118690\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.130371\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.148062\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.153129\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.144350\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.149791\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.162746\n",
            "\n",
            "Test set: Test loss: 1.0819, Accuracy: 3705/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 38: accuracy = 74.1%\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.111226\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.107966\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.107646\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.154134\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.143847\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.163011\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.154644\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.146814\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.158946\n",
            "\n",
            "Test set: Test loss: 1.1001, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.134136\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.109513\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.121019\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.132715\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.132172\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.127141\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.126606\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.145561\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.153864\n",
            "\n",
            "Test set: Test loss: 1.1010, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.128909\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.095388\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.098697\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.120966\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.125986\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.136831\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.135708\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.143881\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.149139\n",
            "\n",
            "Test set: Test loss: 1.1134, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.118010\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.120005\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.099372\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.103266\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.133152\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.136610\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.129660\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.137610\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.127598\n",
            "\n",
            "Test set: Test loss: 1.1176, Accuracy: 3679/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.113821\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.107758\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.105341\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.112538\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.122740\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.108446\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.121302\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.120622\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.112086\n",
            "\n",
            "Test set: Test loss: 1.1351, Accuracy: 3717/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 43: accuracy = 74.34%\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.093068\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.108711\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.098154\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.118146\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.130284\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.123069\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.148246\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.132029\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.125744\n",
            "\n",
            "Test set: Test loss: 1.1160, Accuracy: 3682/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.127645\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.095558\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.106783\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.094002\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.106274\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.109935\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.095624\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.105619\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.123559\n",
            "\n",
            "Test set: Test loss: 1.1271, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.101942\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.082357\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.091027\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.105232\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.116211\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.115013\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.113036\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.101268\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.106477\n",
            "\n",
            "Test set: Test loss: 1.2027, Accuracy: 3672/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.108555\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.099601\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.086485\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.089404\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.094379\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.091846\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.103867\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.102662\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.112997\n",
            "\n",
            "Test set: Test loss: 1.1402, Accuracy: 3707/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.084986\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.096757\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.096644\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.100583\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.098394\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.098157\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.136053\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.108309\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.118050\n",
            "\n",
            "Test set: Test loss: 1.1626, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.085366\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.082822\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.077812\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.093440\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.097229\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.111108\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.088460\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.113893\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.105091\n",
            "\n",
            "Test set: Test loss: 1.1515, Accuracy: 3746/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 49: accuracy = 74.92%\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.095181\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.084641\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.071158\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.098875\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.084104\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.107015\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.115202\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.092292\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.102986\n",
            "\n",
            "Test set: Test loss: 1.1848, Accuracy: 3654/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.078114\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.082178\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.071729\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.090972\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.088045\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.092571\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.081105\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.088483\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.109322\n",
            "\n",
            "Test set: Test loss: 1.1678, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.071059\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.089735\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.084666\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.083506\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.086257\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.097880\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.104046\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.115997\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.103557\n",
            "\n",
            "Test set: Test loss: 1.1682, Accuracy: 3698/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.072538\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.071582\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.068064\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.073487\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.091180\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.100853\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.082964\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.105578\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.081159\n",
            "\n",
            "Test set: Test loss: 1.2534, Accuracy: 3697/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.077349\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.074151\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.075189\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.098706\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.082400\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.079062\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.090574\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.070265\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.078116\n",
            "\n",
            "Test set: Test loss: 1.2308, Accuracy: 3665/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.088167\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.086727\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.086900\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.074018\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.073779\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.064034\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.069490\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.079059\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.093103\n",
            "\n",
            "Test set: Test loss: 1.2309, Accuracy: 3686/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.074109\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.071818\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.068657\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.068585\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.075905\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.078632\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.074672\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.076123\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.104884\n",
            "\n",
            "Test set: Test loss: 1.1993, Accuracy: 3678/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.084345\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.087144\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.065440\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.072645\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.072298\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.076049\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.084039\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.080708\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.077405\n",
            "\n",
            "Test set: Test loss: 1.1814, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.068885\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.068100\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.066829\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.071883\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.084429\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.077428\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.071841\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.077277\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.083984\n",
            "\n",
            "Test set: Test loss: 1.1972, Accuracy: 3719/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.047608\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.056711\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.067039\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.057728\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.073319\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.070977\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.069419\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.069968\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.076190\n",
            "\n",
            "Test set: Test loss: 1.1781, Accuracy: 3730/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.068891\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.063114\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.058642\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.063219\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.066695\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.070938\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.069861\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.066537\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.079454\n",
            "\n",
            "Test set: Test loss: 1.1886, Accuracy: 3705/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.059230\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.061002\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.063708\n",
            "Train Epoch: 61 [20000/50000 (40%)]\tTrain Loss: 0.072940\n",
            "Train Epoch: 61 [25000/50000 (50%)]\tTrain Loss: 0.065550\n",
            "Train Epoch: 61 [30000/50000 (60%)]\tTrain Loss: 0.059016\n",
            "Train Epoch: 61 [35000/50000 (70%)]\tTrain Loss: 0.071011\n",
            "Train Epoch: 61 [40000/50000 (80%)]\tTrain Loss: 0.069782\n",
            "Train Epoch: 61 [45000/50000 (90%)]\tTrain Loss: 0.064795\n",
            "\n",
            "Test set: Test loss: 1.2189, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 62: lr = 0.1\n",
            "Train Epoch: 62 [5000/50000 (10%)]\tTrain Loss: 0.064249\n",
            "Train Epoch: 62 [10000/50000 (20%)]\tTrain Loss: 0.049978\n",
            "Train Epoch: 62 [15000/50000 (30%)]\tTrain Loss: 0.062478\n",
            "Train Epoch: 62 [20000/50000 (40%)]\tTrain Loss: 0.057541\n",
            "Train Epoch: 62 [25000/50000 (50%)]\tTrain Loss: 0.058807\n",
            "Train Epoch: 62 [30000/50000 (60%)]\tTrain Loss: 0.058891\n",
            "Train Epoch: 62 [35000/50000 (70%)]\tTrain Loss: 0.070179\n",
            "Train Epoch: 62 [40000/50000 (80%)]\tTrain Loss: 0.084471\n",
            "Train Epoch: 62 [45000/50000 (90%)]\tTrain Loss: 0.074923\n",
            "\n",
            "Test set: Test loss: 1.1869, Accuracy: 3697/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 63: lr = 0.1\n",
            "Train Epoch: 63 [5000/50000 (10%)]\tTrain Loss: 0.052693\n",
            "Train Epoch: 63 [10000/50000 (20%)]\tTrain Loss: 0.064444\n",
            "Train Epoch: 63 [15000/50000 (30%)]\tTrain Loss: 0.056115\n",
            "Train Epoch: 63 [20000/50000 (40%)]\tTrain Loss: 0.066243\n",
            "Train Epoch: 63 [25000/50000 (50%)]\tTrain Loss: 0.073832\n",
            "Train Epoch: 63 [30000/50000 (60%)]\tTrain Loss: 0.070268\n",
            "Train Epoch: 63 [35000/50000 (70%)]\tTrain Loss: 0.067326\n",
            "Train Epoch: 63 [40000/50000 (80%)]\tTrain Loss: 0.071065\n",
            "Train Epoch: 63 [45000/50000 (90%)]\tTrain Loss: 0.074999\n",
            "\n",
            "Test set: Test loss: 1.2147, Accuracy: 3711/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 64: lr = 0.1\n",
            "Train Epoch: 64 [5000/50000 (10%)]\tTrain Loss: 0.053628\n",
            "Train Epoch: 64 [10000/50000 (20%)]\tTrain Loss: 0.056498\n",
            "Train Epoch: 64 [15000/50000 (30%)]\tTrain Loss: 0.055907\n",
            "Train Epoch: 64 [20000/50000 (40%)]\tTrain Loss: 0.047080\n",
            "Train Epoch: 64 [25000/50000 (50%)]\tTrain Loss: 0.064704\n",
            "Train Epoch: 64 [30000/50000 (60%)]\tTrain Loss: 0.070877\n",
            "Train Epoch: 64 [35000/50000 (70%)]\tTrain Loss: 0.058385\n",
            "Train Epoch: 64 [40000/50000 (80%)]\tTrain Loss: 0.072175\n",
            "Train Epoch: 64 [45000/50000 (90%)]\tTrain Loss: 0.053934\n",
            "\n",
            "Test set: Test loss: 1.2624, Accuracy: 3722/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 65: lr = 0.1\n",
            "Train Epoch: 65 [5000/50000 (10%)]\tTrain Loss: 0.055259\n",
            "Train Epoch: 65 [10000/50000 (20%)]\tTrain Loss: 0.054332\n",
            "Train Epoch: 65 [15000/50000 (30%)]\tTrain Loss: 0.061180\n",
            "Train Epoch: 65 [20000/50000 (40%)]\tTrain Loss: 0.047117\n",
            "Train Epoch: 65 [25000/50000 (50%)]\tTrain Loss: 0.038936\n",
            "Train Epoch: 65 [30000/50000 (60%)]\tTrain Loss: 0.059979\n",
            "Train Epoch: 65 [35000/50000 (70%)]\tTrain Loss: 0.074613\n",
            "Train Epoch: 65 [40000/50000 (80%)]\tTrain Loss: 0.058811\n",
            "Train Epoch: 65 [45000/50000 (90%)]\tTrain Loss: 0.059155\n",
            "\n",
            "Test set: Test loss: 1.2698, Accuracy: 3700/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 66: lr = 0.1\n",
            "Train Epoch: 66 [5000/50000 (10%)]\tTrain Loss: 0.052684\n",
            "Train Epoch: 66 [10000/50000 (20%)]\tTrain Loss: 0.060744\n",
            "Train Epoch: 66 [15000/50000 (30%)]\tTrain Loss: 0.049435\n",
            "Train Epoch: 66 [20000/50000 (40%)]\tTrain Loss: 0.069029\n",
            "Train Epoch: 66 [25000/50000 (50%)]\tTrain Loss: 0.053748\n",
            "Train Epoch: 66 [30000/50000 (60%)]\tTrain Loss: 0.055105\n",
            "Train Epoch: 66 [35000/50000 (70%)]\tTrain Loss: 0.058580\n",
            "Train Epoch: 66 [40000/50000 (80%)]\tTrain Loss: 0.061437\n",
            "Train Epoch: 66 [45000/50000 (90%)]\tTrain Loss: 0.065909\n",
            "\n",
            "Test set: Test loss: 1.2588, Accuracy: 3725/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 67: lr = 0.1\n",
            "Train Epoch: 67 [5000/50000 (10%)]\tTrain Loss: 0.048963\n",
            "Train Epoch: 67 [10000/50000 (20%)]\tTrain Loss: 0.063767\n",
            "Train Epoch: 67 [15000/50000 (30%)]\tTrain Loss: 0.045817\n",
            "Train Epoch: 67 [20000/50000 (40%)]\tTrain Loss: 0.059452\n",
            "Train Epoch: 67 [25000/50000 (50%)]\tTrain Loss: 0.040365\n",
            "Train Epoch: 67 [30000/50000 (60%)]\tTrain Loss: 0.054925\n",
            "Train Epoch: 67 [35000/50000 (70%)]\tTrain Loss: 0.053652\n",
            "Train Epoch: 67 [40000/50000 (80%)]\tTrain Loss: 0.061453\n",
            "Train Epoch: 67 [45000/50000 (90%)]\tTrain Loss: 0.066566\n",
            "\n",
            "Test set: Test loss: 1.2303, Accuracy: 3722/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 68: lr = 0.1\n",
            "Train Epoch: 68 [5000/50000 (10%)]\tTrain Loss: 0.048017\n",
            "Train Epoch: 68 [10000/50000 (20%)]\tTrain Loss: 0.042613\n",
            "Train Epoch: 68 [15000/50000 (30%)]\tTrain Loss: 0.046106\n",
            "Train Epoch: 68 [20000/50000 (40%)]\tTrain Loss: 0.043754\n",
            "Train Epoch: 68 [25000/50000 (50%)]\tTrain Loss: 0.065266\n",
            "Train Epoch: 68 [30000/50000 (60%)]\tTrain Loss: 0.064687\n",
            "Train Epoch: 68 [35000/50000 (70%)]\tTrain Loss: 0.049253\n",
            "Train Epoch: 68 [40000/50000 (80%)]\tTrain Loss: 0.058058\n",
            "Train Epoch: 68 [45000/50000 (90%)]\tTrain Loss: 0.054325\n",
            "\n",
            "Test set: Test loss: 1.2490, Accuracy: 3727/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 69: lr = 0.1\n",
            "Train Epoch: 69 [5000/50000 (10%)]\tTrain Loss: 0.053233\n",
            "Train Epoch: 69 [10000/50000 (20%)]\tTrain Loss: 0.054599\n",
            "Train Epoch: 69 [15000/50000 (30%)]\tTrain Loss: 0.050400\n",
            "Train Epoch: 69 [20000/50000 (40%)]\tTrain Loss: 0.043966\n",
            "Train Epoch: 69 [25000/50000 (50%)]\tTrain Loss: 0.042055\n",
            "Train Epoch: 69 [30000/50000 (60%)]\tTrain Loss: 0.053503\n",
            "Train Epoch: 69 [35000/50000 (70%)]\tTrain Loss: 0.059178\n",
            "Train Epoch: 69 [40000/50000 (80%)]\tTrain Loss: 0.063553\n",
            "Train Epoch: 69 [45000/50000 (90%)]\tTrain Loss: 0.066277\n",
            "\n",
            "Test set: Test loss: 1.2434, Accuracy: 3740/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 70: lr = 0.1\n",
            "Train Epoch: 70 [5000/50000 (10%)]\tTrain Loss: 0.048461\n",
            "Train Epoch: 70 [10000/50000 (20%)]\tTrain Loss: 0.052561\n",
            "Train Epoch: 70 [15000/50000 (30%)]\tTrain Loss: 0.049064\n",
            "Train Epoch: 70 [20000/50000 (40%)]\tTrain Loss: 0.046098\n",
            "Train Epoch: 70 [25000/50000 (50%)]\tTrain Loss: 0.048955\n",
            "Train Epoch: 70 [30000/50000 (60%)]\tTrain Loss: 0.058078\n",
            "Train Epoch: 70 [35000/50000 (70%)]\tTrain Loss: 0.057487\n",
            "Train Epoch: 70 [40000/50000 (80%)]\tTrain Loss: 0.054462\n",
            "Train Epoch: 70 [45000/50000 (90%)]\tTrain Loss: 0.063055\n",
            "\n",
            "Test set: Test loss: 1.2494, Accuracy: 3729/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 71: lr = 0.1\n",
            "Train Epoch: 71 [5000/50000 (10%)]\tTrain Loss: 0.047349\n",
            "Train Epoch: 71 [10000/50000 (20%)]\tTrain Loss: 0.037374\n",
            "Train Epoch: 71 [15000/50000 (30%)]\tTrain Loss: 0.041998\n",
            "Train Epoch: 71 [20000/50000 (40%)]\tTrain Loss: 0.039587\n",
            "Train Epoch: 71 [25000/50000 (50%)]\tTrain Loss: 0.055022\n",
            "Train Epoch: 71 [30000/50000 (60%)]\tTrain Loss: 0.045930\n",
            "Train Epoch: 71 [35000/50000 (70%)]\tTrain Loss: 0.042231\n",
            "Train Epoch: 71 [40000/50000 (80%)]\tTrain Loss: 0.061458\n",
            "Train Epoch: 71 [45000/50000 (90%)]\tTrain Loss: 0.062152\n",
            "\n",
            "Test set: Test loss: 1.2405, Accuracy: 3723/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 72: lr = 0.1\n",
            "Train Epoch: 72 [5000/50000 (10%)]\tTrain Loss: 0.043285\n",
            "Train Epoch: 72 [10000/50000 (20%)]\tTrain Loss: 0.040052\n",
            "Train Epoch: 72 [15000/50000 (30%)]\tTrain Loss: 0.045972\n",
            "Train Epoch: 72 [20000/50000 (40%)]\tTrain Loss: 0.047176\n",
            "Train Epoch: 72 [25000/50000 (50%)]\tTrain Loss: 0.045233\n",
            "Train Epoch: 72 [30000/50000 (60%)]\tTrain Loss: 0.048956\n",
            "Train Epoch: 72 [35000/50000 (70%)]\tTrain Loss: 0.054548\n",
            "Train Epoch: 72 [40000/50000 (80%)]\tTrain Loss: 0.053283\n",
            "Train Epoch: 72 [45000/50000 (90%)]\tTrain Loss: 0.051217\n",
            "\n",
            "Test set: Test loss: 1.2837, Accuracy: 3732/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 73: lr = 0.1\n",
            "Train Epoch: 73 [5000/50000 (10%)]\tTrain Loss: 0.040280\n",
            "Train Epoch: 73 [10000/50000 (20%)]\tTrain Loss: 0.050718\n",
            "Train Epoch: 73 [15000/50000 (30%)]\tTrain Loss: 0.037018\n",
            "Train Epoch: 73 [20000/50000 (40%)]\tTrain Loss: 0.056670\n",
            "Train Epoch: 73 [25000/50000 (50%)]\tTrain Loss: 0.050839\n",
            "Train Epoch: 73 [30000/50000 (60%)]\tTrain Loss: 0.049220\n",
            "Train Epoch: 73 [35000/50000 (70%)]\tTrain Loss: 0.046593\n",
            "Train Epoch: 73 [40000/50000 (80%)]\tTrain Loss: 0.053152\n",
            "Train Epoch: 73 [45000/50000 (90%)]\tTrain Loss: 0.052229\n",
            "\n",
            "Test set: Test loss: 1.2498, Accuracy: 3745/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 74: lr = 0.1\n",
            "Train Epoch: 74 [5000/50000 (10%)]\tTrain Loss: 0.034746\n",
            "Train Epoch: 74 [10000/50000 (20%)]\tTrain Loss: 0.044873\n",
            "Train Epoch: 74 [15000/50000 (30%)]\tTrain Loss: 0.046251\n",
            "Train Epoch: 74 [20000/50000 (40%)]\tTrain Loss: 0.038261\n",
            "Train Epoch: 74 [25000/50000 (50%)]\tTrain Loss: 0.051695\n",
            "Train Epoch: 74 [30000/50000 (60%)]\tTrain Loss: 0.049300\n",
            "Train Epoch: 74 [35000/50000 (70%)]\tTrain Loss: 0.051135\n",
            "Train Epoch: 74 [40000/50000 (80%)]\tTrain Loss: 0.057700\n",
            "Train Epoch: 74 [45000/50000 (90%)]\tTrain Loss: 0.039607\n",
            "\n",
            "Test set: Test loss: 1.2616, Accuracy: 3761/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 74: accuracy = 75.22%\n",
            "\n",
            "Train Epoch 75: lr = 0.1\n",
            "Train Epoch: 75 [5000/50000 (10%)]\tTrain Loss: 0.036275\n",
            "Train Epoch: 75 [10000/50000 (20%)]\tTrain Loss: 0.047355\n",
            "Train Epoch: 75 [15000/50000 (30%)]\tTrain Loss: 0.039838\n",
            "Train Epoch: 75 [20000/50000 (40%)]\tTrain Loss: 0.041933\n",
            "Train Epoch: 75 [25000/50000 (50%)]\tTrain Loss: 0.043909\n",
            "Train Epoch: 75 [30000/50000 (60%)]\tTrain Loss: 0.051062\n",
            "Train Epoch: 75 [35000/50000 (70%)]\tTrain Loss: 0.050254\n",
            "Train Epoch: 75 [40000/50000 (80%)]\tTrain Loss: 0.045275\n",
            "Train Epoch: 75 [45000/50000 (90%)]\tTrain Loss: 0.064026\n",
            "\n",
            "Test set: Test loss: 1.2508, Accuracy: 3752/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 76: lr = 0.1\n",
            "Train Epoch: 76 [5000/50000 (10%)]\tTrain Loss: 0.038635\n",
            "Train Epoch: 76 [10000/50000 (20%)]\tTrain Loss: 0.045951\n",
            "Train Epoch: 76 [15000/50000 (30%)]\tTrain Loss: 0.032019\n",
            "Train Epoch: 76 [20000/50000 (40%)]\tTrain Loss: 0.045700\n",
            "Train Epoch: 76 [25000/50000 (50%)]\tTrain Loss: 0.052230\n",
            "Train Epoch: 76 [30000/50000 (60%)]\tTrain Loss: 0.055918\n",
            "Train Epoch: 76 [35000/50000 (70%)]\tTrain Loss: 0.048639\n",
            "Train Epoch: 76 [40000/50000 (80%)]\tTrain Loss: 0.059151\n",
            "Train Epoch: 76 [45000/50000 (90%)]\tTrain Loss: 0.046912\n",
            "\n",
            "Test set: Test loss: 1.3068, Accuracy: 3740/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 77: lr = 0.1\n",
            "Train Epoch: 77 [5000/50000 (10%)]\tTrain Loss: 0.029386\n",
            "Train Epoch: 77 [10000/50000 (20%)]\tTrain Loss: 0.035816\n",
            "Train Epoch: 77 [15000/50000 (30%)]\tTrain Loss: 0.033614\n",
            "Train Epoch: 77 [20000/50000 (40%)]\tTrain Loss: 0.035148\n",
            "Train Epoch: 77 [25000/50000 (50%)]\tTrain Loss: 0.034950\n",
            "Train Epoch: 77 [30000/50000 (60%)]\tTrain Loss: 0.036712\n",
            "Train Epoch: 77 [35000/50000 (70%)]\tTrain Loss: 0.040101\n",
            "Train Epoch: 77 [40000/50000 (80%)]\tTrain Loss: 0.037602\n",
            "Train Epoch: 77 [45000/50000 (90%)]\tTrain Loss: 0.035620\n",
            "\n",
            "Test set: Test loss: 1.3140, Accuracy: 3746/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 78: lr = 0.1\n",
            "Train Epoch: 78 [5000/50000 (10%)]\tTrain Loss: 0.043269\n",
            "Train Epoch: 78 [10000/50000 (20%)]\tTrain Loss: 0.043798\n",
            "Train Epoch: 78 [15000/50000 (30%)]\tTrain Loss: 0.046150\n",
            "Train Epoch: 78 [20000/50000 (40%)]\tTrain Loss: 0.046101\n",
            "Train Epoch: 78 [25000/50000 (50%)]\tTrain Loss: 0.054744\n",
            "Train Epoch: 78 [30000/50000 (60%)]\tTrain Loss: 0.058816\n",
            "Train Epoch: 78 [35000/50000 (70%)]\tTrain Loss: 0.040837\n",
            "Train Epoch: 78 [40000/50000 (80%)]\tTrain Loss: 0.045084\n",
            "Train Epoch: 78 [45000/50000 (90%)]\tTrain Loss: 0.049561\n",
            "\n",
            "Test set: Test loss: 1.2759, Accuracy: 3760/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 79: lr = 0.1\n",
            "Train Epoch: 79 [5000/50000 (10%)]\tTrain Loss: 0.036376\n",
            "Train Epoch: 79 [10000/50000 (20%)]\tTrain Loss: 0.044606\n",
            "Train Epoch: 79 [15000/50000 (30%)]\tTrain Loss: 0.045786\n",
            "Train Epoch: 79 [20000/50000 (40%)]\tTrain Loss: 0.046872\n",
            "Train Epoch: 79 [25000/50000 (50%)]\tTrain Loss: 0.049026\n",
            "Train Epoch: 79 [30000/50000 (60%)]\tTrain Loss: 0.043462\n",
            "Train Epoch: 79 [35000/50000 (70%)]\tTrain Loss: 0.045755\n",
            "Train Epoch: 79 [40000/50000 (80%)]\tTrain Loss: 0.041733\n",
            "Train Epoch: 79 [45000/50000 (90%)]\tTrain Loss: 0.050911\n",
            "\n",
            "Test set: Test loss: 1.2690, Accuracy: 3735/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 80: lr = 0.1\n",
            "Train Epoch: 80 [5000/50000 (10%)]\tTrain Loss: 0.045390\n",
            "Train Epoch: 80 [10000/50000 (20%)]\tTrain Loss: 0.037588\n",
            "Train Epoch: 80 [15000/50000 (30%)]\tTrain Loss: 0.034809\n",
            "Train Epoch: 80 [20000/50000 (40%)]\tTrain Loss: 0.034272\n",
            "Train Epoch: 80 [25000/50000 (50%)]\tTrain Loss: 0.053583\n",
            "Train Epoch: 80 [30000/50000 (60%)]\tTrain Loss: 0.050677\n",
            "Train Epoch: 80 [35000/50000 (70%)]\tTrain Loss: 0.047012\n",
            "Train Epoch: 80 [40000/50000 (80%)]\tTrain Loss: 0.041571\n",
            "Train Epoch: 80 [45000/50000 (90%)]\tTrain Loss: 0.051927\n",
            "\n",
            "Test set: Test loss: 1.2379, Accuracy: 3772/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 80: accuracy = 75.44%\n",
            "\n",
            "Train Epoch 81: lr = 0.1\n",
            "Train Epoch: 81 [5000/50000 (10%)]\tTrain Loss: 0.042632\n",
            "Train Epoch: 81 [10000/50000 (20%)]\tTrain Loss: 0.027828\n",
            "Train Epoch: 81 [15000/50000 (30%)]\tTrain Loss: 0.027677\n",
            "Train Epoch: 81 [20000/50000 (40%)]\tTrain Loss: 0.026077\n",
            "Train Epoch: 81 [25000/50000 (50%)]\tTrain Loss: 0.035509\n",
            "Train Epoch: 81 [30000/50000 (60%)]\tTrain Loss: 0.037286\n",
            "Train Epoch: 81 [35000/50000 (70%)]\tTrain Loss: 0.046880\n",
            "Train Epoch: 81 [40000/50000 (80%)]\tTrain Loss: 0.038450\n",
            "Train Epoch: 81 [45000/50000 (90%)]\tTrain Loss: 0.050416\n",
            "\n",
            "Test set: Test loss: 1.2601, Accuracy: 3727/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 82: lr = 0.1\n",
            "Train Epoch: 82 [5000/50000 (10%)]\tTrain Loss: 0.036014\n",
            "Train Epoch: 82 [10000/50000 (20%)]\tTrain Loss: 0.031962\n",
            "Train Epoch: 82 [15000/50000 (30%)]\tTrain Loss: 0.029811\n",
            "Train Epoch: 82 [20000/50000 (40%)]\tTrain Loss: 0.037019\n",
            "Train Epoch: 82 [25000/50000 (50%)]\tTrain Loss: 0.045695\n",
            "Train Epoch: 82 [30000/50000 (60%)]\tTrain Loss: 0.041374\n",
            "Train Epoch: 82 [35000/50000 (70%)]\tTrain Loss: 0.028334\n",
            "Train Epoch: 82 [40000/50000 (80%)]\tTrain Loss: 0.037767\n",
            "Train Epoch: 82 [45000/50000 (90%)]\tTrain Loss: 0.044564\n",
            "\n",
            "Test set: Test loss: 1.2797, Accuracy: 3737/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 83: lr = 0.1\n",
            "Train Epoch: 83 [5000/50000 (10%)]\tTrain Loss: 0.034229\n",
            "Train Epoch: 83 [10000/50000 (20%)]\tTrain Loss: 0.040680\n",
            "Train Epoch: 83 [15000/50000 (30%)]\tTrain Loss: 0.037850\n",
            "Train Epoch: 83 [20000/50000 (40%)]\tTrain Loss: 0.029248\n",
            "Train Epoch: 83 [25000/50000 (50%)]\tTrain Loss: 0.039386\n",
            "Train Epoch: 83 [30000/50000 (60%)]\tTrain Loss: 0.043239\n",
            "Train Epoch: 83 [35000/50000 (70%)]\tTrain Loss: 0.046915\n",
            "Train Epoch: 83 [40000/50000 (80%)]\tTrain Loss: 0.056024\n",
            "Train Epoch: 83 [45000/50000 (90%)]\tTrain Loss: 0.053113\n",
            "\n",
            "Test set: Test loss: 1.2784, Accuracy: 3735/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 84: lr = 0.1\n",
            "Train Epoch: 84 [5000/50000 (10%)]\tTrain Loss: 0.037258\n",
            "Train Epoch: 84 [10000/50000 (20%)]\tTrain Loss: 0.041529\n",
            "Train Epoch: 84 [15000/50000 (30%)]\tTrain Loss: 0.037707\n",
            "Train Epoch: 84 [20000/50000 (40%)]\tTrain Loss: 0.032181\n",
            "Train Epoch: 84 [25000/50000 (50%)]\tTrain Loss: 0.039967\n",
            "Train Epoch: 84 [30000/50000 (60%)]\tTrain Loss: 0.039109\n",
            "Train Epoch: 84 [35000/50000 (70%)]\tTrain Loss: 0.045569\n",
            "Train Epoch: 84 [40000/50000 (80%)]\tTrain Loss: 0.051811\n",
            "Train Epoch: 84 [45000/50000 (90%)]\tTrain Loss: 0.031203\n",
            "\n",
            "Test set: Test loss: 1.2645, Accuracy: 3759/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 85: lr = 0.1\n",
            "Train Epoch: 85 [5000/50000 (10%)]\tTrain Loss: 0.035391\n",
            "Train Epoch: 85 [10000/50000 (20%)]\tTrain Loss: 0.039475\n",
            "Train Epoch: 85 [15000/50000 (30%)]\tTrain Loss: 0.039322\n",
            "Train Epoch: 85 [20000/50000 (40%)]\tTrain Loss: 0.036833\n",
            "Train Epoch: 85 [25000/50000 (50%)]\tTrain Loss: 0.033743\n",
            "Train Epoch: 85 [30000/50000 (60%)]\tTrain Loss: 0.043292\n",
            "Train Epoch: 85 [35000/50000 (70%)]\tTrain Loss: 0.031767\n",
            "Train Epoch: 85 [40000/50000 (80%)]\tTrain Loss: 0.031526\n",
            "Train Epoch: 85 [45000/50000 (90%)]\tTrain Loss: 0.029450\n",
            "\n",
            "Test set: Test loss: 1.3283, Accuracy: 3744/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 86: lr = 0.1\n",
            "Train Epoch: 86 [5000/50000 (10%)]\tTrain Loss: 0.045037\n",
            "Train Epoch: 86 [10000/50000 (20%)]\tTrain Loss: 0.032449\n",
            "Train Epoch: 86 [15000/50000 (30%)]\tTrain Loss: 0.034992\n",
            "Train Epoch: 86 [20000/50000 (40%)]\tTrain Loss: 0.041742\n",
            "Train Epoch: 86 [25000/50000 (50%)]\tTrain Loss: 0.034939\n",
            "Train Epoch: 86 [30000/50000 (60%)]\tTrain Loss: 0.048452\n",
            "Train Epoch: 86 [35000/50000 (70%)]\tTrain Loss: 0.045242\n",
            "Train Epoch: 86 [40000/50000 (80%)]\tTrain Loss: 0.044087\n",
            "Train Epoch: 86 [45000/50000 (90%)]\tTrain Loss: 0.047667\n",
            "\n",
            "Test set: Test loss: 1.3112, Accuracy: 3733/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 87: lr = 0.1\n",
            "Train Epoch: 87 [5000/50000 (10%)]\tTrain Loss: 0.031369\n",
            "Train Epoch: 87 [10000/50000 (20%)]\tTrain Loss: 0.027319\n",
            "Train Epoch: 87 [15000/50000 (30%)]\tTrain Loss: 0.028193\n",
            "Train Epoch: 87 [20000/50000 (40%)]\tTrain Loss: 0.039283\n",
            "Train Epoch: 87 [25000/50000 (50%)]\tTrain Loss: 0.036619\n",
            "Train Epoch: 87 [30000/50000 (60%)]\tTrain Loss: 0.041849\n",
            "Train Epoch: 87 [35000/50000 (70%)]\tTrain Loss: 0.034033\n",
            "Train Epoch: 87 [40000/50000 (80%)]\tTrain Loss: 0.044910\n",
            "Train Epoch: 87 [45000/50000 (90%)]\tTrain Loss: 0.030465\n",
            "\n",
            "Test set: Test loss: 1.2888, Accuracy: 3761/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 88: lr = 0.1\n",
            "Train Epoch: 88 [5000/50000 (10%)]\tTrain Loss: 0.036197\n",
            "Train Epoch: 88 [10000/50000 (20%)]\tTrain Loss: 0.038479\n",
            "Train Epoch: 88 [15000/50000 (30%)]\tTrain Loss: 0.037809\n",
            "Train Epoch: 88 [20000/50000 (40%)]\tTrain Loss: 0.031603\n",
            "Train Epoch: 88 [25000/50000 (50%)]\tTrain Loss: 0.041897\n",
            "Train Epoch: 88 [30000/50000 (60%)]\tTrain Loss: 0.034141\n",
            "Train Epoch: 88 [35000/50000 (70%)]\tTrain Loss: 0.040524\n",
            "Train Epoch: 88 [40000/50000 (80%)]\tTrain Loss: 0.029852\n",
            "Train Epoch: 88 [45000/50000 (90%)]\tTrain Loss: 0.028368\n",
            "\n",
            "Test set: Test loss: 1.3667, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 89: lr = 0.1\n",
            "Train Epoch: 89 [5000/50000 (10%)]\tTrain Loss: 0.029294\n",
            "Train Epoch: 89 [10000/50000 (20%)]\tTrain Loss: 0.027893\n",
            "Train Epoch: 89 [15000/50000 (30%)]\tTrain Loss: 0.039494\n",
            "Train Epoch: 89 [20000/50000 (40%)]\tTrain Loss: 0.042287\n",
            "Train Epoch: 89 [25000/50000 (50%)]\tTrain Loss: 0.032153\n",
            "Train Epoch: 89 [30000/50000 (60%)]\tTrain Loss: 0.035177\n",
            "Train Epoch: 89 [35000/50000 (70%)]\tTrain Loss: 0.039284\n",
            "Train Epoch: 89 [40000/50000 (80%)]\tTrain Loss: 0.038517\n",
            "Train Epoch: 89 [45000/50000 (90%)]\tTrain Loss: 0.041512\n",
            "\n",
            "Test set: Test loss: 1.3069, Accuracy: 3746/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 90: lr = 0.1\n",
            "Train Epoch: 90 [5000/50000 (10%)]\tTrain Loss: 0.034032\n",
            "Train Epoch: 90 [10000/50000 (20%)]\tTrain Loss: 0.033002\n",
            "Train Epoch: 90 [15000/50000 (30%)]\tTrain Loss: 0.047886\n",
            "Train Epoch: 90 [20000/50000 (40%)]\tTrain Loss: 0.032973\n",
            "Train Epoch: 90 [25000/50000 (50%)]\tTrain Loss: 0.038026\n",
            "Train Epoch: 90 [30000/50000 (60%)]\tTrain Loss: 0.034467\n",
            "Train Epoch: 90 [35000/50000 (70%)]\tTrain Loss: 0.037608\n",
            "Train Epoch: 90 [40000/50000 (80%)]\tTrain Loss: 0.034961\n",
            "Train Epoch: 90 [45000/50000 (90%)]\tTrain Loss: 0.038852\n",
            "\n",
            "Test set: Test loss: 1.2892, Accuracy: 3798/5000 (76%)\n",
            "\n",
            "Better accuracy at Epoch 90: accuracy = 75.96%\n",
            "\n",
            "Train Epoch 91: lr = 0.1\n",
            "Train Epoch: 91 [5000/50000 (10%)]\tTrain Loss: 0.030590\n",
            "Train Epoch: 91 [10000/50000 (20%)]\tTrain Loss: 0.028849\n",
            "Train Epoch: 91 [15000/50000 (30%)]\tTrain Loss: 0.033606\n",
            "Train Epoch: 91 [20000/50000 (40%)]\tTrain Loss: 0.026042\n",
            "Train Epoch: 91 [25000/50000 (50%)]\tTrain Loss: 0.032477\n",
            "Train Epoch: 91 [30000/50000 (60%)]\tTrain Loss: 0.043815\n",
            "Train Epoch: 91 [35000/50000 (70%)]\tTrain Loss: 0.039109\n",
            "Train Epoch: 91 [40000/50000 (80%)]\tTrain Loss: 0.030099\n",
            "Train Epoch: 91 [45000/50000 (90%)]\tTrain Loss: 0.038164\n",
            "\n",
            "Test set: Test loss: 1.3136, Accuracy: 3714/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 92: lr = 0.1\n",
            "Train Epoch: 92 [5000/50000 (10%)]\tTrain Loss: 0.025382\n",
            "Train Epoch: 92 [10000/50000 (20%)]\tTrain Loss: 0.029170\n",
            "Train Epoch: 92 [15000/50000 (30%)]\tTrain Loss: 0.035066\n",
            "Train Epoch: 92 [20000/50000 (40%)]\tTrain Loss: 0.036666\n",
            "Train Epoch: 92 [25000/50000 (50%)]\tTrain Loss: 0.028044\n",
            "Train Epoch: 92 [30000/50000 (60%)]\tTrain Loss: 0.036170\n",
            "Train Epoch: 92 [35000/50000 (70%)]\tTrain Loss: 0.031926\n",
            "Train Epoch: 92 [40000/50000 (80%)]\tTrain Loss: 0.028979\n",
            "Train Epoch: 92 [45000/50000 (90%)]\tTrain Loss: 0.035423\n",
            "\n",
            "Test set: Test loss: 1.2942, Accuracy: 3732/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 93: lr = 0.1\n",
            "Train Epoch: 93 [5000/50000 (10%)]\tTrain Loss: 0.023340\n",
            "Train Epoch: 93 [10000/50000 (20%)]\tTrain Loss: 0.027071\n",
            "Train Epoch: 93 [15000/50000 (30%)]\tTrain Loss: 0.031900\n",
            "Train Epoch: 93 [20000/50000 (40%)]\tTrain Loss: 0.034822\n",
            "Train Epoch: 93 [25000/50000 (50%)]\tTrain Loss: 0.043964\n",
            "Train Epoch: 93 [30000/50000 (60%)]\tTrain Loss: 0.033153\n",
            "Train Epoch: 93 [35000/50000 (70%)]\tTrain Loss: 0.041727\n",
            "Train Epoch: 93 [40000/50000 (80%)]\tTrain Loss: 0.031737\n",
            "Train Epoch: 93 [45000/50000 (90%)]\tTrain Loss: 0.041787\n",
            "\n",
            "Test set: Test loss: 1.3284, Accuracy: 3731/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 94: lr = 0.1\n",
            "Train Epoch: 94 [5000/50000 (10%)]\tTrain Loss: 0.021029\n",
            "Train Epoch: 94 [10000/50000 (20%)]\tTrain Loss: 0.023152\n",
            "Train Epoch: 94 [15000/50000 (30%)]\tTrain Loss: 0.037508\n",
            "Train Epoch: 94 [20000/50000 (40%)]\tTrain Loss: 0.023847\n",
            "Train Epoch: 94 [25000/50000 (50%)]\tTrain Loss: 0.024558\n",
            "Train Epoch: 94 [30000/50000 (60%)]\tTrain Loss: 0.025709\n",
            "Train Epoch: 94 [35000/50000 (70%)]\tTrain Loss: 0.026009\n",
            "Train Epoch: 94 [40000/50000 (80%)]\tTrain Loss: 0.028688\n",
            "Train Epoch: 94 [45000/50000 (90%)]\tTrain Loss: 0.032983\n",
            "\n",
            "Test set: Test loss: 1.3495, Accuracy: 3722/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 95: lr = 0.1\n",
            "Train Epoch: 95 [5000/50000 (10%)]\tTrain Loss: 0.029071\n",
            "Train Epoch: 95 [10000/50000 (20%)]\tTrain Loss: 0.028753\n",
            "Train Epoch: 95 [15000/50000 (30%)]\tTrain Loss: 0.035100\n",
            "Train Epoch: 95 [20000/50000 (40%)]\tTrain Loss: 0.027597\n",
            "Train Epoch: 95 [25000/50000 (50%)]\tTrain Loss: 0.033076\n",
            "Train Epoch: 95 [30000/50000 (60%)]\tTrain Loss: 0.039192\n",
            "Train Epoch: 95 [35000/50000 (70%)]\tTrain Loss: 0.023832\n",
            "Train Epoch: 95 [40000/50000 (80%)]\tTrain Loss: 0.029793\n",
            "Train Epoch: 95 [45000/50000 (90%)]\tTrain Loss: 0.033859\n",
            "\n",
            "Test set: Test loss: 1.3718, Accuracy: 3728/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 96: lr = 0.1\n",
            "Train Epoch: 96 [5000/50000 (10%)]\tTrain Loss: 0.035246\n",
            "Train Epoch: 96 [10000/50000 (20%)]\tTrain Loss: 0.027858\n",
            "Train Epoch: 96 [15000/50000 (30%)]\tTrain Loss: 0.035870\n",
            "Train Epoch: 96 [20000/50000 (40%)]\tTrain Loss: 0.032043\n",
            "Train Epoch: 96 [25000/50000 (50%)]\tTrain Loss: 0.030446\n",
            "Train Epoch: 96 [30000/50000 (60%)]\tTrain Loss: 0.025188\n",
            "Train Epoch: 96 [35000/50000 (70%)]\tTrain Loss: 0.024972\n",
            "Train Epoch: 96 [40000/50000 (80%)]\tTrain Loss: 0.033098\n",
            "Train Epoch: 96 [45000/50000 (90%)]\tTrain Loss: 0.030755\n",
            "\n",
            "Test set: Test loss: 1.3175, Accuracy: 3772/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 97: lr = 0.1\n",
            "Train Epoch: 97 [5000/50000 (10%)]\tTrain Loss: 0.029951\n",
            "Train Epoch: 97 [10000/50000 (20%)]\tTrain Loss: 0.024483\n",
            "Train Epoch: 97 [15000/50000 (30%)]\tTrain Loss: 0.029166\n",
            "Train Epoch: 97 [20000/50000 (40%)]\tTrain Loss: 0.037227\n",
            "Train Epoch: 97 [25000/50000 (50%)]\tTrain Loss: 0.035764\n",
            "Train Epoch: 97 [30000/50000 (60%)]\tTrain Loss: 0.036645\n",
            "Train Epoch: 97 [35000/50000 (70%)]\tTrain Loss: 0.023147\n",
            "Train Epoch: 97 [40000/50000 (80%)]\tTrain Loss: 0.029128\n",
            "Train Epoch: 97 [45000/50000 (90%)]\tTrain Loss: 0.037504\n",
            "\n",
            "Test set: Test loss: 1.3699, Accuracy: 3716/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 98: lr = 0.1\n",
            "Train Epoch: 98 [5000/50000 (10%)]\tTrain Loss: 0.021976\n",
            "Train Epoch: 98 [10000/50000 (20%)]\tTrain Loss: 0.023663\n",
            "Train Epoch: 98 [15000/50000 (30%)]\tTrain Loss: 0.024028\n",
            "Train Epoch: 98 [20000/50000 (40%)]\tTrain Loss: 0.022424\n",
            "Train Epoch: 98 [25000/50000 (50%)]\tTrain Loss: 0.033249\n",
            "Train Epoch: 98 [30000/50000 (60%)]\tTrain Loss: 0.031182\n",
            "Train Epoch: 98 [35000/50000 (70%)]\tTrain Loss: 0.034796\n",
            "Train Epoch: 98 [40000/50000 (80%)]\tTrain Loss: 0.033328\n",
            "Train Epoch: 98 [45000/50000 (90%)]\tTrain Loss: 0.034444\n",
            "\n",
            "Test set: Test loss: 1.3353, Accuracy: 3760/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 99: lr = 0.1\n",
            "Train Epoch: 99 [5000/50000 (10%)]\tTrain Loss: 0.028030\n",
            "Train Epoch: 99 [10000/50000 (20%)]\tTrain Loss: 0.039447\n",
            "Train Epoch: 99 [15000/50000 (30%)]\tTrain Loss: 0.025989\n",
            "Train Epoch: 99 [20000/50000 (40%)]\tTrain Loss: 0.027849\n",
            "Train Epoch: 99 [25000/50000 (50%)]\tTrain Loss: 0.026666\n",
            "Train Epoch: 99 [30000/50000 (60%)]\tTrain Loss: 0.032007\n",
            "Train Epoch: 99 [35000/50000 (70%)]\tTrain Loss: 0.021876\n",
            "Train Epoch: 99 [40000/50000 (80%)]\tTrain Loss: 0.029725\n",
            "Train Epoch: 99 [45000/50000 (90%)]\tTrain Loss: 0.038894\n",
            "\n",
            "Test set: Test loss: 1.3427, Accuracy: 3777/5000 (76%)\n",
            "\n",
            "CPU times: user 53min 9s, sys: 50.3 s, total: 54min\n",
            "Wall time: 56min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugwBdwv0E4ml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "162fa1b6-97f1-44ed-a131-814deffb07ea"
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet Residual (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet Residual (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5ey2pnT5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 34.08191934023989 seconds\n",
            "Best accuracy: 75.96  Best training loss: 0.0006644439417868853  Best validation loss: 0.7837129467725754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y1nPpcpFB97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3e7abf1f-269f-49f6-af2e-152bcd601f23"
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "[1.3865227699279785, 1.2518517971038818, 1.3459795713424683, 1.1654871702194214, 1.0572248697280884, 0.736412525177002, 0.6382110118865967, 0.674691915512085, 0.5869726538658142, 0.9272210001945496, 0.7194206714630127, 0.9517215490341187, 0.7132426500320435, 0.4628666937351227, 0.7847063541412354, 0.5439581274986267, 0.46174371242523193, 0.38576969504356384, 0.34096577763557434, 0.18660350143909454, 0.30444198846817017, 0.22655923664569855, 0.4085098206996918, 0.34931668639183044, 0.23311159014701843, 0.38107284903526306, 0.3030513823032379, 0.19187286496162415, 0.2670351266860962, 0.5107146501541138, 0.17877079546451569, 0.29285958409309387, 0.15092739462852478, 0.12588492035865784, 0.07349696010351181, 0.1096775233745575, 0.19827377796173096, 0.18524542450904846, 0.2110908180475235, 0.16789622604846954, 0.033203087747097015, 0.2108043134212494, 0.2309979498386383, 0.039499808102846146, 0.03846338391304016, 0.213524729013443, 0.048338327556848526, 0.12311996519565582, 0.11203350871801376, 0.1131778433918953, 0.05028429627418518, 0.04734444618225098, 0.10932132601737976, 0.1845545768737793, 0.16836334764957428, 0.16844268143177032, 0.043377455323934555, 0.06000225991010666, 0.003209619550034404, 0.07091379165649414, 0.10109801590442657, 0.1611541211605072, 0.11402630060911179, 0.0362202450633049, 0.004046192392706871, 0.05020032078027725, 0.08717114478349686, 0.026214761659502983, 0.09777407348155975, 0.18801246583461761, 0.04056987911462784, 0.02017993852496147, 0.13877835869789124, 0.09805114567279816, 0.01577465981245041, 0.010314597748219967, 0.005131940823048353, 0.10586550831794739, 0.035762518644332886, 0.11209654062986374, 0.03656663000583649, 0.035967856645584106, 0.00609211903065443, 0.00825545284897089, 0.0023326778318732977, 0.03726593032479286, 0.23470215499401093, 0.009308194741606712, 0.12354776263237, 0.030797334387898445, 0.04234562814235687, 0.07621195167303085, 0.03218308463692665, 0.004538011737167835, 0.002467040903866291, 0.005364084150642157, 0.006213979795575142, 0.0006644439417868853, 0.01297320332378149, 0.0009643363882787526]\n",
            "[1.3605176353454589, 1.159411541223526, 1.0270540475845338, 0.980360300540924, 0.9803732603788375, 0.9210622584819794, 0.8651850792765617, 0.8501198822259903, 0.8194299104809761, 0.8152713599801064, 0.8477149298787117, 0.7837129467725754, 0.8469611844420433, 0.8196620732545853, 0.8187662988901139, 0.8201657792925835, 0.8452438545227051, 0.8414278244972229, 0.8662025213241578, 0.8495144438743591, 0.861734549999237, 0.9034508684277535, 0.8985910587012768, 0.9230197241902351, 0.9628700244426728, 0.9573292529582977, 1.0078306034207345, 0.9520011842250824, 0.986562195122242, 1.0251235738396645, 0.9785093283653259, 0.9910295122861862, 1.001313408613205, 1.0520796179771423, 1.0220537516474724, 1.041510402560234, 1.067388856112957, 1.0519605439901352, 1.0819401228427887, 1.1001266700029373, 1.1009626519680022, 1.1133876278996468, 1.117610176205635, 1.1350766927003861, 1.1159869596362113, 1.1271492099761964, 1.2027384182810783, 1.1401582372188568, 1.1626129513978958, 1.1515131175518036, 1.184805113375187, 1.1678331980109216, 1.168226275742054, 1.2534448078274727, 1.2307891488075255, 1.2308836641907692, 1.1992921721935272, 1.1814400607347488, 1.1972293293476104, 1.1780537840723992, 1.188604792058468, 1.2189062333106995, 1.186869990825653, 1.2146682733297347, 1.262364194393158, 1.2697842133045196, 1.258816929459572, 1.2302995824813843, 1.24904600918293, 1.243371739387512, 1.2493526312708854, 1.240482140481472, 1.2837053474783897, 1.2498381325602532, 1.2616019234061242, 1.250836768746376, 1.306771864593029, 1.313998658657074, 1.275932187139988, 1.2690372544527053, 1.2378605568408967, 1.2600594621896744, 1.2796865770220756, 1.2784471985697747, 1.2645366376638412, 1.3283256785571576, 1.3111711490154265, 1.2887797424197196, 1.3666978549957276, 1.3068515592813492, 1.2891816048324107, 1.3136011385917663, 1.294228127449751, 1.3283586636185647, 1.3494662874937058, 1.3718096134066582, 1.3175030466914177, 1.3698695740103721, 1.3352974377572537, 1.342706640958786]\n",
            "[52.88, 60.24, 64.16, 65.46, 65.96, 68.12, 69.72, 70.58, 71.66, 71.78, 71.5, 73.88, 72.34, 72.96, 72.8, 73.64, 73.44, 73.96, 73.18, 73.86, 73.32, 73.62, 74.02, 72.28, 72.82, 73.52, 72.3, 73.04, 73.42, 72.72, 73.78, 73.66, 73.38, 73.02, 73.12, 73.38, 73.92, 73.96, 74.1, 73.06, 73.92, 73.82, 73.58, 74.34, 73.64, 74.02, 73.44, 74.14, 73.7, 74.92, 73.08, 74.06, 73.96, 73.94, 73.3, 73.72, 73.56, 73.74, 74.38, 74.6, 74.1, 74.02, 73.94, 74.22, 74.44, 74.0, 74.5, 74.44, 74.54, 74.8, 74.58, 74.46, 74.64, 74.9, 75.22, 75.04, 74.8, 74.92, 75.2, 74.7, 75.44, 74.54, 74.74, 74.7, 75.18, 74.88, 74.66, 75.22, 73.68, 74.92, 75.96, 74.28, 74.64, 74.62, 74.44, 74.56, 75.44, 74.32, 75.2, 75.54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZOK25_FCZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}