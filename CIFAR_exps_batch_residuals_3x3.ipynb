{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_exps_batch_residuals_3x3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu83Jf5qHY_l",
        "colab_type": "code",
        "outputId": "3d593c0f-6655-4c29-8a59-3f53a290ef54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import torchvision\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYTXiSckMwDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%cd /content/\n",
        "#!/usr/bin/python\n",
        "# essential imports\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "import imageio\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.ion() \n",
        "import numpy as np\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import copy\n",
        "import pickle\n",
        "#tensorflow 1.15\n",
        "import tensorflow as tf\n",
        "#print(tf.__version__)\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# default seeding for reproducability\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(50)\n",
        "SEED_FINAL = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MXcpFQSOI50",
        "colab_type": "code",
        "outputId": "86998871-7b00-4cf1-dea6-1f310c98dd61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# Import torch Libraries\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, MSELoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "#from torch.autograd import Variable\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "#adam sgd combined optimizer\n",
        "!pip install adabound\n",
        "import adabound\n",
        "# for evaluating the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "\n",
        "#defaults\n",
        "PIXEL_LENGTH_MODIFIED = 128\n",
        "FEATURE_SIZE_MODIFIED = PIXEL_LENGTH_MODIFIED*PIXEL_LENGTH_MODIFIED\n",
        "PIXEL_LENGTH_MNIST = 28\n",
        "FEATURE_SIZE_MNIST = PIXEL_LENGTH_MNIST*PIXEL_LENGTH_MNIST"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/44/0c2c414effb3d9750d780b230dbb67ea48ddc5d9a6d7a9b7e6fcc6bdcff9/adabound-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.17.4)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Tb-VZDOU5Q",
        "colab_type": "text"
      },
      "source": [
        "## helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLScvSjuOWFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "# def normalize_max(images):\n",
        "#   images_normalize=[]\n",
        "#   for i in images:\n",
        "#     i -= i.min()\n",
        "#     denom = i.max()-i.min()\n",
        "#     images_normalize.append(np.divide(i, denom))\n",
        "#   return np.asarray(images_normalize) \n",
        "\n",
        "# def standardize_mean(images):\n",
        "#   images_standardize=[]\n",
        "#   for i in images:\n",
        "#     mean, std = i.mean(), i.std()\n",
        "#     i = (i - mean) / std\n",
        "#     images_standardize.append(i)\n",
        "#   return np.asarray(images_standardize) \n",
        "  \n",
        "# training/testing functions\n",
        "def train(epoch, train_loader, model, error, optimizer, batch_size):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    loss_total = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        #peak at tensor details\n",
        "        # if batch_idx==1:\n",
        "        #   # visualize one of the images in data set\n",
        "        #   plt.imshow(np.squeeze(images[0].numpy()), cmap='Greys')\n",
        "        #   plt.axis(\"off\")\n",
        "        #   plt.title(str(labels[0].numpy()))\n",
        "        #   #plt.savefig('graph.png')\n",
        "        #   plt.show()\n",
        "        #reshape for training\n",
        "        train = images.view(batch_size,3,32,32).to(device=device, dtype=torch.float)\n",
        "        labels = labels.to(device=device, dtype=torch.long)\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward propagation\n",
        "        outputs = model(train)\n",
        "        # Calculate softmax and cross entropy loss\n",
        "        loss = error(outputs, labels)\n",
        "        #print(loss)\n",
        "        loss_total += loss.item()\n",
        "\n",
        "        # Calculating gradients\n",
        "        loss.backward()\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        #print every 100 batches\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
        "            # plt.imshow(images[0].numpy().reshape(128,128))\n",
        "            # plt.axis('off')\n",
        "            # plt.show()\n",
        "            #Print Loss\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(images), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), running_loss/100))\n",
        "            running_loss = 0.0\n",
        "    loss_total /= float(len(train_loader))\n",
        "    print('Total train loss: {:.4f}'.format(loss_total))\n",
        "    #return last loss\n",
        "    return loss_total#loss.item()\n",
        "\n",
        "def test(test_loader, model, error, batch_size):\n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Calculate Accuracy         \n",
        "        correct = 0\n",
        "        loss_test = 0.0\n",
        "        avg_loss_test = 0.\n",
        "        # Iterate through test dataset\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            # if batch_idx==1:\n",
        "            #   # visualize one of the images in data set\n",
        "            #   plt.imshow(np.squeeze(images[0].numpy()), cmap='Greys')\n",
        "            #   plt.axis(\"off\")\n",
        "            #   plt.title(str(labels[0].numpy()))\n",
        "            #   #plt.savefig('graph.png')\n",
        "            #   plt.show()\n",
        "\n",
        "            test = images.view(batch_size,3,32,32).to(device=device, dtype=torch.float)\n",
        "            labels = labels.to(device=device, dtype=torch.long)\n",
        "            # Forward propagation\n",
        "            outputs = model(test)\n",
        "            \n",
        "\n",
        "            # sum up batch loss\n",
        "            loss = error(outputs, labels).item()\n",
        "            loss_test += loss\n",
        "            avg_loss_test += loss / len(test_loader)\n",
        "            # get the index of the max log-probability\n",
        "            predicted = outputs.max(1, keepdim=True)[1]\n",
        "            # if batch_idx==1:\n",
        "            #   print(predicted[0].item())\n",
        "            correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
        "        loss_test /= len(test_loader)\n",
        "        accuracy = 100. * correct / len(test_loader.dataset)\n",
        "        print('\\nTest set: Test loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "              .format(loss_test, correct, len(test_loader.dataset), accuracy))\n",
        "        #return loss and accuracy\n",
        "        return avg_loss_test, accuracy\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK6XR7HuS4xp",
        "colab_type": "code",
        "outputId": "8848ac9c-5bc3-44ff-8866-2ca7a801d3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# batch_size = 50\n",
        "# workers = os.cpu_count()\n",
        "\n",
        "# transform = transforms.Compose(\n",
        "#     [transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# lengths = [60000*0.8, 60000*0.1, 60000*0.1]\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "#                                         download=True, transform=transform)\n",
        "# train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "#                                           shuffle=True, num_workers=workers)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "#                                        download=True, transform=transform)\n",
        "# test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                          shuffle=False, num_workers=workers)\n",
        "\n",
        "# #check for gpu/cpu\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 50\n",
        "workers = os.cpu_count()\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "lengths = [5000, 5000]\n",
        "\n",
        "fullset_train = torchvision.datasets.CIFAR10(root='./data', download=True, train=True,\n",
        "                                       transform=transform)\n",
        "\n",
        "fullset_test = torchvision.datasets.CIFAR10(root='./data', download=True, train=False,\n",
        "                                       transform=transform)\n",
        "\n",
        "testset, finaltestset = torch.utils.data.random_split(fullset_test, lengths)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(fullset_train, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=workers)\n",
        "\n",
        "#test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "#                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(finaltestset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=workers)\n",
        "\n",
        "#check for gpu/cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:03, 44111134.95it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXhP2GvLy-pw",
        "colab_type": "code",
        "outputId": "38a88cfd-19ac-4539-efbb-1a02e61ec47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(final_test_loader.dataset.indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7NZIrYFL1GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# # load mnist dataset\n",
        "# (x_train_stack, y_train_stack), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# # split train into train-val set\n",
        "# #x_train_stack_normalized = standardize_mean(x_train_stack)\n",
        "# features_train, features_test, targets_train, targets_test = train_test_split(x_train_stack,\n",
        "#                                                       y_train_stack,\n",
        "#                                                       test_size = 5000,\n",
        "#                                                       random_state = 42) \n",
        "\n",
        "\n",
        "# print(features_train.shape)\n",
        "# print(features_test.shape)\n",
        "# print(targets_train.shape)\n",
        "# print(targets_test.shape)\n",
        "# # visualize one of the images in data set\n",
        "# plt.imshow(x_train_stack[10], cmap='Greys')\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(str(y_train_stack[10]))\n",
        "# #plt.savefig('graph.png')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_y6fbLsVBbd",
        "colab_type": "code",
        "outputId": "3c64c958-92dc-4313-d915-4a456c39b832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_loader.dataset.data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVJbpUaRk2fP",
        "colab_type": "code",
        "outputId": "45eb2a27-dc9f-449e-c42e-b259aa78f94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_loader.dataset.indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4FXgWmMgqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # set batch_size, epoch and iteration\n",
        "# batch_size = 50\n",
        "# workers = os.cpu_count()\n",
        "# # n_iters = 10000\n",
        "# # num_epochs = n_iters / (len(features_train) / batch_size)\n",
        "# # num_epochs = int(num_epochs)\n",
        "\n",
        "# # create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
        "# featuresTrain = torch.from_numpy(features_train).type(torch.LongTensor)\n",
        "# targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# # create feature and targets tensor for test set.\n",
        "# featuresTest = torch.from_numpy(features_test).type(torch.LongTensor)\n",
        "# targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
        "\n",
        "# # Pytorch train and test sets\n",
        "# train_data = torch.utils.data.TensorDataset(featuresTrain,targetsTrain, transform=transform)\n",
        "# test_data = torch.utils.data.TensorDataset(featuresTest,targetsTest, transform=transform)\n",
        "\n",
        "# # prepare data loaders (combine dataset and sampler)\n",
        "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=False, num_workers=workers)\n",
        "\n",
        "# #check for gpu/cpu\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uFPVldi_EUmD"
      },
      "source": [
        "## squeeze skip residuals (batch normed) (removed 2 fire layer) Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M6Tk35WgEUmI",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(384, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VfFIOrFqEUmQ",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lAVIfnZEUmV",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c03ea213-959e-4080-af3d-fe74afef66ab",
        "id": "V_Vxk3lWEUmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.156244\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.833881\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.741549\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.680840\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.618428\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.570918\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.524497\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.469279\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.470967\n",
            "Total train loss: 1.6474\n",
            "\n",
            "Test set: Test loss: 1.3677, Accuracy: 2669/5000 (53%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 53.38%\n",
            "Better loss at Epoch 0: loss = 1.367715888023376%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.402989\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.356739\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.350589\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.311794\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.300421\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.277710\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.266681\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.234895\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.217329\n",
            "Total train loss: 1.2916\n",
            "\n",
            "Test set: Test loss: 1.1591, Accuracy: 3001/5000 (60%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 60.02%\n",
            "Better loss at Epoch 1: loss = 1.1590824961662292%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.177124\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.164125\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.173951\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.129570\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.166227\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.135189\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.083317\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.083904\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.094696\n",
            "Total train loss: 1.1294\n",
            "\n",
            "Test set: Test loss: 1.0648, Accuracy: 3153/5000 (63%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 63.06%\n",
            "Better loss at Epoch 2: loss = 1.064776983857155%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.032221\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.022550\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.023620\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.033515\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.041606\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.026503\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.002593\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 0.967158\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.996138\n",
            "Total train loss: 1.0121\n",
            "\n",
            "Test set: Test loss: 1.0130, Accuracy: 3265/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 65.3%\n",
            "Better loss at Epoch 3: loss = 1.0129841655492782%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.924962\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.944645\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.914322\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.954731\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.926173\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.907974\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.911966\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.944930\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.943935\n",
            "Total train loss: 0.9275\n",
            "\n",
            "Test set: Test loss: 0.9207, Accuracy: 3409/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 68.18%\n",
            "Better loss at Epoch 4: loss = 0.9206824535131454%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.844901\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.858828\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.864414\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.858802\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.865217\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.864321\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.859999\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.857083\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.859528\n",
            "Total train loss: 0.8572\n",
            "\n",
            "Test set: Test loss: 0.8808, Accuracy: 3470/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 69.4%\n",
            "Better loss at Epoch 5: loss = 0.8807934176921842%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.787316\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.774353\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.807347\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.798702\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.805345\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.785514\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.790558\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.783392\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.814033\n",
            "Total train loss: 0.7942\n",
            "\n",
            "Test set: Test loss: 0.8359, Accuracy: 3569/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 71.38%\n",
            "Better loss at Epoch 6: loss = 0.8358556658029556%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.741099\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.729009\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.747563\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.745947\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.745434\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.759598\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.740866\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.746890\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.750533\n",
            "Total train loss: 0.7485\n",
            "\n",
            "Test set: Test loss: 0.8373, Accuracy: 3541/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.702056\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.681538\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.715436\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.697697\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.704908\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.704535\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.705967\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.701120\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.711520\n",
            "Total train loss: 0.7018\n",
            "\n",
            "Test set: Test loss: 0.8352, Accuracy: 3598/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 71.96%\n",
            "Better loss at Epoch 8: loss = 0.8351708376407626%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.643305\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.640157\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.652211\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.638429\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.662960\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.668061\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.681255\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.661100\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.669460\n",
            "Total train loss: 0.6593\n",
            "\n",
            "Test set: Test loss: 0.8109, Accuracy: 3611/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 72.22%\n",
            "Better loss at Epoch 9: loss = 0.810875016450882%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.600837\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.612214\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.606300\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.624281\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.627390\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.636949\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.620653\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.660078\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.639308\n",
            "Total train loss: 0.6284\n",
            "\n",
            "Test set: Test loss: 0.7934, Accuracy: 3646/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 72.92%\n",
            "Better loss at Epoch 10: loss = 0.7933797407150268%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.578054\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.561063\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.581870\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.578070\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.636228\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.583980\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.650007\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.606993\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.612871\n",
            "Total train loss: 0.5994\n",
            "\n",
            "Test set: Test loss: 0.8055, Accuracy: 3646/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.566493\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.534765\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.557311\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.558018\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.580416\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.549736\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.577170\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.590528\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.552369\n",
            "Total train loss: 0.5630\n",
            "\n",
            "Test set: Test loss: 0.8272, Accuracy: 3620/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.530829\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.515901\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.526044\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.529546\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.539091\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.543894\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.560170\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.542722\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.563983\n",
            "Total train loss: 0.5392\n",
            "\n",
            "Test set: Test loss: 0.7882, Accuracy: 3693/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 73.86%\n",
            "Better loss at Epoch 13: loss = 0.7881953290104867%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.463164\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.500834\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.486812\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.519215\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.504097\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.512146\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.529631\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.519280\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.529025\n",
            "Total train loss: 0.5074\n",
            "\n",
            "Test set: Test loss: 0.8471, Accuracy: 3620/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.450900\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.454968\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.462098\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.454719\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.486782\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.502143\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.513415\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.496929\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.501864\n",
            "Total train loss: 0.4864\n",
            "\n",
            "Test set: Test loss: 0.8339, Accuracy: 3657/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.453705\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.424454\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.448098\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.452889\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.486479\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.464202\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.453138\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.472992\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.451151\n",
            "Total train loss: 0.4587\n",
            "\n",
            "Test set: Test loss: 0.8388, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.429233\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.396836\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.438741\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.423119\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.461368\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.442333\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.447617\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.456389\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.451943\n",
            "Total train loss: 0.4403\n",
            "\n",
            "Test set: Test loss: 0.8450, Accuracy: 3672/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.389439\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.403609\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.419312\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.415043\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.413986\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.424365\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.417609\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.412389\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.445279\n",
            "Total train loss: 0.4200\n",
            "\n",
            "Test set: Test loss: 0.8176, Accuracy: 3719/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 74.38%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.347071\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.372700\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.400642\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.398791\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.395670\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.393589\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.413153\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.421796\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.433623\n",
            "Total train loss: 0.3990\n",
            "\n",
            "Test set: Test loss: 0.8822, Accuracy: 3642/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.355831\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.342494\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.354158\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.396751\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.367453\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.386404\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.393069\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.398723\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.402181\n",
            "Total train loss: 0.3807\n",
            "\n",
            "Test set: Test loss: 0.8617, Accuracy: 3651/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.345930\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.344553\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.336598\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.353377\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.365048\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.361568\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.374756\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.381090\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.364614\n",
            "Total train loss: 0.3610\n",
            "\n",
            "Test set: Test loss: 0.8635, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.307536\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.300054\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.329624\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.338625\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.342723\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.354218\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.366955\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.385931\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.364035\n",
            "Total train loss: 0.3456\n",
            "\n",
            "Test set: Test loss: 0.8767, Accuracy: 3667/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.285292\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.301263\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.323403\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.310760\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.320238\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.328909\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.328131\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.343955\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.338717\n",
            "Total train loss: 0.3248\n",
            "\n",
            "Test set: Test loss: 0.9123, Accuracy: 3660/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.270507\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.287311\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.284806\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.318912\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.309090\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.307716\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.322250\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.316903\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.337001\n",
            "Total train loss: 0.3113\n",
            "\n",
            "Test set: Test loss: 0.9196, Accuracy: 3637/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.279008\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.278784\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.307613\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.272209\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.307045\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.347353\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.314927\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.299592\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.334239\n",
            "Total train loss: 0.3052\n",
            "\n",
            "Test set: Test loss: 0.9635, Accuracy: 3623/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.261161\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.271596\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.277648\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.268854\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.295000\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.285891\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.298935\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.314169\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.313065\n",
            "Total train loss: 0.2872\n",
            "\n",
            "Test set: Test loss: 0.9411, Accuracy: 3673/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.231108\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.227045\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.230733\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.265748\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.251706\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.278048\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.265923\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.286387\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.308011\n",
            "Total train loss: 0.2650\n",
            "\n",
            "Test set: Test loss: 0.9506, Accuracy: 3652/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.237484\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.237964\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.263686\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.262098\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.280053\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.274856\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.265947\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.269982\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.279305\n",
            "Total train loss: 0.2650\n",
            "\n",
            "Test set: Test loss: 0.8976, Accuracy: 3706/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.214019\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.220135\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.235079\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.243087\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.257296\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.264822\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.245475\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.253580\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.264936\n",
            "Total train loss: 0.2464\n",
            "\n",
            "Test set: Test loss: 0.9722, Accuracy: 3688/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.202293\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.218829\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.227675\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.222303\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.214529\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.253264\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.241687\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.255756\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.271833\n",
            "Total train loss: 0.2362\n",
            "\n",
            "Test set: Test loss: 0.9897, Accuracy: 3622/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.198313\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.215353\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.198936\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.232639\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.249092\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.247595\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.230232\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.233287\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.228352\n",
            "Total train loss: 0.2288\n",
            "\n",
            "Test set: Test loss: 1.0065, Accuracy: 3646/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.183806\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.186752\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.182531\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.203005\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.221020\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.234569\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.220036\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.225787\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.239149\n",
            "Total train loss: 0.2132\n",
            "\n",
            "Test set: Test loss: 0.9974, Accuracy: 3669/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.188166\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.180344\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.198795\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.193824\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.222945\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.213884\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.222431\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.223709\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.229536\n",
            "Total train loss: 0.2096\n",
            "\n",
            "Test set: Test loss: 0.9639, Accuracy: 3735/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 33: accuracy = 74.7%\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.168086\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.159110\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.197389\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.208899\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.214041\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.205634\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.212234\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.231137\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.213580\n",
            "Total train loss: 0.2014\n",
            "\n",
            "Test set: Test loss: 1.0641, Accuracy: 3644/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.166505\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.186516\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.190601\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.189618\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.176723\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.176449\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.207226\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.207538\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.225139\n",
            "Total train loss: 0.1945\n",
            "\n",
            "Test set: Test loss: 0.9997, Accuracy: 3713/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.167115\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.170871\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.173070\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.195487\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.173366\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.179901\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.195760\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.193095\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.182490\n",
            "Total train loss: 0.1830\n",
            "\n",
            "Test set: Test loss: 1.0491, Accuracy: 3668/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.163696\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.161092\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.176817\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.184345\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.191217\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.188569\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.182338\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.176898\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.194072\n",
            "Total train loss: 0.1817\n",
            "\n",
            "Test set: Test loss: 1.0591, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.131175\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.147041\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.163115\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.162790\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.176341\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.170851\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.174164\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.175374\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.183949\n",
            "Total train loss: 0.1661\n",
            "\n",
            "Test set: Test loss: 1.0337, Accuracy: 3705/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.148308\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.147032\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.163651\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.168192\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.142024\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.163984\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.152793\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.194288\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.182289\n",
            "Total train loss: 0.1657\n",
            "\n",
            "Test set: Test loss: 1.0787, Accuracy: 3652/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.138153\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.139077\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.144010\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.147226\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.171993\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.154399\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.211319\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.167752\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.151756\n",
            "Total train loss: 0.1610\n",
            "\n",
            "Test set: Test loss: 1.0629, Accuracy: 3676/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.127110\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.127884\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.140298\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.162673\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.142861\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.149937\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.165729\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.180319\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.162677\n",
            "Total train loss: 0.1518\n",
            "\n",
            "Test set: Test loss: 1.0761, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.121706\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.120868\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.131789\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.130064\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.141685\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.145365\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.168725\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.170607\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.143782\n",
            "Total train loss: 0.1424\n",
            "\n",
            "Test set: Test loss: 1.0728, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.125992\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.120357\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.109289\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.116796\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.138102\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.141983\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.141120\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.146862\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.188808\n",
            "Total train loss: 0.1398\n",
            "\n",
            "Test set: Test loss: 1.1173, Accuracy: 3635/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.130490\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.113534\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.140709\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.146826\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.136449\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.143452\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.136157\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.163526\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.144345\n",
            "Total train loss: 0.1419\n",
            "\n",
            "Test set: Test loss: 1.1381, Accuracy: 3671/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.121760\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.104880\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.109311\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.123211\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.142327\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.137240\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.133081\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.120106\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.138907\n",
            "Total train loss: 0.1272\n",
            "\n",
            "Test set: Test loss: 1.1046, Accuracy: 3698/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.131466\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.120367\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.114206\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.122709\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.121269\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.138698\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.123739\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.127994\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.134002\n",
            "Total train loss: 0.1298\n",
            "\n",
            "Test set: Test loss: 1.1351, Accuracy: 3673/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.111397\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.109677\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.113675\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.111278\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.130856\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.115401\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.119837\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.129710\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.125958\n",
            "Total train loss: 0.1198\n",
            "\n",
            "Test set: Test loss: 1.0781, Accuracy: 3723/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.102833\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.098892\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.094753\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.121795\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.115444\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.142579\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.133269\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.122707\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.121679\n",
            "Total train loss: 0.1178\n",
            "\n",
            "Test set: Test loss: 1.1351, Accuracy: 3714/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.108655\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.093757\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.132280\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.119387\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.105894\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.126450\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.120155\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.128055\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.128959\n",
            "Total train loss: 0.1187\n",
            "\n",
            "Test set: Test loss: 1.0787, Accuracy: 3728/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.096357\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.091195\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.106577\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.098319\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.108089\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.104630\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.098612\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.122142\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.125482\n",
            "Total train loss: 0.1072\n",
            "\n",
            "Test set: Test loss: 1.1581, Accuracy: 3657/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.102846\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.077352\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.088303\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.094364\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.113151\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.102376\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.097476\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.125803\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.130940\n",
            "Total train loss: 0.1037\n",
            "\n",
            "Test set: Test loss: 1.1669, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.095065\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.089015\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.108335\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.096784\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.106893\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.112165\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.101241\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.122058\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.129421\n",
            "Total train loss: 0.1081\n",
            "\n",
            "Test set: Test loss: 1.1330, Accuracy: 3706/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.097821\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.091537\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.086384\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.089671\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.114205\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.115749\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.104193\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.104725\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.124284\n",
            "Total train loss: 0.1035\n",
            "\n",
            "Test set: Test loss: 1.1687, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.084184\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.078436\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.106169\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.105122\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.100728\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.097163\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.097542\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.110625\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.124772\n",
            "Total train loss: 0.1022\n",
            "\n",
            "Test set: Test loss: 1.1380, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.085322\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.084005\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.086016\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.090071\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.092788\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.105575\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.102273\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.122423\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.113736\n",
            "Total train loss: 0.1001\n",
            "\n",
            "Test set: Test loss: 1.1657, Accuracy: 3694/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.082061\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.083740\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.079884\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.087157\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.079959\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.094686\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.095618\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.097859\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.105497\n",
            "Total train loss: 0.0917\n",
            "\n",
            "Test set: Test loss: 1.1601, Accuracy: 3673/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.082946\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.072045\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.070931\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.083143\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.100833\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.100909\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.110557\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.094135\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.098862\n",
            "Total train loss: 0.0900\n",
            "\n",
            "Test set: Test loss: 1.2231, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.097843\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.084953\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.084798\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.078396\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.090671\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.082060\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.098747\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.070448\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.087460\n",
            "Total train loss: 0.0884\n",
            "\n",
            "Test set: Test loss: 1.1902, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.089803\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.084191\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.069623\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.066436\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.072619\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.072204\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.089166\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.079106\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.094642\n",
            "Total train loss: 0.0807\n",
            "\n",
            "Test set: Test loss: 1.1818, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.078690\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.084063\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.079157\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.083725\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.085455\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.074088\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.073697\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.088089\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.098207\n",
            "Total train loss: 0.0846\n",
            "\n",
            "Test set: Test loss: 1.1701, Accuracy: 3715/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.074896\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.065936\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.078237\n",
            "Train Epoch: 61 [20000/50000 (40%)]\tTrain Loss: 0.080874\n",
            "Train Epoch: 61 [25000/50000 (50%)]\tTrain Loss: 0.084430\n",
            "Train Epoch: 61 [30000/50000 (60%)]\tTrain Loss: 0.072154\n",
            "Train Epoch: 61 [35000/50000 (70%)]\tTrain Loss: 0.091287\n",
            "Train Epoch: 61 [40000/50000 (80%)]\tTrain Loss: 0.084471\n",
            "Train Epoch: 61 [45000/50000 (90%)]\tTrain Loss: 0.102489\n",
            "Total train loss: 0.0817\n",
            "\n",
            "Test set: Test loss: 1.2347, Accuracy: 3657/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 62: lr = 0.1\n",
            "Train Epoch: 62 [5000/50000 (10%)]\tTrain Loss: 0.084359\n",
            "Train Epoch: 62 [10000/50000 (20%)]\tTrain Loss: 0.061953\n",
            "Train Epoch: 62 [15000/50000 (30%)]\tTrain Loss: 0.057361\n",
            "Train Epoch: 62 [20000/50000 (40%)]\tTrain Loss: 0.069970\n",
            "Train Epoch: 62 [25000/50000 (50%)]\tTrain Loss: 0.078652\n",
            "Train Epoch: 62 [30000/50000 (60%)]\tTrain Loss: 0.077608\n",
            "Train Epoch: 62 [35000/50000 (70%)]\tTrain Loss: 0.080656\n",
            "Train Epoch: 62 [40000/50000 (80%)]\tTrain Loss: 0.094577\n",
            "Train Epoch: 62 [45000/50000 (90%)]\tTrain Loss: 0.083177\n",
            "Total train loss: 0.0762\n",
            "\n",
            "Test set: Test loss: 1.2013, Accuracy: 3707/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 63: lr = 0.1\n",
            "Train Epoch: 63 [5000/50000 (10%)]\tTrain Loss: 0.084503\n",
            "Train Epoch: 63 [10000/50000 (20%)]\tTrain Loss: 0.072114\n",
            "Train Epoch: 63 [15000/50000 (30%)]\tTrain Loss: 0.067427\n",
            "Train Epoch: 63 [20000/50000 (40%)]\tTrain Loss: 0.065144\n",
            "Train Epoch: 63 [25000/50000 (50%)]\tTrain Loss: 0.069447\n",
            "Train Epoch: 63 [30000/50000 (60%)]\tTrain Loss: 0.078369\n",
            "Train Epoch: 63 [35000/50000 (70%)]\tTrain Loss: 0.078444\n",
            "Train Epoch: 63 [40000/50000 (80%)]\tTrain Loss: 0.075876\n",
            "Train Epoch: 63 [45000/50000 (90%)]\tTrain Loss: 0.081692\n",
            "Total train loss: 0.0758\n",
            "\n",
            "Test set: Test loss: 1.1624, Accuracy: 3756/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 63: accuracy = 75.12%\n",
            "\n",
            "Train Epoch 64: lr = 0.1\n",
            "Train Epoch: 64 [5000/50000 (10%)]\tTrain Loss: 0.060505\n",
            "Train Epoch: 64 [10000/50000 (20%)]\tTrain Loss: 0.069980\n",
            "Train Epoch: 64 [15000/50000 (30%)]\tTrain Loss: 0.068138\n",
            "Train Epoch: 64 [20000/50000 (40%)]\tTrain Loss: 0.067669\n",
            "Train Epoch: 64 [25000/50000 (50%)]\tTrain Loss: 0.075797\n",
            "Train Epoch: 64 [30000/50000 (60%)]\tTrain Loss: 0.079802\n",
            "Train Epoch: 64 [35000/50000 (70%)]\tTrain Loss: 0.074926\n",
            "Train Epoch: 64 [40000/50000 (80%)]\tTrain Loss: 0.079366\n",
            "Train Epoch: 64 [45000/50000 (90%)]\tTrain Loss: 0.080848\n",
            "Total train loss: 0.0742\n",
            "\n",
            "Test set: Test loss: 1.2277, Accuracy: 3707/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 65: lr = 0.1\n",
            "Train Epoch: 65 [5000/50000 (10%)]\tTrain Loss: 0.059594\n",
            "Train Epoch: 65 [10000/50000 (20%)]\tTrain Loss: 0.060350\n",
            "Train Epoch: 65 [15000/50000 (30%)]\tTrain Loss: 0.078311\n",
            "Train Epoch: 65 [20000/50000 (40%)]\tTrain Loss: 0.065230\n",
            "Train Epoch: 65 [25000/50000 (50%)]\tTrain Loss: 0.075425\n",
            "Train Epoch: 65 [30000/50000 (60%)]\tTrain Loss: 0.076889\n",
            "Train Epoch: 65 [35000/50000 (70%)]\tTrain Loss: 0.087989\n",
            "Train Epoch: 65 [40000/50000 (80%)]\tTrain Loss: 0.078139\n",
            "Train Epoch: 65 [45000/50000 (90%)]\tTrain Loss: 0.077348\n",
            "Total train loss: 0.0729\n",
            "\n",
            "Test set: Test loss: 1.2336, Accuracy: 3694/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 66: lr = 0.1\n",
            "Train Epoch: 66 [5000/50000 (10%)]\tTrain Loss: 0.072029\n",
            "Train Epoch: 66 [10000/50000 (20%)]\tTrain Loss: 0.080826\n",
            "Train Epoch: 66 [15000/50000 (30%)]\tTrain Loss: 0.070839\n",
            "Train Epoch: 66 [20000/50000 (40%)]\tTrain Loss: 0.059526\n",
            "Train Epoch: 66 [25000/50000 (50%)]\tTrain Loss: 0.065439\n",
            "Train Epoch: 66 [30000/50000 (60%)]\tTrain Loss: 0.066558\n",
            "Train Epoch: 66 [35000/50000 (70%)]\tTrain Loss: 0.076079\n",
            "Train Epoch: 66 [40000/50000 (80%)]\tTrain Loss: 0.070961\n",
            "Train Epoch: 66 [45000/50000 (90%)]\tTrain Loss: 0.077813\n",
            "Total train loss: 0.0710\n",
            "\n",
            "Test set: Test loss: 1.1865, Accuracy: 3708/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 67: lr = 0.1\n",
            "Train Epoch: 67 [5000/50000 (10%)]\tTrain Loss: 0.068466\n",
            "Train Epoch: 67 [10000/50000 (20%)]\tTrain Loss: 0.068513\n",
            "Train Epoch: 67 [15000/50000 (30%)]\tTrain Loss: 0.059309\n",
            "Train Epoch: 67 [20000/50000 (40%)]\tTrain Loss: 0.075006\n",
            "Train Epoch: 67 [25000/50000 (50%)]\tTrain Loss: 0.071302\n",
            "Train Epoch: 67 [30000/50000 (60%)]\tTrain Loss: 0.069465\n",
            "Train Epoch: 67 [35000/50000 (70%)]\tTrain Loss: 0.058585\n",
            "Train Epoch: 67 [40000/50000 (80%)]\tTrain Loss: 0.070114\n",
            "Train Epoch: 67 [45000/50000 (90%)]\tTrain Loss: 0.070048\n",
            "Total train loss: 0.0681\n",
            "\n",
            "Test set: Test loss: 1.2116, Accuracy: 3715/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 68: lr = 0.1\n",
            "Train Epoch: 68 [5000/50000 (10%)]\tTrain Loss: 0.063182\n",
            "Train Epoch: 68 [10000/50000 (20%)]\tTrain Loss: 0.049264\n",
            "Train Epoch: 68 [15000/50000 (30%)]\tTrain Loss: 0.063330\n",
            "Train Epoch: 68 [20000/50000 (40%)]\tTrain Loss: 0.068995\n",
            "Train Epoch: 68 [25000/50000 (50%)]\tTrain Loss: 0.060097\n",
            "Train Epoch: 68 [30000/50000 (60%)]\tTrain Loss: 0.063019\n",
            "Train Epoch: 68 [35000/50000 (70%)]\tTrain Loss: 0.064805\n",
            "Train Epoch: 68 [40000/50000 (80%)]\tTrain Loss: 0.070992\n",
            "Train Epoch: 68 [45000/50000 (90%)]\tTrain Loss: 0.079626\n",
            "Total train loss: 0.0661\n",
            "\n",
            "Test set: Test loss: 1.2072, Accuracy: 3726/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 69: lr = 0.1\n",
            "Train Epoch: 69 [5000/50000 (10%)]\tTrain Loss: 0.072421\n",
            "Train Epoch: 69 [10000/50000 (20%)]\tTrain Loss: 0.063185\n",
            "Train Epoch: 69 [15000/50000 (30%)]\tTrain Loss: 0.066318\n",
            "Train Epoch: 69 [20000/50000 (40%)]\tTrain Loss: 0.060675\n",
            "Train Epoch: 69 [25000/50000 (50%)]\tTrain Loss: 0.077945\n",
            "Train Epoch: 69 [30000/50000 (60%)]\tTrain Loss: 0.058566\n",
            "Train Epoch: 69 [35000/50000 (70%)]\tTrain Loss: 0.073001\n",
            "Train Epoch: 69 [40000/50000 (80%)]\tTrain Loss: 0.064477\n",
            "Train Epoch: 69 [45000/50000 (90%)]\tTrain Loss: 0.070832\n",
            "Total train loss: 0.0681\n",
            "\n",
            "Test set: Test loss: 1.1847, Accuracy: 3706/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 70: lr = 0.1\n",
            "Train Epoch: 70 [5000/50000 (10%)]\tTrain Loss: 0.064010\n",
            "Train Epoch: 70 [10000/50000 (20%)]\tTrain Loss: 0.066365\n",
            "Train Epoch: 70 [15000/50000 (30%)]\tTrain Loss: 0.064706\n",
            "Train Epoch: 70 [20000/50000 (40%)]\tTrain Loss: 0.070413\n",
            "Train Epoch: 70 [25000/50000 (50%)]\tTrain Loss: 0.054531\n",
            "Train Epoch: 70 [30000/50000 (60%)]\tTrain Loss: 0.058189\n",
            "Train Epoch: 70 [35000/50000 (70%)]\tTrain Loss: 0.072157\n",
            "Train Epoch: 70 [40000/50000 (80%)]\tTrain Loss: 0.079685\n",
            "Train Epoch: 70 [45000/50000 (90%)]\tTrain Loss: 0.079043\n",
            "Total train loss: 0.0678\n",
            "\n",
            "Test set: Test loss: 1.2256, Accuracy: 3713/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 71: lr = 0.1\n",
            "Train Epoch: 71 [5000/50000 (10%)]\tTrain Loss: 0.069014\n",
            "Train Epoch: 71 [10000/50000 (20%)]\tTrain Loss: 0.051163\n",
            "Train Epoch: 71 [15000/50000 (30%)]\tTrain Loss: 0.059528\n",
            "Train Epoch: 71 [20000/50000 (40%)]\tTrain Loss: 0.055221\n",
            "Train Epoch: 71 [25000/50000 (50%)]\tTrain Loss: 0.044117\n",
            "Train Epoch: 71 [30000/50000 (60%)]\tTrain Loss: 0.058287\n",
            "Train Epoch: 71 [35000/50000 (70%)]\tTrain Loss: 0.055450\n",
            "Train Epoch: 71 [40000/50000 (80%)]\tTrain Loss: 0.058463\n",
            "Train Epoch: 71 [45000/50000 (90%)]\tTrain Loss: 0.081232\n",
            "Total train loss: 0.0594\n",
            "\n",
            "Test set: Test loss: 1.2645, Accuracy: 3716/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 72: lr = 0.1\n",
            "Train Epoch: 72 [5000/50000 (10%)]\tTrain Loss: 0.066681\n",
            "Train Epoch: 72 [10000/50000 (20%)]\tTrain Loss: 0.067951\n",
            "Train Epoch: 72 [15000/50000 (30%)]\tTrain Loss: 0.061057\n",
            "Train Epoch: 72 [20000/50000 (40%)]\tTrain Loss: 0.070902\n",
            "Train Epoch: 72 [25000/50000 (50%)]\tTrain Loss: 0.067592\n",
            "Train Epoch: 72 [30000/50000 (60%)]\tTrain Loss: 0.056854\n",
            "Train Epoch: 72 [35000/50000 (70%)]\tTrain Loss: 0.059931\n",
            "Train Epoch: 72 [40000/50000 (80%)]\tTrain Loss: 0.063089\n",
            "Train Epoch: 72 [45000/50000 (90%)]\tTrain Loss: 0.057749\n",
            "Total train loss: 0.0635\n",
            "\n",
            "Test set: Test loss: 1.2261, Accuracy: 3710/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 73: lr = 0.1\n",
            "Train Epoch: 73 [5000/50000 (10%)]\tTrain Loss: 0.048132\n",
            "Train Epoch: 73 [10000/50000 (20%)]\tTrain Loss: 0.047423\n",
            "Train Epoch: 73 [15000/50000 (30%)]\tTrain Loss: 0.056419\n",
            "Train Epoch: 73 [20000/50000 (40%)]\tTrain Loss: 0.060326\n",
            "Train Epoch: 73 [25000/50000 (50%)]\tTrain Loss: 0.061817\n",
            "Train Epoch: 73 [30000/50000 (60%)]\tTrain Loss: 0.067070\n",
            "Train Epoch: 73 [35000/50000 (70%)]\tTrain Loss: 0.054009\n",
            "Train Epoch: 73 [40000/50000 (80%)]\tTrain Loss: 0.061798\n",
            "Train Epoch: 73 [45000/50000 (90%)]\tTrain Loss: 0.059979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9001a69423c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs\\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accura...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e8e76cf8595a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "51a675b4-72f4-4867-cc9c-cf95c5204683",
        "id": "5f85Xwm8EUmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKk5WQgSJIkaTl/AL/N2CrrzOHXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmJrp3dgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 28.08398314573972 seconds\n",
            "Best accuracy: 75.12  Best training loss: 0.05943454007990658  Best validation loss: 0.7881953290104867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1b690fea-eada-4d97-86de-292e79d99d6f",
        "id": "RjORD2CSEUmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]\n",
            "[1.6473578805923461, 1.2915961208343505, 1.1294073311686517, 1.012139591217041, 0.9274672510027886, 0.8571678162813187, 0.7942030563950538, 0.7484979167580604, 0.7018296536207199, 0.6593402114212513, 0.6284279014766216, 0.5993730073273182, 0.5629559449702501, 0.5391781126409769, 0.5073888074159623, 0.486400278031826, 0.4587193107157946, 0.4403483587652445, 0.42000688348710535, 0.3989686287492514, 0.38070115342736244, 0.3609559191688895, 0.3455553373545408, 0.3248455015718937, 0.3113302046880126, 0.30523529996350407, 0.2871591752246022, 0.2649945173449814, 0.26498789989203214, 0.24644527194648982, 0.23623166743293406, 0.22884558813646436, 0.21318567657098175, 0.20962583773210644, 0.2013636082932353, 0.19445719679072498, 0.1829744456168264, 0.18172311689332127, 0.16611289869062604, 0.16565497217327357, 0.16095331154298037, 0.15176874992623926, 0.14239234097674489, 0.139847850269638, 0.14193976203072817, 0.12715992797911166, 0.12984711675997823, 0.11980781042110175, 0.11784731177426874, 0.11870961719285697, 0.10715352241788059, 0.10372175688203424, 0.10805400455277413, 0.10354958534985781, 0.10218180703325197, 0.10014095070911572, 0.09172777166124434, 0.08995721433497965, 0.08843533014925196, 0.08065928249736316, 0.0846298597524874, 0.08165925228595734, 0.07615478796605021, 0.07582092796824873, 0.07420715041365475, 0.07294338909350336, 0.07100813581515103, 0.06812060994957574, 0.06612067942996509, 0.06813935192907229, 0.06778025785926729, 0.05943454007990658, 0.06350453926506452]\n",
            "[1.367715888023376, 1.1590824961662292, 1.064776983857155, 1.0129841655492782, 0.9206824535131454, 0.8807934176921842, 0.8358556658029556, 0.8372503840923313, 0.8351708376407626, 0.810875016450882, 0.7933797407150268, 0.8055043086409569, 0.8271897265315055, 0.7881953290104867, 0.8471262645721435, 0.8339160832762718, 0.8387645223736763, 0.8450250315666197, 0.8175677049160004, 0.8822240307927138, 0.8616535264253616, 0.8634666025638583, 0.8767169120907786, 0.9122688254714013, 0.9196407425403592, 0.9634624356031416, 0.9410889321565626, 0.9506140634417534, 0.8975824320316312, 0.9721511614322663, 0.9897409397363659, 1.0064793926477436, 0.9974004966020579, 0.96385066896677, 1.0640555799007414, 0.9997187036275865, 1.0490957286953926, 1.0590652412176131, 1.0336969232559206, 1.0787018489837648, 1.0629032784700394, 1.0761179855465892, 1.0728015729784965, 1.1173107707500463, 1.1380558267235763, 1.1045734152197837, 1.1350947713851924, 1.0780703413486479, 1.1350961396098131, 1.078662904202938, 1.1581392395496368, 1.1668535518646244, 1.1329990044236184, 1.1686869919300078, 1.1379924637079246, 1.1657042068243026, 1.1601374155282973, 1.2231005194783209, 1.190235919356346, 1.1818336018919948, 1.1700850290060045, 1.2347376391291618, 1.2012782686948773, 1.1623816537857057, 1.2277007055282596, 1.2335785636305814, 1.1864973512291908, 1.2115565192699436, 1.2071927291154858, 1.1847258692979816, 1.225605225563049, 1.2645153516530991, 1.2260541456937788]\n",
            "[53.38, 60.02, 63.06, 65.3, 68.18, 69.4, 71.38, 70.82, 71.96, 72.22, 72.92, 72.92, 72.4, 73.86, 72.4, 73.14, 73.28, 73.44, 74.38, 72.84, 73.02, 73.8, 73.34, 73.2, 72.74, 72.46, 73.46, 73.04, 74.12, 73.76, 72.44, 72.92, 73.38, 74.7, 72.88, 74.26, 73.36, 73.28, 74.1, 73.04, 73.52, 74.02, 73.7, 72.7, 73.42, 73.96, 73.46, 74.46, 74.28, 74.56, 73.14, 73.7, 74.12, 73.74, 74.06, 73.88, 73.46, 73.92, 73.8, 73.8, 74.3, 73.14, 74.14, 75.12, 74.14, 73.88, 74.16, 74.3, 74.52, 74.12, 74.26, 74.32, 74.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I38j-LzMN8_g",
        "colab_type": "text"
      },
      "source": [
        "## alexnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoGYjfQ6N90P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['AlexNet', 'alexnet']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(pretrained=False, **kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hhLKkJrN-e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "model = alexnet(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.1\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_KS7Rk0OAJP",
        "colab_type": "code",
        "outputId": "32c555c7-e12c-4c43-8d14-72c55c6418ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/alexnet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/alexnet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.210915\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.962640\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.836767\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.748397\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.695877\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.640653\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.580237\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.542793\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.502671\n",
            "\n",
            "Test set: Test loss: 1.4176, Accuracy: 2343/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 46.86%\n",
            "Better loss at Epoch 0: loss = 1.4175948703289025%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.440749\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.374844\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.375904\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.352652\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.330812\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.299804\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.260781\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.263461\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.225642\n",
            "\n",
            "Test set: Test loss: 1.1766, Accuracy: 2926/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 58.52%\n",
            "Better loss at Epoch 1: loss = 1.1765833520889284%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.155375\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.124767\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.105064\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.095071\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.086886\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.091599\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.094332\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.046946\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.066674\n",
            "\n",
            "Test set: Test loss: 1.0330, Accuracy: 3168/5000 (63%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 63.36%\n",
            "Better loss at Epoch 2: loss = 1.0330202198028562%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 0.951708\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 0.966097\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 0.938865\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 0.957312\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.943493\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 0.958508\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 0.916614\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 0.956217\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.920733\n",
            "\n",
            "Test set: Test loss: 0.9073, Accuracy: 3484/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 69.68%\n",
            "Better loss at Epoch 3: loss = 0.9073415133357045%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.802404\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.820668\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.824508\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.839357\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.823451\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.784601\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.842694\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.823105\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.840295\n",
            "\n",
            "Test set: Test loss: 0.9377, Accuracy: 3363/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.730840\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.702070\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.700375\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.716316\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.737186\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.738939\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.708982\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.734384\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.746487\n",
            "\n",
            "Test set: Test loss: 0.8604, Accuracy: 3543/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 70.86%\n",
            "Better loss at Epoch 5: loss = 0.8603745141625405%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.585948\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.619200\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.602336\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.607238\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.681712\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.645993\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.646251\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.627034\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.629426\n",
            "\n",
            "Test set: Test loss: 0.8860, Accuracy: 3524/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.492542\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.550158\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.535312\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.527526\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.561797\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.532362\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.591067\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.556343\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.543288\n",
            "\n",
            "Test set: Test loss: 0.8563, Accuracy: 3662/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 73.24%\n",
            "Better loss at Epoch 7: loss = 0.8562646558880807%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.397489\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.430388\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.457244\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.481877\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.467004\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.465162\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.485688\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.496689\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.498582\n",
            "\n",
            "Test set: Test loss: 0.8444, Accuracy: 3715/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 74.3%\n",
            "Better loss at Epoch 8: loss = 0.8444360390305519%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.360742\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.355827\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.397372\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.397898\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.391695\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.406024\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.385903\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.412762\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.419470\n",
            "\n",
            "Test set: Test loss: 0.8595, Accuracy: 3718/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 74.36%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.270591\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.300600\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.313842\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.316074\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.325910\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.342562\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.352368\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.360542\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.384011\n",
            "\n",
            "Test set: Test loss: 0.8964, Accuracy: 3714/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.239708\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.277116\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.280243\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.285335\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.281736\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.303712\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.298227\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.305059\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.299551\n",
            "\n",
            "Test set: Test loss: 0.9654, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.217392\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.189467\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.241114\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.250376\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.241743\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.220276\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.243584\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.271535\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.245239\n",
            "\n",
            "Test set: Test loss: 0.9391, Accuracy: 3734/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 74.68%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.145912\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.203354\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.198021\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.190984\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.199279\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.196357\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.212403\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.232999\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.201447\n",
            "\n",
            "Test set: Test loss: 1.0018, Accuracy: 3771/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 75.42%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.132469\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.128570\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.174954\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.155194\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.173390\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.191417\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.175586\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.183135\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.171703\n",
            "\n",
            "Test set: Test loss: 1.0259, Accuracy: 3726/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.096655\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.114364\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.165511\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.129706\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.135906\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.182332\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.155954\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.150407\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.169430\n",
            "\n",
            "Test set: Test loss: 1.1667, Accuracy: 3665/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.103557\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.102468\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.112604\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.135047\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.132298\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.129492\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.120139\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.142956\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.161059\n",
            "\n",
            "Test set: Test loss: 1.1738, Accuracy: 3735/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.101152\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.094311\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.087977\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.116773\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.121719\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.125761\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.116891\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.104845\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.131137\n",
            "\n",
            "Test set: Test loss: 1.2542, Accuracy: 3719/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.075528\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.087486\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.075530\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.112356\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.096826\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.111223\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.111051\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.114943\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.102692\n",
            "\n",
            "Test set: Test loss: 1.3065, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.068948\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.077035\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.072538\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.085741\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.085171\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.088224\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.073084\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.094941\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.097859\n",
            "\n",
            "Test set: Test loss: 1.2576, Accuracy: 3764/5000 (75%)\n",
            "\n",
            "CPU times: user 4min 24s, sys: 50.9 s, total: 5min 15s\n",
            "Wall time: 5min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp5SzXncOO39",
        "colab_type": "code",
        "outputId": "fbbca3e6-59da-4589-d048-2027d21d6e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDwlhRn8Ms0uQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf2wL2ngW9DPmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 17.45083315845013 seconds\n",
            "Best accuracy: 75.42  Best training loss: 0.02247416414320469  Best validation loss: 0.8444360390305519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EApzFYGfLjw",
        "colab_type": "code",
        "outputId": "7a0ac75c-2599-427e-fcf8-1f19cd0958b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "[1.6308766603469849, 1.3429898023605347, 0.9622616767883301, 1.1100956201553345, 0.655852735042572, 0.524598240852356, 0.7864339351654053, 0.621737003326416, 0.502821683883667, 0.7517127990722656, 0.18797433376312256, 0.19942814111709595, 0.21666258573532104, 0.15186657011508942, 0.31530871987342834, 0.07428207993507385, 0.37790507078170776, 0.06306121498346329, 0.0740111768245697, 0.14583754539489746]\n",
            "[1.409133423566818, 1.2283282226324086, 1.0814817500114442, 0.8936923462152482, 0.8564886212348937, 0.8699691197276113, 0.8164491868019103, 0.7932574903964993, 0.752289607822895, 0.8448375365138052, 0.8733229354023934, 0.8786852282285693, 0.9630569016933443, 1.024801244735718, 0.993808183670044, 1.0776556742191314, 1.18092718243599, 1.131519162505865, 1.1523174196481705, 1.2154082900285719]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FbRgLJKN8Mit"
      },
      "source": [
        "## alexnet (batch normed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DzO9OYGO8Miw",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['AlexNet', 'alexnet']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "}\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 2 * 2, 4096),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(pretrained=False, **kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IjXsjwyI8Miz",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "model = alexnet(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.1\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e1167f9f-342a-42c3-c917-a33e32858a98",
        "id": "KJB2m5DJ8Mi1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/alexnet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/alexnet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.230557\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.835540\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.776237\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.689643\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.510512\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.647828\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.522134\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.442038\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.510945\n",
            "\n",
            "Test set: Test loss: 1.3165, Accuracy: 2795/5000 (56%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 55.9%\n",
            "Better loss at Epoch 0: loss = 1.316491122245789%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.391351\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.408898\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.286650\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.319900\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.334819\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.176328\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.223126\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.224275\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.247388\n",
            "\n",
            "Test set: Test loss: 1.0734, Accuracy: 3083/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 61.66%\n",
            "Better loss at Epoch 1: loss = 1.073440764546394%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.139037\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.190773\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.119538\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.032878\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 0.994619\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.096712\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.033208\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.015557\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.046297\n",
            "\n",
            "Test set: Test loss: 1.1977, Accuracy: 3111/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 62.22%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 0.960660\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 0.893087\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 0.904110\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 0.984784\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 0.963961\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 0.865989\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.084301\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.015813\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.921613\n",
            "\n",
            "Test set: Test loss: 0.8420, Accuracy: 3520/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 70.4%\n",
            "Better loss at Epoch 3: loss = 0.8420078158378599%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.824295\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.864188\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.783313\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.777640\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.848587\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.889321\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.804943\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.880764\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.837907\n",
            "\n",
            "Test set: Test loss: 0.8466, Accuracy: 3609/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 72.18%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.713579\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.770345\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.736577\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.741102\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.703420\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.706665\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.717143\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.730979\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.742134\n",
            "\n",
            "Test set: Test loss: 0.8003, Accuracy: 3718/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 74.36%\n",
            "Better loss at Epoch 5: loss = 0.8003313493728638%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.651237\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.670133\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.652226\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.616789\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.592106\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.680992\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.638358\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.648964\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.672702\n",
            "\n",
            "Test set: Test loss: 0.8711, Accuracy: 3654/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.596766\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.567559\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.567643\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.613267\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.570217\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.554702\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.593561\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.576722\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.587164\n",
            "\n",
            "Test set: Test loss: 0.8683, Accuracy: 3688/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.516425\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.515378\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.490287\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.452797\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.530575\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.498517\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.562676\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.553137\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.511493\n",
            "\n",
            "Test set: Test loss: 0.7483, Accuracy: 3823/5000 (76%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 76.46%\n",
            "Better loss at Epoch 8: loss = 0.7482532548904418%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.418088\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.432083\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.404887\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.430650\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.449513\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.438580\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.436407\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.490658\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.585767\n",
            "\n",
            "Test set: Test loss: 0.6944, Accuracy: 3876/5000 (78%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 77.52%\n",
            "Better loss at Epoch 9: loss = 0.6944435071945192%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.392946\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.349837\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.367411\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.375649\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.373721\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.416733\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.439020\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.390755\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.404910\n",
            "\n",
            "Test set: Test loss: 0.7001, Accuracy: 3914/5000 (78%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 78.28%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.312613\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.313182\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.297069\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.319480\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.338929\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.318806\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.322135\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.358986\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.376334\n",
            "\n",
            "Test set: Test loss: 0.7613, Accuracy: 3906/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.263426\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.261777\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.305508\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.289980\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.276834\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.297640\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.344501\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.355618\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.320991\n",
            "\n",
            "Test set: Test loss: 0.7454, Accuracy: 3927/5000 (79%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 78.54%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.192372\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.205775\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.215457\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.263781\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.318691\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.286486\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.288441\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.264556\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.265803\n",
            "\n",
            "Test set: Test loss: 0.7869, Accuracy: 3917/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.200777\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.195728\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.220488\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.216786\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.208368\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.217917\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.242317\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.239208\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.241726\n",
            "\n",
            "Test set: Test loss: 0.8456, Accuracy: 3961/5000 (79%)\n",
            "\n",
            "Better accuracy at Epoch 14: accuracy = 79.22%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.142534\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.146977\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.175237\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.179568\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.186190\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.190255\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.239622\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.201668\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.186808\n",
            "\n",
            "Test set: Test loss: 0.9166, Accuracy: 3898/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.145201\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.146945\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.146466\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.165671\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.155256\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.162027\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.181210\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.168456\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.168845\n",
            "\n",
            "Test set: Test loss: 0.9717, Accuracy: 3868/5000 (77%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.124643\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.118309\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.113364\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.149640\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.126204\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.135000\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.143380\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.150432\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.150947\n",
            "\n",
            "Test set: Test loss: 0.9626, Accuracy: 3876/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.096121\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.091762\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.093378\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.111767\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.139979\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.120372\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.128084\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.131190\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.129640\n",
            "\n",
            "Test set: Test loss: 0.9579, Accuracy: 3903/5000 (78%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.082210\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.078754\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.081105\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.103885\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.095182\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.102985\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.111097\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.110003\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.108483\n",
            "\n",
            "Test set: Test loss: 1.0207, Accuracy: 3918/5000 (78%)\n",
            "\n",
            "CPU times: user 5min 49s, sys: 28.9 s, total: 6min 18s\n",
            "Wall time: 6min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e682a266-5800-4535-ac18-dfaa554e497f",
        "id": "vPCj-YnR8Mi4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "# visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AlexNet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeVhV1frA8e/LjEwKIqioOCCCA4o4\nlDmlmZZmllqWqc1ZXX/NebuN3lu3OdPrbbY5rfRWZpaVWU7lmPOEigOOiKgIMq/fH/tgqIAM53CA\n836e5zycc/bae7/nAPvde6211xJjDEoppVyXm7MDUEop5VyaCJRSysVpIlBKKReniUAppVycJgKl\nlHJxmgiUUsrFaSJQ1ZaIfCAi/3J2HGUlIv8WkfvssJ1NItLH3mUvsJ1xIrLE9txbRLaKSGhlt6tq\nBk0EyulE5FcRSRMRbwduP0tEmhR5r7+I7C7j+k+LyCcXKBMKjAHeKvJeXRF5Q0QOiUimiGwQkZsv\ntD9jTFtjzK9lia08ZcvKGJMNTAcm2nO7qvrSRKCcSkQigZ6AAa5y4K4ygCccuP1xwDxjzGkAEfEC\nfgaaARcBQcDDwPMi8kBxGxARDwfGV16fAWMdlZxV9aKJQDnbGOAP4ANgbGkFRWSwiKwVkeMiskxE\nOtjebykix0Qk3va6kYiknFNlMgUYJSItS9h2IxGZbVsvSUQm2N4fCDwGXCcip0RkXQnhDQJ+K/L6\nJqApMMIYk2SMyTXG/ABMACaJSKBt+7tF5FERWQ9kiIiH7b3+tuW+IvKh7Yppi4g8IiLJReIuWvZp\nEflCRD4SkXRbtVFCkbITRWSnbdlmERlW0ndtjEkG0oDuJZVRtYcmAuVsY4BPbY/LRSSsuEIi0gmr\nuuJOIASrCmaOiHgbY3YCjwKfiEgd4H3gw3OqTPYD7wDPFLNtN+BbYB3QGOgH3Ccil9sO3s8Bnxtj\n/I0xcSV8jvbAtiKvLwO+N8ZknFNuNuCDdZVQaBRwJVDXGJN3TvmngEighW2bo0vYf6GrgJlAXWAO\n8J8iy3ZiXX0FYX0Pn4hIw1K2tQUo6fOqWkQTgXIaEbkEq+rkC2PMaqwD1Q0lFL8DeMsYs9wYk2+M\n+RDIxnbGaox5B9gBLAcaAv8oZhv/BoaISNtz3u8ChBpjJhljcowxu7CSxvXl+Dh1gfQir+sDB88t\nZDvQH7UtLzTFGLOvsFrpHCOB54wxabaz9CkXiGOJMWaeMSYf+JgiB3JjzJfGmAPGmAJjzOdAItC1\nlG2l2z6XquU0EShnGgv8aIw5anv9GSVXDzUDHrRVCx0XkeNAE6BRkTLvAO2AqbYGz7MYY1KwzpAn\nFbPtRuds+zGg2KuTEqQBAUVeH8VKSGextQPUty0vtK+U7TY6Z3lpZQEOFXmeCfgUtj2IyJgiVWvH\nsb6r+sVtxCYAOH6B/alaoDo1TikXIiK+WGe77iJSePDyBuqKSJwx5ty6+H3As8aYZ0vYnj8wGXgP\neFpEZhtjjhVT9CVgF7DinG0nGWOiSgi3LEP0rgdaAyttr38GnhMRv3Oqh67FupL5o4zbPwhEAJtt\nr5uUUrZEItIMK1H2A343xuSLyFpASlktBnilIvtTNYteEShnuRrIB2KBjrZHDLAYq93gXO8Ad4lI\nN7H4iciVIlJ4Fv46sMoYcxvwHfBmcTs1xhzHOrg9UuTtFUC6rdHWV0TcRaSdiHSxLT8MRNraEkoy\nD+hd5PXHQDLwpYhEioiniFyOVbXztDHmRCnbKuoL4O8iUk9EGgP3lnG9c/lhJZwUAFs31nYlFbbt\nK5izE5aqpTQRKGcZC7xvjNlrjDlU+MCqurnx3K6UxphVwO225WlY7QHjAERkKDAQGG8r/gAQLyI3\nlrDv17GSUOG284HBWMkoCava5l2sRlWAL20/U0VkTQnb/Ai4wnalU9gXvz/W1cZy4CTwKvAPY8xL\npXwv55qElVCSsK4yZmFdUZSLMWYzVgL8HSuxtQeWlrLKDVgN7uXel6p5RCemUco+ROQ54IgxZrID\n9zEeuN4Y0/uChSu+D2+sHlS9jDFHHLUfVX1oIlCqGrN172yBdSYfhVXt9R9HJhvlerSxWKnqzQvr\nnonmWD14ZgL/dWpEqtbRKwKllHJx2lislFIursZVDdWvX99ERkY6OwyllKpRVq9efdQYU+zQ4jUu\nEURGRrJq1Spnh6GUUjWKiOwpaZlWDSmllIvTRKCUUi5OE4FSSrm4GtdGoJSqWrm5uSQnJ5OVleXs\nUFQZ+Pj4EBERgaenZ5nX0USglCpVcnIyAQEBREZGIlLaYKXK2YwxpKamkpycTPPmzcu8nlYNKaVK\nlZWVRUhIiCaBGkBECAkJKffVmyYCpdQFaRKoOSryu3KZRLD10En+/f0W0rNynR2KUkpVKy6TCPYd\nO81bv+1i++FTzg5FKVUOqampdOzYkY4dOxIeHk7jxo3PvM7JySnTNm6++Wa2bdtWaplp06bx6aef\n2iNkLrnkEtauXWuXbVUFl2ksjg6zJrJKPJxO52b1nByNUqqsQkJCzhxUn376afz9/XnooYfOKmOM\nwRiDm1vx57bvv//+Bfdzzz33VD7YGsplrggi6vni6+nOtsPpzg5FKWUHO3bsIDY2lhtvvJG2bdty\n8OBB7rjjDhISEmjbti2TJk06U7bwDD0vL4+6desyceJE4uLiuOiiizhyxJp75/HHH2fy5Mlnyk+c\nOJGuXbsSHR3NsmXLAMjIyODaa68lNjaW4cOHk5CQcMEz/08++YT27dvTrl07HnvsMQDy8vK46aab\nzrw/ZcoUAF577TViY2Pp0KEDo0ePtvt3VhKXuSJwcxNah/mzXROBUhX2zLeb2HzgpF23GdsokKeG\ntK3Qulu3buWjjz4iISEBgOeff57g4GDy8vLo27cvw4cPJzY29qx1Tpw4Qe/evXn++ed54IEHmD59\nOhMnTjxv28YYVqxYwZw5c5g0aRI//PADU6dOJTw8nNmzZ7Nu3Tri4+NLjS85OZnHH3+cVatWERQU\nRP/+/Zk7dy6hoaEcPXqUDRs2AHD8+HEAXnzxRfbs2YOXl9eZ96qCy1wRAESFBWgbgVK1SMuWLc8k\nAYAZM2YQHx9PfHw8W7ZsYfPmzeet4+vry6BBgwDo3Lkzu3fvLnbb11xzzXlllixZwvXXXw9AXFwc\nbduWnsCWL1/OpZdeSv369fH09OSGG25g0aJFtGrVim3btjFhwgTmz59PUJA1PXbbtm0ZPXo0n376\nabluCKssl7kiAKudYNbqZI5l5BDs5+XscJSqcSp65u4ofn5+Z54nJiby+uuvs2LFCurWrcvo0aOL\n7U/v5fXX/767uzt5eXnFbtvb2/uCZSoqJCSE9evX8/333zNt2jRmz57N22+/zfz58/ntt9+YM2cO\nzz33HOvXr8fd3d2u+y6OS10RtA63Goy1ekip2ufkyZMEBAQQGBjIwYMHmT9/vt330aNHD7744gsA\nNmzYUOwVR1HdunVj4cKFpKamkpeXx8yZM+nduzcpKSkYYxgxYgSTJk1izZo15Ofnk5yczKWXXsqL\nL77I0aNHyczMtPtnKI5LXRG0DvMHrETQvUWIk6NRStlTfHw8sbGxtGnThmbNmtGjRw+77+Nvf/sb\nY8aMITY29syjsFqnOBEREfzzn/+kT58+GGMYMmQIV155JWvWrOHWW2/FGIOI8MILL5CXl8cNN9xA\neno6BQUFPPTQQwQEBNj9MxSnxs1ZnJCQYCo6MY0xhg7P/MjQjo3419Xt7RyZUrXTli1biImJcXYY\n1UJeXh55eXn4+PiQmJjIgAEDSExMxMOjep1TF/c7E5HVxpiE4so7LHoRaQJ8BIQBBnjbGPP6OWUE\neB24AsgExhlj1jgwJqLDAth+SBuMlVLld+rUKfr160deXh7GGN56661qlwQqwpGfIA940BizRkQC\ngNUi8pMxpmil2iAgyvboBrxh++kwUWEBzNtw8MwlmVJKlVXdunVZvXq1s8OwO4c1FhtjDhae3Rtj\n0oEtQONzig0FPjKWP4C6ItLQUTEBRIf5c+J0Linp2Y7cjVJK1RhV0mtIRCKBTsDycxY1BvYVeZ3M\n+ckCEblDRFaJyKqUlJRKxVLYc0jvMFZKKYvDE4GI+AOzgfuMMRW6JdEY87YxJsEYkxAaGlqpeArH\nHNp2SBOBUkqBgxOBiHhiJYFPjTH/K6bIfqBJkdcRtvccJsTfmxA/LxL1DmOllAIcmAhsPYLeA7YY\nY14todgcYIxYugMnjDEHHRVTodZhAVo1pFQN0bdv3/NuDps8eTLjx48vdT1/f+u+oQMHDjB8+PBi\ny/Tp04cLdUefPHnyWTd2XXHFFXYZB+jpp5/m5ZdfrvR27MGRVwQ9gJuAS0Vkre1xhYjcJSJ32crM\nA3YBO4B3gLsdGM8Z0eEBJB5Op6bdQ6GUKxo1ahQzZ848672ZM2cyatSoMq3fqFEjZs2aVeH9n5sI\n5s2bR926dSu8verIkb2GlhhjxBjTwRjT0faYZ4x50xjzpq2MMcbcY4xpaYxpb4yp2J1i5RQV5k9G\nTj77j5+uit0ppSph+PDhfPfdd2cmodm9ezcHDhygZ8+eZ/r1x8fH0759e7755pvz1t+9ezft2rUD\n4PTp01x//fXExMQwbNgwTp/+6xgwfvz4M0NYP/XUUwBMmTKFAwcO0LdvX/r27QtAZGQkR48eBeDV\nV1+lXbt2tGvX7swQ1rt37yYmJobbb7+dtm3bMmDAgLP2U5y1a9fSvXt3OnTowLBhw0hLSzuz/8Jh\nqQsHu/vtt9/OTMzTqVMn0tMrX7tR8++EqIDCBuPth9OJqFfHydEoVYN8PxEObbDvNsPbw6DnS1wc\nHBxM165d+f777xk6dCgzZ85k5MiRiAg+Pj589dVXBAYGcvToUbp3785VV11V4j1Cb7zxBnXq1GHL\nli2sX7/+rGGkn332WYKDg8nPz6dfv36sX7+eCRMm8Oqrr7Jw4ULq169/1rZWr17N+++/z/LlyzHG\n0K1bN3r37k29evVITExkxowZvPPOO4wcOZLZs2eXOr/AmDFjmDp1Kr179+bJJ5/kmWeeYfLkyTz/\n/PMkJSXh7e19pjrq5ZdfZtq0afTo0YNTp07h4+NTnm+7WC416FyhqDM9h7TBWKmaoGj1UNFqIWMM\njz32GB06dKB///7s37+fw4cPl7idRYsWnTkgd+jQgQ4dOpxZ9sUXXxAfH0+nTp3YtGnTBQeUW7Jk\nCcOGDcPPzw9/f3+uueYaFi9eDEDz5s3p2LEjUPpQ12DNj3D8+HF69+4NwNixY1m0aNGZGG+88UY+\n+eSTM3cw9+jRgwceeIApU6Zw/Phxu9zZ7JJXBEG+njQM8tFRSJUqr1LO3B1p6NCh3H///axZs4bM\nzEw6d+4MwKeffkpKSgqrV6/G09OTyMjIYoeevpCkpCRefvllVq5cSb169Rg3blyFtlOocAhrsIax\nvlDVUEm+++47Fi1axLfffsuzzz7Lhg0bmDhxIldeeSXz5s2jR48ezJ8/nzZt2lQ4VnDRKwIonKRG\nE4FSNYG/vz99+/bllltuOauR+MSJEzRo0ABPT08WLlzInj17St1Or169+OyzzwDYuHEj69evB6wh\nrP38/AgKCuLw4cN8//33Z9YJCAgoth6+Z8+efP3112RmZpKRkcFXX31Fz549y/3ZgoKCqFev3pmr\niY8//pjevXtTUFDAvn376Nu3Ly+88AInTpzg1KlT7Ny5k/bt2/Poo4/SpUsXtm7dWu59nsslrwjA\nGmriw12p5BcY3N10zCGlqrtRo0YxbNiws3oQ3XjjjQwZMoT27duTkJBwwTPj8ePHc/PNNxMTE0NM\nTMyZK4u4uDg6depEmzZtaNKkyVlDWN9xxx0MHDiQRo0asXDhwjPvx8fHM27cOLp27QrAbbfdRqdO\nnUqtBirJhx9+yF133UVmZiYtWrTg/fffJz8/n9GjR3PixAmMMUyYMIG6devyxBNPsHDhQtzc3Gjb\ntu2Z2dYqw6WGoS7qy1X7eHjWen55sDctQv3tEJlStZMOQ13zlHcYapetGmp9pueQNhgrpVybyyaC\nqCKzlSmllCtz2URQx8uDpsF1dKgJpcqgplUhu7KK/K5cNhGANYdxoiYCpUrl4+NDamqqJoMawBhD\nampquW8yc9leQ2C1E/y6LYWcvAK8PFw6JypVooiICJKTk6nsXCCqavj4+BAREVGudVw6EUSHB5BX\nYEg6mkG0bcIapdTZPD09ad68ubPDUA7k0qfBrYuMOaSUUq7KpRNBi1A/3N1EE4FSyqW5dCLw9nAn\nMqSOTluplHJpLp0IwGon0CsCpZQrc/lEENUggD3HMsnKzXd2KEop5RQunwiiwwMwBnYc0aEmlFKu\nyeUTQeszk9Ro9ZBSyjW5fCKIDKmDl7sb249oIlBKuSaXTwQe7m60CPVju14RKKVclMsnAijsOaRt\nBEop16SJAKudYP/x06Rn5To7FKWUqnKaCPirwThRew4ppVyQJgIgunDMIW0nUEq5IE0EQEQ9X3w9\n3XWSGqWUS9JEALi5iW2SGq0aUkq5Hk0ENlFhAXpFoJRySZoIbKLDAkhJzyYtI8fZoSilVJXSRGDT\nOlwnqVFKuSZNBDatw/wBTQRKKdejicAmPNCHAB8PbSdQSrkcTQQ2IkJ0mA41oZRyPZoIiogKs2Yr\nM8Y4OxSllKoymgiKiA7z53hmLinp2c4ORSmlqowmgiL+6jmk1UNKKdehiaCIwjGHtMFYKeVKNBEU\nEeLvTYiflw4+p5RyKZoIztFah5pQSrkYhyUCEZkuIkdEZGMJy/uIyAkRWWt7POmoWMojOjyARO05\npJRyIY68IvgAGHiBMouNMR1tj0kOjKXMosL8ycjJZ//x084ORSmlqoTDEoExZhFwzFHbd5Qzk9Ro\n9ZBSykU4u43gIhFZJyLfi0jbkgqJyB0iskpEVqWkpDg0oKgw7UKqlHItzkwEa4Bmxpg4YCrwdUkF\njTFvG2MSjDEJoaGhDg0qyNeThkE+2nNIKeUynJYIjDEnjTGnbM/nAZ4iUt9Z8RSlk9QopVyJ0xKB\niISLiNied7XFkuqseIqKDvNnx5FT5BdozyGlVO3n4agNi8gMoA9QX0SSgacATwBjzJvAcGC8iOQB\np4HrTTXps9k6LIDsvAL2HsukeX0/Z4ejlFIO5bBEYIwZdYHl/wH+46j9V0brwqEmDqVrIlBK1XrO\n7jVULUXpbGVKKReiiaAYdbw8aBpcRxOBUsolaCIoQeswf00ESimXoImgBK3DAtiVkkFOXoGzQ1FK\nKYfSRFCC6PAA8goMu1MznB2KUko5lCaCEhTtOaSUUrWZJoIStAj1w91NtJ1AKVXraSIogbeHO5Eh\n2nNIKVX7aSIoRXR4gI5CqpSq9TQRlCKqQQC7UzPIys13dihKKeUwrpMIju6A/90JuVllXiU6PABj\nYMcRvSpQStVerpMIju+G9TNhQdlnxGyts5UppVyA6ySCVv2h6x3wxzTY9WuZVokMqYOXu5vOTaCU\nqtVcJxEA9H8GQqLg67vh9PELFvdwd6NFqJ/OVqaUqtVcKxF41YFr3oZTh2Hew2VaRXsOKaVqO9dK\nBACN46H3o7DhC9g4+4LFW4cFsP/4adKzcqsgOKWUqnqulwgALnkAGifA3Afg5IFSixY2GCdqzyGl\nVC3lmonA3cOqIsrPsdoLCkoeYTS6MBFog7FSqpZyzUQAENISLn8Wdi2Ele+WWCyini++nu5sO6RX\nBEqp2sl1EwFA55shagD89ASkbC+2iJubEKWT1CilajHXTgQicNVU8KwD/7sd8otvEG4dFqCJQClV\na7l2IgAICIchr8PBtfDbi8UWiQ4L4Eh6NmkZORXfT34uHFhb8fWVUspBNBEAxF4FcTfA4pdh38rz\nFrcOr+RQE2l74P1B8HZv2DK3MpEqpZTdaSIoNOh5CIyAr+6A7LMbhluH+QMVTASbv4G3ekLKNvAP\ng8WvgDHl3szBE6d58YetnMjU+xmUUvaliaCQTxAMewOOJcGPj5+1KDzQhwAfj/LdYZx72rpP4Ysx\nENwS7lwEff4OB9ZA0m/lCi0tI4eb3lvBf3/dyfM/bC3XukopdSFlSgQi0lJEvG3P+4jIBBGp69jQ\nnCDyErj4b7D6fdg+/8zbIkJ0WEDZB59L2Qbv9odV71nbu2U+BDeHjjeAf7h1VVBGGdl5jPtgJXuP\nZdI3OpSZK/eyZm9aeT+ZUkqVqKxXBLOBfBFpBbwNNAE+c1hUznTp49CgLXxzL2Sknnk7ytZzyJRW\nrWMM/PkJvN0H0g/CjbNgwL/Aw8ta7uENF98LSYsgedUFQ8nOy+euT1azIfk4/xnViak3xBMW4MPj\nX20kL7/km+CUUqo8ypoICowxecAwYKox5mGgoePCciIPb+uu46zj8O2EM/X50WH+HM/MZXdqZvHr\nZadbXVC/uQcad4a7lkLUZeeX63wz+NSFxa+WGkZ+geGBL9axOPEoz1/bgQFtw/H39uDJIbFsPniS\nj//YU9lPqpRSQNkTQa6IjALGAoXdXjwdE1I1EN4OLn0Cts6FdTMA6NU6FD8vd65/+3c2JJ84u/yB\nP+GtXtYgdn0fhzHfQGAJedLbH7rdBdu+g8Obiy1ijOGpORv5bv1BHruiDSMTmpxZNqhdOL1ah/LK\nj9s5fLLss60ppVRJypoIbgYuAp41xiSJSHPgY8eFVQ1cdA80uwTmPQJpe2gR6s/suy/Gw82NEW8t\nY96Gg9bVwu//hXcvg7xsGPcd9H4Y3NxL33a3O8HTD5a8Vuzi135O5JM/9nJn7xbc0avlWctEhElX\ntSUnv4B/fbfFXp9WKeXCypQIjDGbjTETjDEzRKQeEGCMecHBsTmXm7vViwjgq7ugIJ824YF8c28P\nYhsG8o9PfyVp6hCY/3erCuiuJdDs4rJtu04wJNxsXUEcSzpr0ftLk5iyIJGRCRFMHNim2NUj6/tx\nd5+WfLvuAEsSj1bmUyqlVJl7Df0qIoEiEgysAd4RkdIruWuDuk3hihdh7zL4/T8A1Pf3ZsaAfBb6\nP06j1N+Z3WACWdd+bB3cy+Oie6xks2zKmbe+WbufZ77dzIDYMJ4b1h4RKXH1u3q3JDKkDk9+s5Hs\nvPwKfTyllIKyVw0FGWNOAtcAHxljugH9HRdWNRI3CmKGwIJ/WkNE/Po83p8OJSgwiK87f8iDe7tz\nw7vLSUnPLt92AxtZ3Un//BTSD7Fw2xEe/GId3VsEM2VUJzzcS//V+Hi6M2loO3YdzeDt33ZV4gMq\npVxdWROBh4g0BEbyV2OxaxCBwa+Dbz3r3oBf/w3tRyJ3/sZ1Vw3mjRvj2XzwJFdPW8rWQyfLt+0e\n/wcFuRya/yrjP1lNdHgA74xJwMfzAm0MNr1ah3Jl+4b8Z+EO9pbUm0kppS6grIlgEjAf2GmMWSki\nLYBEx4VVzfiFWO0FgQ3h6jfhmrfA2xp/aFD7hnx558XkFRRw7X+XsWDL4bJvN7gFJ1sOIWDjh7QK\nyOODm7sS4FO+zlhPDI7Fw014as7G0u9xUEqpEpS1sfhLY0wHY8x42+tdxphrHRtaNdOqP9y3ATqO\nOm9R+4ggvrnnElqE+nPbR6t4d/GuMh2U9x3LZPzuXviRxacd1hEa4F3usMKDfLj/stYs3JbC/E3l\nSEJKKWVT1sbiCBH5SkSO2B6zRSTC0cHVJOFBPnxx50UMbBvOv77bwt//t4GcvJLv/j16Kpub3lvO\nxvymnGrWn6B170FORoX2Pe7iSNqEB/DMt5vIyM6r6EdQSrmoslYNvQ/MARrZHt/a3lNF+Hq5M+2G\neO7p25KZK/cxZvryYucwSM/KZez0FRw6mcX0cQn4938UTh+D1R9WaL8e7m48O6wdB09kMWWB69TY\nKaXso6yJINQY874xJs/2+AAILW0FEZluu3rYWMJyEZEpIrJDRNaLSHw5Y6+W3NyEhy9vw2vXxbFm\nz3GG/XcpO478NWppVm4+t3+0im2H0nljdGc6NwuGJl2tm9eWTbVuTKuAzs2CuS6hCe8tSWLbIZ1N\nTala5UQyLHoZdpVv5OKyKmsiSBWR0SLibnuMBlIvsM4HwMBSlg8ComyPO4A3yhhLjTCsUwQz7uhG\nelYew/67lCWJR8nLL2DCjD/5Y9cxXhkZR9/oBn+t0PMBSD8A62ZWeJ8TB7UhwMeDx7/eoA3HStV0\nORnW8eDDq+C1dvDLP60BKx2grIngFqyuo4eAg8BwYFxpKxhjFgHHSikyFOueBGOM+QOoa+uiWmt0\nbhbM1/f0oFGQL2PfX8EN7yznx82HeXpILEM7Nj67cMtLoWEcLJ0MBRW7QayenxcTB7Vh5e40Zq/Z\nb4dPoJSqUgUFkLQYvr4bXm4NX90Jabuhz0SYsBb6PeGQ3Za119AeY8xVxphQY0wDY8zVQGV7DTUG\n9hV5nWx77zwicoeIrBKRVSkpKZXcbdVqElyHWeMvonfrUFbsPsaES1sxrkfz8wuKQM8H4dgu2Px1\nhfc3onMTOjerx3PztnA8sxJzLCulqk7qTvjlWZgSBx8Ohs1zoO3VMG6elQD6TLTmNHEQqWgVgojs\nNcY0vUCZSGCuMaZdMcvmAs8bY5bYXi8AHjXGlDpQf0JCglm16sJj+Vc3+QWGbYfSiWkYUPLQEQUF\nMK0rePjAXYut5FABWw6eZPDUJVzXpQnPDWtfiaiVUg6TdQI2fQVrZ8C+PwCBFn2sEQfaDAavOnbd\nnYisNsYkFLfMozLbrcS6APuxJrgpFGF7r1ZydxNiGwWWXsjNDS65H765GxJ/gtYDKrSvmIaBjLs4\nkulLkxjROYJOTetVaDtK1XjGWMPEb5kD23+E/BxrKHjvAPAOtP0MAC//v56fef+ccl7+4Olb4RM0\nwKr23bXQOvhvnQt5WVC/NfR7CjpcB0HFVoo4XGUSQWVbI+cA94rITKAbcMIYc7CS26z5Ooy0hrFY\n/EqFEwHA/Ze1Zu76Azz+9Ua+uafHBccuUqrWKCiA/atg8zdWFcuJvSDu1lS0dUKsSaSy0yFzN2Sf\ntJ5nnQRTxrY5N09rAit3ryI/fayZCN29i1nmbb0vwI4F1uyFPnWh02iIuwEax1cuudhBqYlARNIp\n/oAvgO8F1p0B9AHqi0gy8L39kTwAACAASURBVBS2yWyMMW8C84ArgB1AJtacB8rd05rn+PtHYM+y\nsg9tfQ5/bw+eHNyWez5bwyd/7Cm+XUKp2qIgH/b+bh38t3xrHWzdPK1OGH0ehegrSh8h2Bjr7Lww\nSZz3sCWMvCyri3d+ju1nNuTlnP8z5xRkphYpl2M9GneGgc9D9CArQVQTFW4jcJaa2kZQLjmZMLk9\nNOoIo2dXeDPGGMa+v5I/96Sx4MHeNAj0sWOQSjlZfi7sXmyd9W+dCxkp1pl5q/4QOxRaXw4+Qc6O\nstpwVBuBchSvOtB9vNVv+MBaKyFUQOFsZgMmL+Jf321hyqhOdg5UqSqWlw27frUO/tu+g9Np1mx/\nrQdAzFUQNcCq21floomguupyGyx93ZrOcmTFhp6Av2Yzm/xzItd1aUKPVvXtGKRSVcAYSF4Jq6bD\n1u+sahrvQGg90Drzb9XPasRVFaaJoLryrWslgyWvwdFEqB9V4U3d1bslX/+5nye+3sj39/XE26Ns\n8x0o5VS5p2HDLFjxNhxaD14B1oE/9iqrm2U1qmOv6bQrSXXW/W7rj33p5EptxsfTnWdss5m9tyTp\nwiso5Uxpu+HHJ+DVGJhzr9UWcOWr8OBWuHqaVfevScCu9IqgOvMPhfgx1iVx74lQt8mF1ylB79ah\nDIgN4z+/7OCaThGEB2nDsapGCgpg1y+w4h3YPh/EDdpcCV3vsLp9Orl7ZW2nvYaqu+N7YUonq5po\n0AuV2tS+Y5n0f/U3Lm8bXvUNxzkZViNffo7Vrc/Nw/Zwt7rMFn1d0nJPX+0FUtucPg7rZlgJ4NhO\n8AuFzuOg881Ou7mqttJeQzVZ3abQfqQ1V0Gvh8Gv4o29TYLrcGfvlkxZkMjo7s3o2ryUftX2kroT\nVr4Lf34K2Scqv70ut8OAf2rjYE13eJN18F//OeRmQkRX6PN3q/5fq32qnF4R1AQp22BaN2tQuoqO\nPmgMGMPpPEP/V38j0NeTuX+7BHc3B1xyF+Rbl/cr34Gdv1hn+LFDofNY64yvIM+q9y3It54X5Np+\n5hdZVvi68HkeHNpgVZPVj4Zr34WGHewfu3Kc/Fyr18+Kd2DPEqvPf/vhVnKvYBdpVXZ6RVDThUZD\nzGCr90T6Idvdi9nWP9ZZdzRmn30n41llskEE38YJfNC8C4+uDWXG7w0Z3aOV/eLMSIU/P4KV063b\n+gMaQd9/QPxYCAizzz7aDIavx8O7/azxWbrfbY3RpKonY+DAGtgwGzbOhlOHoG4zuOyf1hALpd3t\nq6qMXhHUFIc2woxR1ngoxY1j4n7u+CfeRcY+sf3Mz4bdSzD71yAYTlIHn6i+eEX3t27FrxdZsdiS\nV1tn/xv/Z+0jsid0vd26rd/d065fA2AlnDl/s24oatEHrn4TAmvVVBY135EtVtfPjbMhLcn6u4wa\nAJ1ugqjLrLYfVaVKuyLQROCKMo9xYO18Fn//OZf7bKJu7hHr/eAWVkJoeal1MPcpZbTU3CzY9D/r\nMv/AGmtkxrjrrUbtBjGO/wzGwOoPYP5jVuK7airEDHH8flXJ0nZbB/4Ns+HIJqvnT/PeVvVPm8HW\nvTHKaTQRqGI98+0mPliWxPzRjWh9aqVVn5+0GHIzrNEam3T9KzE06mSdxaXtgVXvwZqP4fQxawjd\nLrdbSaC0xOEoRxNh9m1wcK3V1fbyf+sQA1Up/bA1pv7GWdbdvwBNukG74dbEKv4NSl9fVRlNBKpY\nJ07ncunLv9K8vh9f3nWRNWFOXg4kr7CSws5frLGOMNawuaHRsG+F1ae7zZVWAmjey/l9vPNy4Nfn\nYMlk66rm2nesUR6VY5xOs0b43DDLGvTNFEBYe2h/LbS71urppqodTQSqRJ+v3Mujszcw+bqOXN2p\nmH7bGamQ9KuVFA6ut+p5E26GoIgqj/WCkhZbc7yeOgx9H4Me92ldtL2kH4ak36yz/8SfrN5cwS2s\nM//2w62TBFWtaSJQJSooMFz936UcOpHFLw/1wd+7hnckO50G395nzfvcrAcMe6tSd2S7rBPJsHup\n1c1zzzJI3WG9H9AI2l1jnfk36uT8q0FVZpoIVKn+3JvGsP8u467eLZk4qI2zw6k8Y6y7Vec9bLV1\nDHnNOnCp4hljNfTuWWod9HcvgeN7rGU+QdD0YmuCpMge0LCTdtetofQ+AlWqTk3rMbxzBO8t2cXI\nhAhahFa/xtYj6Vm88etO7u7TitCAC9x5KmJNAN60O8y+HWbdYlVnDHrROQ3alVHYD3/d57Dte6s7\nrn+Y1Qh71s8iz/1Cwb2Uf21jrDu+9yyxnfUvhZO26cJ9g62Dfvfx1hVVWFutXnMBmggUAI8ObMP8\njYeYNHcz74/rYjUcVxMns3IZO30lWw6exNfTnUcGlvGqJbgF3PIDLHrJeuxZBt3utLoy1mvm2KAr\nK20PrP/CGoIhNdG6D6RVf6ur7Kkj1hANOxeWMGyHWHPznpswfOtad2fvWWa1owD4NbDO9JvZHqFt\n9IzfBWnVkDrj3cW7+Nd3W3hvbAL9Yux0J3AlZeXmM+79FazanUbTkDpkZuezdOKl5R8aY+8fMO8h\n60AI0DAO2gyx7j0Ija4edd2n02DT19bBf+/v1nvNekCH66whOorrh5972koMp45YB/ezHkfO/pmf\nY9XxFx74Iy+BkFbV47Mrh9M2AlUmufkFDHp9Mbn5Bcy/rxc+ns6tEsgvMPxtxhrmbTjE5Os64u3h\nxvhP1/DBzV3oE13B/umpO635bbfMtbrJgnUwjBliJYbG8VV7YMzLgcQfYf1Ma3ym/Bzr3owO10GH\nkfbrimmMNaG6l78e+F2UthGoMvF0d+OpIbHc9N4K3luSxD197TgOUTkZY3h6zibmbTjE41fGcHWn\nxuTkFVCvjidfrkqueCIIaQk9/s96nDxoDVOx5VtYNtWaDS6wsXWPRMwQq5G0tLr2ijLGuh9j/Uyr\nO+bpNKteP+FWiLsOGna0/8FaBLwD7LtNVWtoIlBn6RkVyuVtrQlshnVqTKO6zhnueeovO/j4jz3c\n2bsFt/VsAYCXhxtDOzbms+V7ScvIoZ6fV+V2EtjQGhKjy22Qecw6I986F9Z8ZA3w5xtsjZcUMxha\n9AXPck7mY4x1hp97GvKyIDMVNn9jVf2k7QYPXyvpxF1vbd8RSUepMtCqIXWewglsBrQNZ2pVT2AD\nfLp8D//4aiPXxkfw8ogOZzVcbz5wkiumLObpIbGM69HcMQHkZMCOn63qo+0/WJOle/lbA9x5B1gH\n9dws62fhIzcL8k5bo7zm2n7mZQHn/n+JdTd23PVWo3VN68WkaiytGlLl0iS4Dnf1bsnrCxK5sVtT\nurcIqbJ9/7DxIE98vZG+0aE8f23783ovxTYKpG2jQL5cney4RODlZ5skfahVh797kZUUdv5ineV7\neFtXBx62h1+o9Z6Hr22ZbzGvfaztRvbUmbdUtaOJQBXrrt4tmbU6mafnbGLu3y7Bw93xXQr/2JXK\nhJlriWtSl2k3xuNZwj5HJjThqTmb2HTgBG0bOXjqSg8vq9tmq/6O3Y9STqQdhlWxfL3cefzKGLYe\nSuezFXsdvr/NB05y+4eraFLPl+lju1DHq+RzlKEdG+Hl7saXq5IdHpdSrkATgSrRwHbh9GgVwis/\nbudYRo7D9rPvWCZj31+Bn7cHH93a7YKNwHXreHFZbBjfrN1Pdl6+w+KyJ2MMG/efIC+/wNmhKHUe\nTQSqRCLCU0Pacio7j5fmb3PIPlJPZTNm+gqyc/P56NauNC5jL6URCRGkZeayYMsRh8RlL8YYftl6\nmMFTlzB46hIe+GIdBQU1q4OGqv00EahStQ4LYOxFkcxcuZeN+4sbzqDiMrLzuPmDlRw4fprp47rQ\nOqzs/dx7RoUSHujDl6v22TUmezHGsDgxhWH/XcYtH6ziZFYuwztHMGfdAZ75dhM1rbeeqt20sVhd\n0H2XRTFn3X6emrOJGbd3x8uj8ucPOXkF3PXJajYdOMlbozuTEFm+Sczd3YRrOzfmjV93cvhkFmGB\n5ezj70DLd6Xyyk/bWZF0jIZBPjw3rD0jEiLwcBOC/bx4e9Eu6vl5cV//1s4OVSlAE4Eqg0AfTx4Z\n2IZHZq2nwzPz6RIZTI9W9enRsj6xjQLLPe5PQYHh4VnrWJx4lBeHd6B/bMXGNRreuQnTFu5k9ppk\n7u7jvLugC63Zm8arP25nyY6jhAZ48/SQWK7v2vSsoTr+PqgNaRk5TP45kXp1vBh7caTzAlbKRhOB\nKpMRnSMIDfDmt20pLN1xlOe/3wpAkK8nF7UIoUerEC5uVZ8W9f1KHbnUGMO/vtvCN2sP8MjAaEYm\nVHzSmOb1/egSWY9Zq5IZ37ul00ZM3bj/BK/+tJ1fth4h2M+Lf1wRw+juzfD1On+sJhHh39e05/jp\nXJ6as4m6dTwZ2lHvK1DOpYlAlYmI0De6AX1tY/wcOZnFsp2pLN1xlGU7U/lh0yEAwgN9uLhVCD1a\n1qdHq/qEB51dZfPmb7uYvjSJm3tEMr53y0rHNSKhCY/MWs+avWl0bla+6qXK2nroJK/9tJ35mw4T\n5OvJw5dHM+7iSPwuMMubh7sbU0d1Yuz0FTz4xToCfT3PfK9KOYMOMaEqzRjDntRMlu48yrIdqSzb\neZS0zFwAWoT62ZJCCCmncnji641cFdeIydd1xK28Q0kXIyM7jy7P/syQDo14YXiHSm+vLHamnGLy\nz4nMXX8APy8Pbr2kObf2bE6gj2e5tpOelcuod/5gx5FTfHJrt3K3kyhVHjoMtapSBQWGLYdOsmxH\nKkt3HmVF0jEyc6z+/pe0qs/0cV3s0uBc6OEv1zFvw0FWPt6/1BvRKmtvaiavL0jkqz+T8fZwZ1yP\nSO7o2aJSg98dPZXNyDd/5+ipbD6/8yJiGurYQ8oxNBEop8rJK2Bd8nG2HjzJsPgI/C9QdVJeK5KO\nMfKt33l5RBzDO0fYdduF1u07zoi3fkeAm7o3464+Lanvf4EpM8soOS2T4W/8Tr4xzL7rYpqG1LHL\ndpUqqrREoPcRKIfz8nCjS2QwN10UafckANAlsh6RIXUcdk9BVm4+D3yxlhA/L357uC+PD461WxIA\niKhXh49v7UpufgGj31vOkfQsu21bqbLQRKBqPBFhREITlicdY09qht23/9L8bexMyeCl4XHnNX7b\nS1RYAO+P68LRU9mMeW8FJ07nOmQ/ShXHoYlARAaKyDYR2SEiE4tZPk5EUkRkre1xmyPjUbXXNfGN\ncROYtdq+A9H9sSuV6UuTGHNRMy6Jqm/XbZ+rU9N6vHVTZ3amnOK2D1dyOqdmjKOkaj6HJQIRcQem\nAYOAWGCUiMQWU/RzY0xH2+NdR8WjareGQb70jApl9upk8u00ls+p7Dwe+nIdzYLrMHFQG7ts80J6\nRoUy+bpOrNqTxj2frSFXB6lTVcCRVwRdgR3GmF3GmBxgJjDUgftTLm5EQgQHTmSxdMdRu2zv2e82\nc+D4aV4ZGefQ3kjnurJDQ569uj2/bD3CI7PW6yB1yuEcmQgaA0Vb75Jt753rWhFZLyKzRKTit5kq\nl9c/JowgX0++tEP10MKtR5ixYh939GpZ5TeqAdzQrSkPXx7NV3/uZ9LczTpInXIoZzcWfwtEGmM6\nAD8BHxZXSETuEJFVIrIqJSWlSgNUNYePpztXd2zE/E2HOJFZ8cbW45k5PDp7PdFhAdx/WZQdIyyf\nu/u05LZLmvPBst1M/WWH0+JQtZ8jE8F+oOgZfoTtvTOMManGmGzby3eBzsVtyBjztjEmwRiTEBoa\n6pBgVe0wIqEJOXkFzFm3/8KFS/DkN5s4lpHDKyPj8PY4f7ygqiIiPHZFDNfGR/DqT9v5+PfdTotF\n1W6OTAQrgSgRaS4iXsD1wJyiBUSkYZGXVwFbHBiPcgFtGwUS0zCwwtVD360/yJx1B/i/flG0a+zg\n+ZDLwM1NeOHa9vSPCePJOZt49cdtZOVqbyJlXw5LBMaYPOBeYD7WAf4LY8wmEZkkIlfZik0QkU0i\nsg6YAIxzVDzKNYgIIzpHsD75BFsPnSzXukfSs3j86w3ERQQxvk/lB8SzFw93N/5zQyeGxjViyi87\nGDh5EYu2axWpsh8dYkLVOscycuj23M+MuSiSJwYX12P5fMYYbvtwFUt2HOW7CT1p1cDfwVFWzJLE\nozzxzUaSjmYwJK4RT1wZQ4NqNCmPqr50iAnlUoL9vOgfE8bXf+4nJ69s/fC/XJ3Mgq1HeGRgm2qb\nBAAuiarP9//Xk/v7t2b+pkP0e+U3Ply22273TijXpIlA1UojEiJIzcjhl60Xntw+OS2TSd9uplvz\nYG6uATOG+Xi683/9o5h/Xy86Nq3LU3M2cfW0paxPPu7s0FQNpYlA1Uq9okJpEODNrNWlD0RXUGB4\nZNZ6jDG8PCLOLnMkVJXm9f346JauTB3ViUMnsxg6bSlPfbORk1k6TpEqH00EqlbycHfjmvgIFm5L\nKXU0z49+382ynak8MTiWJsE1b/hnEWFIXCMWPNibMd2b8dEfe+j3ym98u+6A3oSmykwTgaq1RiRE\nkF9g+GpN8fcU7Eo5xfM/bKVPdCjXdanZN7UH+njyzNB2fHNPD8IDffjbjD8ZM30Fu4/afzRWVfto\nIlC1VstQfzo3q8eXq5PPOzvOyy/gwS/X4e3hzgvXdnDaxPf21iGiLl/f04NnrmrL2r3HGTB5Ea//\nnEh2nt57oEqmiUDVaiM6R7DjyCn+3Hd2Q+pbi3bx597j/PPqdoTVsu6X7m7C2IsjWfBgby5vG85r\nP29n0OTFdhuMz1l+255SqaFDVMk0Eaha7coODfH1dOfLVX/dabz5wEkm/7ydK9s3ZEiHhqWsXbM1\nCPRh6qhOfHRLV/KN4cZ3lzPyzd/56s/kGnd38rKdRxk7fQUPz1rn7FBqJU0EqlYL8PFkUPtw5q47\nwOmcfHLyCnjgi7UE+Xrxz6vb1ZoqodL0ah3K/Pt68dgVbTiSnsX9n6+j23MLeObbTWw/nO7s8C7I\nGMPL87chAj9uPsySxJp9ZVMdaSJQtd6Izk1Iz87jh00HeX3BdrYeSuf5a9oT7Ofl7NCqjI+nO3f0\naskvD/bhs9u60TOqPp/8sYcBry1i+BvLmL06udrOiPbL1iOs2XucJwfH0jS4DpPmbiJPJ+yxKx1i\nQtV6BQWGPi//irubsCc1g2vjI3hpRJyzw3K61FPZ/G/Nfmas2MuuoxkE+HhwTafGjOrWlDbhgc4O\nD7B+d1dMWczp3Hx+fqA3v2w9wp0fr2bS0LaMuSjS2eHVKDrEhHJpbm7C8M4RJB3NoGGQL08OKdv4\nQ7VdiL83t/dqwYIHezPzju5c2qYBM1bsY+DkxQz771K+WLWPzJw8p8Y4d8NBth5K54HLWuPp7saA\n2DB6tArhlR+3k5aR49TYahNNBMoljExoQpvwAF4ZGUeAj6ezw6lWRITuLUJ4/fpOLH+sH49fGcPJ\n07k8Mms93Z5dwBNfb2TTgRNVHldefgGv/bSdNuEBDOnQ6EysTw5uS3pWLpN/3l7lMdVWWjWklDqP\nMYZVe9KYsXwvczccJCevgLiIIF4aEUfrsIAqieHzlXt5dPYG3hmTwGWxYWcte/KbjXy6fC/zJvQk\nOrxq4qnptGpIKVUuIkKXyGBeva4jKx7rx1NDYklOO83fPvuzSm5Oy8rN5/WfE4lrUpf+MQ3OW35/\n/9b4e3swae4mHUrDDjQRKKVKVbeOFzf3aM5LIzqw7XA6UxYkOnyfny3fy4ETWTxyeXSxXXzr+Xlx\nf/8olu5I5afNhx0eT22niUApVSaXtgljROcI3vh1J+v2OW7I64zsPKYt3MHFLUPo0ap+ieVu7N6M\nqAb+/Ou7LTqERiVpIlBKldnjg2MJC/ThwS/XOezu5PeXJpGakcNDl0eXWs7T3Y0nh8Sy91gm05fs\ndkgsrkITgVKqzIJ8PXn+2g7sOHKK1xzQa+d4Zg5vLdpF/5gw4pvWu2D5nlGh9I8J4z+/JHLkZMnD\njavSaSJQSpVL79ahjOralHcW7WL1njS7bvutRbs4lZ3HgwNal3mdx6+MISe/gBfnb7NrLK5EE4FS\nqtz+cWUMDYN8ediOVURH0rN4f2kSV8U1IqZh2e9sjqzvxy2XNGfW6mSHtl3UZpoIlFLl5u/twYvD\nO7DraAYv2+lMfNovO8jNN9zfv+xXA4Xu7duK+v7ePPNt7e1Oeiwjh9RT2Q7ZtiYCpVSF9GhVn5u6\nN+O9pUms3H2sUtvadyyTz1bsZWRCEyLr+5V7/QAfTx4ZGM2avceZs+5ApWKpTnLzC/hp82Hu/HgV\n3Z77mXeXJDlkP5oIlFIVNnFQGyLqWVVElRmX6PUFiYgIE/q1qvA2hsdH0L5xEP+et9XpYyRV1paD\nJ/nn3M10f24Bt3+0itV7jnNzj+Zc06mxQ/bn4ZCtKqVcgp+3By8Nj+P6t//gxR+28fRVbcu9jR1H\n0vnfmmRu6dGchkG+FY7FzU14+qpYrn3jd978dScPDCi9+2l1cywjhzlr9zNrTTIb95/E013oHxPG\n8M4R9Godiqe7487bNREopSqle4sQxl0cyQfLdnN523AuahlSrvVf/Wk7vp7ujO/TstKxdG4WzNCO\njXhr0S5GJDShSXCdSm/TkXLzC/htWwqzViezYOthcvMN7RoH8vSQWIZ2bEy9KpozQxOBUqrSHhkY\nza/bjvDwrHXMv68Xft5lO7Rs3H+CeRsOMaFfFCH+3naJZeKgNvy46TDPf7+VaTfG22Wb9rbtUDqz\nVu/jqz8PcPRUNvX9vRh7USTXdo4oV48pe9FEoJSqtDpeHrw0Io6Rb/3Ov7/fwr+ubl+m9V7+cRt1\n63hyW8/mdoulYZAv4/u05NWftnPTrlS6tyjfFYqjpGXkMGfdAWatTmbD/hN4uAn9YhowvHMT+kQ7\nturnQjQRKKXsoktkMLf2aM67S5IY2LYhl0SVPE4QwIqkY/y6LYW/D2pDoJ3niLijVws+X7mPZ77d\nzNy/XYK7W9XPTZ16KpuVu4/xx65jrEg6xpZDJzEGYhsG8uTgWIZ2bGS3q6DK0kSglLKbhy6P5pet\nR3h09np+uK9niZMAGWN4af5WQgO8HTLlpI+nO3+/og33fvYnn6/cxw3dmtp9H+c6dCKL5UmpLE+y\nDvw7jpyyxeJGfNN6TLg0igFtw2jbKMjhsZSXJgKllN34eLrz8sg4hr+xjOfmbeHf13Qottxv21NY\nuTuNfw5ti6+Xu0NiubJ9Qz6K3MPLP27jyg4NCfK131WHMYZ9x07zR1IqK2wH/r3HMgHrZruEyHpc\nE9+Ybs1DaN84CC+P6t1TXxOBUsqu4pvW4/ZeLXjrt10MbNeQ3q1Dz1peUGB4af42Iur5cl0Xx52p\niwhPDollyH+WMGVBIk8Mrvhc1Vm5+SQdzWD1nrQzB/5DtkHu6tXxpGvzYMZeHEm35sHENAx0SlVU\nZWgiUErZ3f39W7NgyxEenbWe+ff3Outs/IdNh9h04CSvjIhz+Jlyu8ZBXN+lCR8u282ork1p1cC/\nxLLGGFJOZbMrJYOdKafO+pmclkmBbeSKBgHedGsRQtfmwXRrHkyrUH/catiB/1w6Z7FSyiHW7TvO\nNW8s45pOjXlpRBxgTUh/+eRFiAjz7+tVJWfOR09l0/elX+kcWY8Pbu5Kdl4+e1Iz2ZVyip22g/3O\nlAx2pZwiPeuvO5J9PN1oUd+fFqF+tAj1p2WoH3ERdWkWUqfYWdOqu9LmLNYrAqWUQ8Q1qctdvVsw\nbeFOBrUP59I2YXz15352pmTw5uj4Kqs+qe/vzf/1j+Jf323hkhd+4cDx02fO7gEaBvnQItSPqzs2\npmXhQb+BPw0DfWr8mX5ZaSJQSjnMhH5RLNhyhImzNzB3QhCTf06kfeMgLm8bXqVxjLkokg37T5BX\nYLgmPoKWoX60DPWneX2/Mt/8Vptp1ZBSyqE27j/B1dOWEhbow/7jp/nolq70OqcBWTleaVVD1btP\nk1KqxmvXOIh7+rZi//HTdGseTM8L3Gimqp5eEymlHO6evq0wxjC0U+Ma2dBa2zn0ikBEBorINhHZ\nISITi1nuLSKf25YvF5FIR8ajlHIOLw83HhgQTcvQkrtvKudxWCIQEXdgGjAIiAVGici5d3TcCqQZ\nY1oBrwEvOCoepZRSxXPkFUFXYIcxZpcxJgeYCQw9p8xQ4EPb81lAP9HrRqWUqlKOTASNgX1FXifb\n3iu2jDEmDzgBnDdmrIjcISKrRGRVSkqKg8JVSinXVCN6DRlj3jbGJBhjEkJDtduZUkrZkyMTwX6g\nSZHXEbb3ii0jIh5AEJDqwJiUUkqdw5GJYCUQJSLNRcQLuB6Yc06ZOcBY2/PhwC+mpt3hppRSNZzD\n7iMwxuSJyL3AfMAdmG6M2SQik4BVxpg5wHvAxyKyAziGlSyUUkpVIYfeUGaMmQfMO+e9J4s8zwJG\nODIGpZRSpatxYw2JSAqwp4Kr1weO2jEce6vu8UH1j1HjqxyNr3Kqc3zNjDHF9rapcYmgMkRkVUmD\nLlUH1T0+qP4xanyVo/FVTnWPryQ1ovuoUkopx9FEoJRSLs7VEsHbzg7gAqp7fFD9Y9T4Kkfjq5zq\nHl+xXKqNQCml1Plc7YpAKaXUOTQRKKWUi6uViaA6T4gjIk1EZKGIbBaRTSLyf8WU6SMiJ0Rkre3x\nZHHbcmCMu0Vkg23f500QLZYptu9vvYjEV2Fs0UW+l7UiclJE7junTJV/fyIyXUSOiMjGIu8Fi8hP\nIpJo+1mvhHXH2sokisjY4so4KL6XRGSr7Xf4lYjULWHdUv8eHBjf0yKyv8jv8YoS1i31/92B8X1e\nJLbdIrK2hHUd/v1VmjGmVj2whrPYCbQAvIB1QOw5Ze4G3rQ9vx74vArjawjE254HANuLia8PMNeJ\n3+FuoH4py68AvgcE6A4sd+Lv+hDWjTJO/f6AXkA8sLHIey8CE23PJwIvFLNeMLDL9rOe7Xm9Kopv\nAOBhe/5CcfGV5e/BIfimTAAABx9JREFUgfE9DTxUhr+BUv/fHRXfOctfAZ501vdX2UdtvCKo1hPi\nGGMOGmPW2J6nA1s4f56G6m4o8JGx/AHUFZGGToijH7DTGFPRO83txhizCGu8rKKK/p19CFxdzKqX\nAz8ZY44ZY9KAn4CBVRGfMeZHY80DAvAH1gjBTlHC91cWZfl/r7TS4rMdO0YCM+y936pSGxOB3SbE\ncTRblVQnYHkxiy8SkXUi8r2ItK3SwMAAP4rIahG5o5jlZfmOq8L1lPzP58zvr1CYMeag7fkhIKyY\nMtXlu7wF6yqvOBf6e3Cke21VV9NLqFqrDt9fT+CwMSaxhOXO/P7KpDYmghpBRPyB2cB9xpiT5yxe\ng1XdEQdMBb6u4vAuMcbEY803fY+I9Kri/V+QbWjzq4Avi1ns7O/vPMaqI6iWfbVF5B9AHvBpCUWc\n9ffwBtAS6AgcxKp+qY5GUfrVQLX/f6qNiaDaT4gjIp5YSeBTY8z/zl1ujDlpjDllez4P8BSR+lUV\nnzFmv+3nEeArrMvvosryHTvaIGCNMebwuQuc/f0Vcbiwysz280gxZZz6XYrIOGAwcKMtWZ2nDH8P\nDmGMOWyMyTfGFADvlLBfZ39/HsA1wOcllXHW91cetTERVOsJcWz1ie8BW4wxr5ZQJrywzUJEumL9\nnqokUYmIn4gEFD7HalDceE6xOcAYW++h7sCJIlUgVaXEszBnfn/nKPp3Nhb4ppgy84EBIlLPVvUx\nwPaew4nIQOAR4CpjTGYJZcry9+Co+Iq2Ow0rYb9l+X93pP7AVmNMcnELnfn9lYuzW6sd8cDq1bId\nqzfBP2zvTcL6gwfwwapS2AGsAFpUYWyXYFURrAfW2h5XAHcBd9nK3AtswuoB8QdwcRXG18K233W2\nGAq/v6LxCTDN9v1uABKq+Pfrh3VgDyrynlO/P6ykdBDIxaqnvhWr3WnB/7d3byFWlWEYx/+P0uEi\nSKIumsCGpsSiw5RjEQRpRFdBR5EQxIyIDhMEXcxVRIFYIgRREFlK5UUSQV501EaRoMZSHK20xKyL\nLoqKTuCUzdvF+61a7vZoe5Rp53p+IDN77bW+9c3Mcr3rsNfzAZ8DG4DTyrwDwKraskvLtrgXuH0K\n+7eXvL5ebYfVJ+l6gNcPtz1MUf9eLNvXKLlzP7O1f+X1P/6/T0X/yvQ11XZXm3fKf39H+88RE2Zm\nDXc8XhoyM7MOuBCYmTWcC4GZWcO5EJiZNZwLgZlZw7kQWFeRFJJW1l4/KOnhY9T2Gkm3Hou2jrCe\nBZI+lTTcMr1H0ivl+/6J0jQnuc4Zku5pty6zI3EhsG4zBtz8Hz0JPKHyBOm/dQdwZ0TMr0+MiK8j\noipE/eTn349VH2aQqbrt1mV2WC4E1m0OkuO+PtD6RusRvaRfytd5kjZLek3SPknLJS2SNFJy4Ptq\nzVwr6UNJn0m6viw/XZnNv7UEnN1Va3eLpPXAJ236c1tpf5ekx8q0h8iHBp+TtKJl/t4y74nkA44L\nS0b9wvIE6vOlz9sl3VCWWSJpvaR3gY2STpG0UdK2su4qaXM50FfaW1Gtq7RxsqTVZf7tkubX2n5V\n0pvKsRAe7/ivZceFTo5yzKbKU8BohzumS4DzyajgfeSTu5crB/4ZBKrBa3rJrJc+YFjSucBiMiZj\nrqSTgPckvV3mvwy4MCK+qK9MUg+Z4T8H+IFMl7wxIh6RdA2Zo992EJKI+K0UjIGIuK+0t4yMOlmq\nHCBmRNKGWh8ujojvy1nBTRHxUzlrer8UqqHSz/7SXm9tlffmauMiSbNLX2eV9/rJBNwxYI+kJyOi\nnuZpDeAzAus6kWmsLwD3d7DY1sixHsbIqIFqR76T3PlX1kXEeGRk8D5gNpn/slg5wtQHZDTEeWX+\nkdYiUMwFNkXEt5FR5mvJwUsm6zpgqPRhExmDMrO8905EVFn4ApZJGiVjK86ifbx13VXASwARsRv4\nEqgKwcaI+DEiDpBnPWcfxc9g/1M+I7Bu9QQZJ726Nu0g5eBF0jRyRKrKWO378drrcQ7dzlszVYLc\nuQ5GxCFhb5LmAb9OrvsdE3BLROxp6cMVLX1YBJwBzImI3yXtJ4vGZNV/b3/gfUIj+YzAulI5Al5H\n3nit7CcvxUCORXDCJJpeIGlauW9wDrCHTPu8WxkPjqRZJSnycEaAqyWdLmk6mYa6uYN+/EwOVVp5\nCxiU/kpNvXSC5U4FvilFYD5/H8G3tle3hSwglEtCM8mf2wxwIbDuthKof3roWXLnuwO4kskdrX9F\n7sTfIFMjDwCryMsi28oN1mc4wpFxZOz2EDBMJkt+FBHtYqYnMgxcUN0sBh4lC9uopI/L63bWAgOS\ndpL3NnaX/nxH3tvY1XqTGngamFaWeRlYUi6hmQE4fdTMrOl8RmBm1nAuBGZmDedCYGbWcC4EZmYN\n50JgZtZwLgRmZg3nQmBm1nB/AkxclL4ymZYpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZgU5bXH8e8RBBUREEYFRFFjNG6g\n4hrjhgugEcElrrgkLve6xETjGhNjErdoTDSJEVfcruIokSi4IRoZWQTFFTcQFGUZZQeREc79460O\nTdMz9DBTXT1dv8/z9NNd1dVVh6Ln1NtvvXXK3B0REUmPdZIOQEREikuJX0QkZZT4RURSRolfRCRl\nlPhFRFJGiV9EJGWU+KWkmNn9ZvaHpOMolJldb2YXNcJ63jOzAxt72TWs53QzGxW9bmlmH5hZRUPX\nK6VPiV8SYWYvm9lcM2sZ4/qXmlmXrHmHmNnUAj9/jZk9tIZlKoABwJ1Z89qa2R1mNtPMlpjZO2Z2\nxpq25+47uvvLhcRWn2UL5e7fAvcClzfmeqU0KfFL0ZlZV+BHgANHxbipxcDVMa7/dGCYu38DYGYt\ngBeBLYF9gDbAr4AbzOyX+VZgZs1jjK++HgFOi+tgLKVDiV+SMAAYA9wPnFbXgmZ2pJlNNLN5Zvaa\nme0Szd/GzOaY2W7RdCczq87pArkNONHMtqll3Z3M7Inoc5+a2YXR/F7AlcBPzGyRmb1VS3i9gVey\npk8FtgCOc/dP3b3G3Z8FLgSuNbONovVPNbPLzOxtYLGZNY/mHRK9v76ZDYp+EU0ys0vNbHpW3NnL\nXmNmg83sATNbGHUD9cha9nIzmxy9976Z9attX7v7dGAusHdty0h5UOKXJAwAHo4eh5vZpvkWMrNd\nCd0P5wDtCV0qQ82spbtPBi4DHjKzDYD7gEE5XSBfAHcBv8uz7nWAfwNvAZ2BnsBFZnZ4lKyvAx5z\n9w3dvVst/46dgQ+zpg8Fhrv74pzlngDWI/wKyDgROAJo6+7f5Sz/W6ArsHW0zlNq2X7GUcCjQFtg\nKPC3rPcmE35dtSHsh4fMrGMd65oE1PbvlTKhxC9FZWb7EbpCBrv7BEJiOqmWxc8G7nT3se6+3N0H\nAd8StUjd/S7gE2As0BG4Ks86rgd+bGY75szfA6hw92vdfZm7TyEcJE6oxz+nLbAwa7oDMCN3oSix\nfxW9n3Gbu3+e6SbKcTxwnbvPjVrht60hjlHuPszdlwMPkpW43f1xd//S3Ve4+2PAx8CedaxrYfTv\nkjKmxC/FdhrwvLt/FU0/Qu3dPVsCF0fdPPPMbB7QBeiUtcxdwE7A7dEJylW4ezWhBXxtnnV3yln3\nlUDeXx+1mAu0zpr+inAAWkXUj98hej/j8zrW2ynn/bqWBZiZ9XoJsF7m3IGZDcjqKptH2Fcd8q0k\n0hqYt4btSRNXSieWpMyZ2fqE1mwzM8skq5ZAWzPr5u65femfA3909z/Wsr4Ngb8A9wDXmNkT7j4n\nz6J/AqYA43LW/am7b1tLuIWUrX0b+D7wejT9InCdmbXK6e45hvBLZUyB658BbA68H013qWPZWpnZ\nloQDY09gtLsvN7OJgNXxsR8At6zN9qTpUItfiuloYDmwA9A9evwAeJXQ75/rLuBcM9vLglZmdoSZ\nZVrZfwXGu/vPgGeAf+bbqLvPIySzS7NmjwMWRidZ1zezZma2k5ntEb0/C+ganQuozTDggKzpB4Hp\nwONm1tXM1jWzwwldNde4+/w61pVtMHCFmbUzs87A+QV+LlcrwgGmGiAaVrpTbQtH29qYVQ9QUoaU\n+KWYTgPuc/fP3H1m5kHoijk5d2iju48Hzoren0vozz8dwMz6Ar2A/4kW/yWwm5mdXMu2/0o46GTW\nvRw4knDw+ZTQDXM34SQowOPR89dm9kYt63wA6BP9ksmMhT+E8GtiLLAA+DNwlbv/qY79kutawgHk\nU8KviErCL4Z6cff3CQe80YQD2c5AVR0fOYlwgrze25KmxXQjFpG1Z2bXAbPd/S8xbuN/gBPc/YA1\nLrz222hJGOG0v7vPjms7UhqU+EVKTDTccmtCS31bQjfW3+I8uEi66OSuSOlpQbhmYSvCCJtHgX8k\nGpGUFbX4RURSRid3RURSJtauHjP7BfAzwpCyd4AzCBe4PEq4BH8CcKq7L6trPR06dPCuXbvGGaqI\nSNmZMGHCV+6+Wqnt2Lp6ojHBo4Ad3P0bMxtMGPfcB3jS3R81s38Cb7n7HXWtq0ePHj5+/PhY4hQR\nKVdmNsHde+TOj7urpzmwfjQ+ewPCFYkHE8YlAwwiXNQjIiJFElvid/cvgJuBzwgJfz6ha2deVjXC\n6YTKiCIiUiSxJX4zawf0JQxJ60S4fLxXPT5/tpmNN7Px1dXVMUUpIpI+cXb1HEIoglXt7jXAk8AP\nCQW5MieVNyfUTF+Nuw909x7u3qOiQrcBFRFpLHEm/s+Avc1sAzMzQoXA94GRwLHRMqcBT8UYg4iI\n5Iizj38s4STuG4ShnOsAAwl3TfqlmX1CGNJ5T1wxiIjI6mIdx+/uvyXcRi7bFOq+A5CIiMRIV+6K\nSDwmTYKBA2GebuhVapT4RaRxjR4NRx8NO+wA55wDO+0Ew4cnHZVkUeIXkYZzh2HDYP/9Yd994T//\ngauvhhdegLZtoU8f+OlP1fovEUr8IrL2amrgoYegWzc44giYOhVuvRU++wyuvRYOOQQmTIArr4RB\ng9T6LxFK/CJSf4sXw+23w/e+B6eeCsuXh8Q+eTJcdBFsuOHKZVu2hD/+EcaMUeu/RCjxi0jhvv4a\nfvc72HJLuPBC6NIFhg6Fd96BAQNg3XVr/2yPHmr9lwjdgUuknLjD55+HE6yjR8OHH0JFBXTuvPpj\ns82gWbPC1jttGvz5z3D33bBkCfz4x3DZZfDDH9Yvvkzrv18/OP300Po/80y45Zbwa0CKQolfpClb\nujS0ojOJfvRomDEjvLf++rD99vD++/Dll/Ddd6t+dp11QvLPPSBsvvnK10uWhD77Rx4BMzjpJLj0\nUthxx4bFnWn9X3st3HgjPPcc3HUX9O7dsPWWk7lz4YMPwuioNm0addVN4taLqscvTY47vPwyjBwZ\nWrIVFbDJJisfFRXQokX915ndmh89Gt58M5xgBdh6a9hnn5WPnXde2fWyYgVUV8MXX6z+mD595ev5\n81ffbqtWcNZZ8ItfwBZbNGi35DV+fGj9v/de+lr/7mH/f/BBuO5h0qSVr2fNCssMG7bWB8Ta6vEr\n8Ys0pq++gvvvDxcuffxx3cu2abPyIJB9QMh+3bw5vP56/tb8HnusTPJ77w2bbtrw+BcvXvWgsGQJ\nHHMMtG/f8HXX5dtvV7b+N9us/Fr/NTXwySerJvbM68WLVy7Xti384Afhsf324XmffdZ6/yvxi8TF\nPYxbv/NOeOIJWLYs9H2fcw4ce2yYnj07tLhnz175yJ7OvK6uDq3zXFtvHZJ7JtHvskvdJ1KbqnJp\n/U+ZEk5cv/RS+LdMnrxqV1uXLisTe3aS32ST0KXWSJT4RRrbnDlhdMrAgaHl1qZNGNly9tlhxMra\nWLEirDdzIPjmG9h118ZpzTcV2a3/TTYJ+7Rfv/ALZ50SHYi4dGk4+A8bFhL+Rx+F+VttFf7/spP8\ndtutOtw1Rkr8Io3BHaqqQuv+8cdDktp779C6P/542GCDpCMsH+PHh6GfI0eG1nKnTqEURP/+4Qrh\npH/xZFr1w4eHGJcsgfXWgwMPDN1UvXvDttsmGqISv0hDzJ0LDz4YEv7778NGG8Epp4SEv8suSUdX\n3ubOhaefhiFD4Nlnw6+gdu3CkNJ+/eCww4pzwM206ocPDy37TKt+m21Cku/TBw44oKQO/kr8IvXl\nHq42vfNOeOyx8Ie/xx4h2Z9wQhjtIsW1ZEkY+jlkCPz73+Hq3w02gF69wkHgyCMb75xATU24fuH5\n50Oiz7TqW7aEgw4qmVZ9XZT4Repj4kS4+OJwcm7DDeHkk0PC33XXpCOTjJoaeOUVePJJ+Ne/woin\n5s3h4IPDQaBvX+jYceXyy5eHK49rO7Ge+zq7pESmVd+7d+jKKaFWfV2U+EUK8eWX8OtfhyGZG28c\nKkyeeSa0bp10ZFKXFStg3LhwEBgyJAydNAvdcDU1IZF//XX4FZfLDDp0yD+ctmPHkOhLuFVfFyV+\nkbosXgw33ww33RROJF54IVx1VdMcSph27mEI5ZAhoU9+o43qvl5i440LL13RxNSW+FWyQdJtxQp4\n4IGQ5L/8Eo47Dm64IYybl6bJLAynXdshtSlQooNiRYpg5MhQM+aMM0J9mlGjYPBgJX0pe0r8kj4f\nfhhO/B18cOj3feSRUA6hvpUmRZooJX5Jj6+/Dn33O+0UWvvXXx+uuD3xxNK9IlQkBurjl/L37bfw\nt7/BH/4ACxaESpO/+126yiCIZFHil/LlHob3XXppuLy+V68wcqehteRFmjj9vpXy9fvfh+qY668f\nLvUfPlxJXwS1+KVcjRkTKjyedFKooNlcX3WRDLX4pfwsWhQKqG2+OfzjH0r6Ijn0FyHl55e/DH36\nL7/c6PcqFSkHavFLeRk6NNy271e/CjXbRWQ1SvxSPmbNgp/9DLp1C/37IpKXunqkPLiHpL9gQbg4\nq2XLpCMSKVlK/FIeBg4Md2n6y180ZFNkDdTVI03fRx+FE7qHHAIXXJB0NCIlT4lfmraaGjj11NC1\nc//9qrkjUoDYunrMbDvgsaxZWwO/AdoCZwHV0fwr3X1YXHFImfvjH8Odlx57DDp3TjoakSYhtsTv\n7h8C3QHMrBnwBTAEOAO41d1vjmvbkhJjx4bCa6ecAscfn3Q0Ik1GsX4X9wQmu/u0Im1Pyl3m6tzO\nnUPlTREpWLES/wnA/2VNn29mb5vZvWbWLt8HzOxsMxtvZuOrq6vzLSJpdvHFMHkyPPigrs4VqafY\nE7+ZtQCOAh6PZt0BbEPoBpoB3JLvc+4+0N17uHuPioqKuMOUpmTo0DB8U1fniqyVYrT4ewNvuPss\nAHef5e7L3X0FcBewZxFikHKhq3NFGqwYF3CdSFY3j5l1dPcZ0WQ/4N0ixCDlQFfnijSKWBO/mbUC\nDgXOyZp9k5l1BxyYmvOelJLFi8ONyXfbLelIgrvuClfn3nqrrs4VaYBYu3rcfbG7t3f3+VnzTnX3\nnd19F3c/Kqv1L6Vk6dJwq8Ldd4fTToN585KN5+OP4Re/gJ49ww3TRWSt6TJHWd2KFXDGGTBqFJx4\nIjz8MOy0Ezz3XDLx1NSEoZu6OlekUegvSFZ39dXw6KNw/fXwyCPhNoZt2oRfAOecAwsXFjeezNW5\n//xnuKuWiDSIEr+s6p574Lrr4Kyz4LLLwrwePWDCBLj00tDPvssu4e5WxaCrc0UanRK/rPT886FF\nf/jh8Pe/g9nK99ZbD268MXT/NG8OBx0EP/85LFkSXzwLFujqXJEYKPFL8M47cOyxYbTM4MGw7rr5\nl9t3X3jrrXCC9bbboHt3eO21xotj+XJ48UU4/fSQ8CdPhgce0NW5Io1IiV/gyy+hTx9o3RqeeQY2\n2qju5TfYAP76V3jpJVi2DH70o9AttHTp2m3fHSZOhEsugS5d4NBDYcgQ+MlPwkHlgAPWbr0ikpfu\nwJV2CxfCEUeE4Zqvvlq/k6cHHRR+KVx8Mdx0Uxhj/8ADYQhoIT7/PIwYeugheO+98CujT5/QvXPk\nkaF7SUQanVr8afbdd3DCCSF5Dx4cum3qq3XrUDdn+HCYPx/22gt++9vwSyCfefPCCeSDDoIttoAr\nrgjdOHfcATNmwL/+FbqclPRFYqPEn1bu4TaFw4aFE7m9ezdsfb16hQPISSeFGjp77x2mIRwEnnoK\njjsONtsslF348suw3OTJUFUF554L7ds3/N8lImukrp60uuWWMC7+0kvDSJ7G0K5d6Orp3z+sc/fd\noW/fcC5gzhyoqAjzTzklDBHNHjUkIkWjxJ9Gjz8eShofd1y4SKuxHX00/PCHcP758Oyz4RzCqaeG\nm6HXNlpIRIpGiT9tXnstJOF994VBg+Irf1BREe6DKyIlR338aTJ5cuh66dIl9Lmvv37SEYlIApT4\n0+Lrr8NQSfdwQrdDh6QjEpGEqKsnDZYuDf3u06bBiBGw7bZJRyQiCVLiL3fZJZYffTScdBWRVFNX\nT7nLlFi+4YZQAkFEUk+Jv5zdfvvKEsuXXpp0NCJSIpT4y9X114cKmn37rl5iWURSTYm/3LjD5ZfD\nlVfCySeHi7V00ZSIZNHJ3XKyYgWcd14oxXDuuaGlr/vTikgOZYVyUVMDAwasrL/zj38o6YtIXmrx\nl4OlS0N55aeeCidzr7gi6YhEpIQp8Td1ixaFi7NGjAj3pT3vvKQjEpESp8TflM2dG8owjBsXCq4N\nGJB0RCLSBCjxN1WzZsFhh8EHH0BlJfTrl3REItJEKPE3RZ99Fm5IPn16uM/toYcmHZGINCFK/E3N\nRx+FG5osWADPP6/aOyJSb0r8Tcnbb4fW/YoVMHIk7Lpr0hGJSBOkgd5NxZgxcMAB4SrcV19V0heR\ntabE3xSMGBG6d9q3D+WVt98+6YhEpAlT4i91Q4eGm5VvtVVo6XftmnREItLEKfGXsueeg/79YZdd\n4OWXoWPHpCMSkTIQW+I3s+3MbGLWY4GZXWRmG5vZC2b2cfTcLq4Ymrx774VNNgldPe3bJx2NiJSJ\n2BK/u3/o7t3dvTuwO7AEGAJcDoxw922BEdG05HKHqqpwQrd166SjEZEyUqyunp7AZHefBvQFBkXz\nBwFHFymGpuWzz+CLLzROX0QaXbES/wnA/0WvN3X3GdHrmcCm+T5gZmeb2XgzG19dXV2MGEvLqFHh\neb/9ko1DRMrOGhO/mV3QkH54M2sBHAU8nvueuzvg+T7n7gPdvYe796ioqFjbzTddVVWhi2fnnZOO\nRETKTCEt/k2B181ssJn1Mqv3zVt7A2+4+6xoepaZdQSInmfXc33pUFUFe+8NzZolHYmIlJk1Jn53\n/zWwLXAPcDrwsZldZ2bbFLiNE1nZzQMwFDgten0a8FTB0abF/Pnwzjvq3xeRWBTUxx91ycyMHt8B\n7YBKM7uprs+ZWSvgUODJrNk3AIea2cfAIdG0ZBszJozqUeIXkRissUibmf0cGAB8BdwN/Mrda8xs\nHeBj4NLaPuvui4H2OfO+JozykdpUVYX75e61V9KRiEgZKqQ658ZA/2go5n+5+wozOzKesFJu1Cjo\n1k3j90UkFoV09QwH5mQmzGwjM9sLwN0nxRVYatXUwNixGsYpIrEpJPHfASzKml4UzZM4vPUWLFmi\n/n0RiU0hid+ik7tA6OJBN3CJT1VVeFbiF5GYFJL4p5jZhWa2bvT4OTAl7sBSq6oKttgCNt886UhE\npEwVkvjPBfYFvgCmA3sBZ8cZVGplCrOptS8iMVpjl427zybU2pG4TZsGX36pxC8isSpkHP96wE+B\nHYH1MvPd/cwY40qnTGE2JX4RiVEhXT0PApsBhwOvAJsDC+MMKrVUmE1EiqCQxP89d78aWOzug4Aj\nCP380tiqqmCffVSYTURiVUjir4me55nZTkAbYJP4QkqpefPg3XfVzSMisStkPP7AqB7/rwmVNTcE\nro41qjRSYTYRKZI6E39UiG2Bu88F/gNsXZSo0qiqKnTxqDCbiMSszq6e6CrdWqtvSiOqqgqF2Tbc\nMOlIRKTMFdLH/6KZXWJmXcxs48wj9sjSpKYmdPWoMJuIFEEhffw/iZ7Py5rnqNun8UycCN98o/59\nESmKQq7c3aoYgaSaCrOJSBEVcuXugHzz3f2Bxg8npaqqYMstoXPnpCMRkRQopKtnj6zX6xFum/gG\noMTfGDKF2Q46KOlIRCQlCunquSB72szaAo/GFlHaTJ0KM2aom0dEiqaQUT25FgPq928s6t8XkSIr\npI//34RRPBAOFDsAg+MMKlVGjYKNNoKddko6EhFJiUL6+G/Oev0dMM3dp8cUT/qoMJuIFFkhif8z\nYIa7LwUws/XNrKu7T401sjSYNw/eew+OPz7pSEQkRQrp438cWJE1vTyaJw01erQKs4lI0RWS+Ju7\n+7LMRPS6RXwhpYgKs4lIAgpJ/NVmdlRmwsz6Al/FF1KKVFVB9+7QqlXSkYhIihTSx38u8LCZ/S2a\nng7kvZpX6qGmBsaOhbPOSjoSEUmZQi7gmgzsbWYbRtOLYo8qDd58MxRmU0VOESmyNXb1mNl1ZtbW\n3Re5+yIza2dmfyhGcGVNF26JSEIK6ePv7e7zMhPR3bj6xBdSSlRVQdeu0KlT0pGISMoUkvibmVnL\nzISZrQ+0rGN5WZNMYTa19kUkAYWc3H0YGGFm9wEGnA4MijOosvfppzBzphK/iCSikJO7N5rZW8Ah\nhJo9zwFbxh1YWVP/vogkqNDqnLMISf844GBgUiEfMrO2ZlZpZh+Y2SQz28fMrjGzL8xsYvRI3/mC\nqipo0wZ23DHpSEQkhWpt8ZvZ94ETo8dXwGOAuXt97hjyV+BZdz/WzFoAGwCHA7e6+811f7SMjRql\nwmwikpi6WvwfEFr3R7r7fu5+O6FOT0HMrA2wP3APhFIP2aODUmvu3FCYTd08IpKQuhJ/f2AGMNLM\n7jKznoSTu4XaCqgG7jOzN83sbjPL1CY438zeNrN7zaxdvg+b2dlmNt7MxldXV9djsyVu9OjwrMQv\nIgmpNfG7+7/c/QRge2AkcBGwiZndYWaHFbDu5sBuwB3uvivhzl2XA3cA2wDdCQeWW2rZ/kB37+Hu\nPSoqKurzbyptmcJse+6ZdCQiklJrPLnr7ovd/RF3/zGwOfAmcFkB654OTHf3sdF0JbCbu89y9+Xu\nvgK4C0hXBqyqgl13VWE2EUlMve656+5zo5Z4zwKWnQl8bmbbRbN6Au+bWcesxfoB79YnhiZt2TIY\nN07dPCKSqEIu4GqICwiVPVsAU4AzgNvMrDtheOhU4JyYYygdKswmIiUg1sTv7hOBHjmzT41zmyVN\nF26JSAmoV1ePNFBVFWy1FXTsuOZlRURiosRfLCrMJiIlQom/WKZMgVmzlPhFJHFK/MWi/n0RKRFK\n/MWiwmwiUiKU+Itl1CjYd19YR7tcRJKlLFQMc+bA+++rm0dESoISfzGoMJuIlBAl/mKoqoLmzVWY\nTURKghJ/MWQKs22wQdKRiIgo8cdOhdlEpMQo8cftzTdh6VIlfhEpGUr8cRs1Kjwr8YtIiVDij1tV\nFWy9tQqziUjJUOKPkwqziUgJUuKP0+TJMHu2Er+IlBQl/rgsXw5DhoTXSvwiUkLivvViusydC889\nB08/DcOHh1IN224LO+yQdGQiIv+lxN8Q7vDhhyHRP/10GMGzfDl06ABHHhkehx+uwmwiUlKU+Ovr\n22/hP/8Jif6ZZ0I/PkC3bnDZZSHZ77knNGuWbJwiIrVQ4i/ErFkwbFhI9s8/D4sWwXrrQc+ecMkl\n0KcPbLFF0lGKiBREib8uY8fCBRfA66+H6c6d4eSTQ6v+4INVe0dEmiQl/rpcdRVMnQq//31I9t26\ngVnSUYmINIgSf22qq+Hll+GKK+DXv046GhGRRqPhJrV56qkwQufYY5OORESkUSnx16ayEr73Pdhl\nl6QjERFpVEr8+cyZAyNGhNa++vRFpMwo8eczdCh89526eUSkLCnx51NZCV27wm67JR2JiEijU+LP\nNX9+uEhL3TwiUqaU+HP9+99QU6NuHhEpW0r8uSoroUuXUG9HRKQMKfFnW7gQnn0WjjlG3TwiUrZi\nTfxm1tbMKs3sAzObZGb7mNnGZvaCmX0cPbeLM4Z6eeaZUH1T3TwiUsbibvH/FXjW3bcHugGTgMuB\nEe6+LTAimi4NlZXhpuj77JN0JCIisYkt8ZtZG2B/4B4Ad1/m7vOAvsCgaLFBwNFxxVAvixeH0svH\nHKMbp4hIWYszw20FVAP3mdmbZna3mbUCNnX3GdEyM4FN833YzM42s/FmNr66ujrGMCPDh8M336ib\nR0TKXpyJvzmwG3CHu+8KLCanW8fdHfB8H3b3ge7ew917VFRUxBhmpLISNtkE9tsv/m2JiCQozsQ/\nHZju7mOj6UrCgWCWmXUEiJ5nxxhDYb75Jtxdq39/3TJRRMpebInf3WcCn5vZdtGsnsD7wFDgtGje\nacBTccVQsOeeC338xxyTdCQiIrGL+0YsFwAPm1kLYApwBuFgM9jMfgpMA46POYY1q6yE9u3hgAOS\njkREJHaxJn53nwj0yPNWzzi3Wy/ffhuqcR5/PKy7btLRiIjETuMWX3ghXLGr0TwikhJK/JWV0LYt\nHHxw0pGIiBRFuhP/smXh3rp9+0KLFklHIyJSFOlO/C+9BPPmqZtHRFIl3Ym/shJat4ZDD006EhGR\noklv4q+pgSFD4KijoGXLpKMRESma9Cb+V16BOXPUzSMiqZPexF9ZCa1aweGHJx2JiEhRpTPxL18O\nTz4JRx4J66+fdDQiIkWVzsT/6qtQXa1uHhFJpXQm/srK0NLv3TvpSEREii59iX/FCnjiCejTJ/Tx\ni4ikTPoS/2uvwcyZ6uYRkdRKX+KvrAzj9o84IulIREQSka7En+nm6dUrXLErIpJC6Ur848bB9Onq\n5hGRVEtX4q+sDDdb+fGPk45ERCQx6Un87iHxH3YYtGmTdDQiIolJT+KfMAGmTVM3j4ikXnoSf2Ul\nNG8eqnGKiKRYOhJ/ppunZ0/YeOOkoxERSVQ6Ev9bb8HkyermEREhLYm/shKaNYOjj046EhGRxJV/\n4neHxx+HAw+EDh2SjkZEJHHln/jfew8++kjdPCIikfJP/JWVYAb9+iUdiYhISUhH4t9/f9h006Qj\nEREpCeWd+CdNCl096uYREfmv8k78TzwRnvv3TzYOEZESUt6Jv1MnOPPM8CwiIgA0TzqAWJ15ZniI\niMh/lXeLX0REVqPELyKSMrEmfjObambvmNlEMxsfzbvGzL6I5k00sz5xxiAiIqsqRh//Qe7+Vc68\nW9395iJsW0REcqirR0QkZeJO/A48b2YTzOzsrPnnm9nbZnavmbWLOQYREckSd+Lfz913A3oD55nZ\n/sAdwDZAd2AGcEu+D5rZ2WY23szGV1dXxxymiEh6xJr43f2L6Hk2MATY091nuftyd18B3AXsWctn\nB7p7D3fvUVFREWeYIiKpEqY9aU8AAAjESURBVNvJXTNrBazj7guj14cB15pZR3efES3WD3h3Teua\nMGHCV2Y2bS1D6QDknlwuJYqvYRRfwyi+hivlGLfMNzPOUT2bAkPMLLOdR9z9WTN70My6E/r/pwLn\nrGlF7r7WTX4zG+/uPdb283FTfA2j+BpG8TVcU4gxV2yJ392nAN3yzD81rm2KiMiaaTiniEjKpCHx\nD0w6gDVQfA2j+BpG8TVcU4hxFebuSccgIiJFlIYWv4iIZFHiFxFJmbJJ/GbWy8w+NLNPzOzyPO+3\nNLPHovfHmlnXIsbWxcxGmtn7Zvaemf08zzIHmtn8rKqlvylWfNH2V6ukmvO+mdlt0f5728x2K2Js\n22Xtl4lmtsDMLspZpqj7Lyo3MtvM3s2at7GZvWBmH0fPecuRmNlp0TIfm9lpRYzvT2b2QfT/N8TM\n2tby2Tq/CzHGV1Dl3jX9rccY32NZsU01s4m1fDb2/ddg7t7kH0AzYDKwNdACeAvYIWeZ/wX+Gb0+\nAXisiPF1BHaLXrcGPsoT34HA0wnuw6lAhzre7wMMBwzYGxib4P/1TGDLJPcfsD+wG/Bu1rybgMuj\n15cDN+b53MbAlOi5XfS6XZHiOwxoHr2+MV98hXwXYozvGuCSAv7/6/xbjyu+nPdvAX6T1P5r6KNc\nWvx7Ap+4+xR3XwY8CvTNWaYvMCh6XQn0tOjqsri5+wx3fyN6vRCYBHQuxrYbUV/gAQ/GAG3NrGMC\ncfQEJrv72l7J3Sjc/T/AnJzZ2d+xQcDReT56OPCCu89x97nAC0CvYsTn7s+7+3fR5Bhg88bebqFq\n2X+FKORvvcHqii/KG8cD/9fY2y2Wckn8nYHPs6ans3pi/e8y0Zd/PtC+KNFlibqYdgXG5nl7HzN7\ny8yGm9mORQ2s9kqqGYXs42I4gdr/4JLcfwCb+spyJDMJV6/nKpX9eCbhF1w+a/ouxGlNlXtLYf/9\nCJjl7h/X8n6S+68g5ZL4mwQz2xB4ArjI3RfkvP0GofuiG3A78K8ih5evkmpJMbMWwFHA43neTnr/\nrcLDb/6SHCttZlcB3wEP17JIUt+Fgir3loATqbu1X/J/S+WS+L8AumRNbx7Ny7uMmTUH2gBfFyW6\nsM11CUn/YXd/Mvd9d1/g7oui18OAdc2sQ7Hi8zyVVHMWKWQfx6038Ia7z8p9I+n9F5mV6f6Knmfn\nWSbR/WhmpwNHAidHB6fVFPBdiIUXVrk36f3XHOgPPFbbMkntv/ool8T/OrCtmW0VtQpPAIbmLDMU\nyIygOBZ4qbYvfmOL+gTvASa5+59rWWazzDkHM9uT8H9TlAOTmbUys9aZ14STgLlVU4cCA6LRPXsD\n87O6NYql1pZWkvsvS/Z37DTgqTzLPAccZmbtoq6Mw6J5sTOzXsClwFHuvqSWZQr5LsQVX/Y5o9oq\n9xbytx6nQ4AP3H16vjeT3H/1kvTZ5cZ6EEadfEQ4439VNO9awpccYD1CF8EnwDhg6yLGth/hZ//b\nwMTo0Qc4Fzg3WuZ84D3CKIUxwL5FjG/raLtvRTFk9l92fAb8Pdq/7wA9ivz/24qQyNtkzUts/xEO\nQDOAGkI/808J54xGAB8DLwIbR8v2AO7O+uyZ0ffwE+CMIsb3CaF/PPMdzIxy6wQMq+u7UKT4Hoy+\nW28TknnH3Pii6dX+1osRXzT//sx3LmvZou+/hj5UskFEJGXKpatHREQKpMQvIpIySvwiIimjxC8i\nkjJK/CIiKaPEL4kyMzezW7KmLzGzaxpp3feb2bGNsa41bOc4M5tkZiNz5ncys8rodffaqk2u5Tbb\nmtn/5tuWyJoo8UvSvgX6J3CVbZ2iKzQL9VPgLHc/KHumu3/p7pkDT3fC+PPGiqEtoeJsvm2J1EmJ\nX5L2HeGepb/IfSO3xW5mi6LnA83sFTN7ysymmNkNZnaymY2L6qBvk7WaQ8xsvJl9ZGZHRp9vZqE2\n/etRQbBzstb7qpkNBd7PE8+J0frfNbMbo3m/IVygd4+Z/Sln+a7Rsi0IFxP+JKrR/pPoCs97o5jf\nNLO+0WdON7OhZvYSMMLMNjSzEWb2RrTtTCXKG4BtovX9KbOtaB3rmdl90fJvmtlBWet+0syetXAv\ngJvq/b8lZaE+rRqRuPwdeLueiagb8ANC6dwphCtj97Rwk5sLgMyNWroSaqVsA4w0s+8BAwglJ/Yw\ns5ZAlZk9Hy2/G7CTu3+avTEz60SoYb87MJdQffFod7/WzA4m1JHPe9MNd18WHSB6uPv50fquI5QN\nOdPCDVHGmdmLWTHs4u5zolZ/P3dfEP0qGhMdmC6P4uwera9r1ibPC5v1nc1s+yjW70fvdSdUh/0W\n+NDMbnf37GqXkgJq8UviPFQqfQC4sB4fe93DfQ6+JVy6n0nc7xCSfcZgd1/hoYTuFGB7Qv2UARbu\noDSWUGph22j5cblJP7IH8LK7V3so6/0w4WYda+sw4PIohpcJJUW2iN57wd0zteANuM7M3iaUgehM\n/nLP2fYDHgJw9w+AaUAm8Y9w9/nuvpTwq2bLBvwbpIlSi19KxV8IpZXvy5r3HVHjxMzWIdxxKePb\nrNcrsqZXsOr3OrcmiROS6QXuvkpxNDM7EFi8duHXmwHHuPuHOTHslRPDyUAFsLu715jZVMJBYm1l\n77flKAekklr8UhKiFu5gwonSjKmErhUIdfjXXYtVH2dm60T9/lsDHxKqYf6PhVLZmNn3o0qKdRkH\nHGBmHcysGaFS6Cv1iGMh4babGc8BF5j9t6LorrV8rg0wO0r6B7GyhZ67vmyvEg4YRF08WxD+3SKA\nEr+UlluA7NE9dxGS7VvAPqxda/wzQtIeTqiquBS4m9DN8UZ0QvRO1tDy9VCC+nJgJKHy4gR3z1d2\nuTYjgR0yJ3eB3xMOZG+b2XvRdD4PAz3M7B3CuYkPoni+JpybeDf3pDLwD2Cd6DOPAadHXWIiAKrO\nKSKSNmrxi4ikjBK/iEjKKPGLiKSMEr+ISMoo8YuIpIwSv4hIyijxi4ikzP8DaKaUFqOxrHcAAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 20.120753936350138 seconds\n",
            "Best accuracy: 79.22  Best training loss: 0.04313662648200989  Best validation loss: 0.6944435071945192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "396fa70d-01c1-4af3-b65b-07fa30d6bee0",
        "id": "gNVrk67w8Mi7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "[2.1278011798858643, 1.1381068229675293, 0.9963151812553406, 1.1801337003707886, 0.9036651849746704, 0.6166149973869324, 0.572441577911377, 0.7587618827819824, 0.3146998882293701, 0.562298595905304, 0.6489260792732239, 0.4657740890979767, 0.3616662919521332, 0.29444873332977295, 0.04313662648200989, 0.3660165071487427, 0.07367490977048874, 0.09971118718385696, 0.1407468020915985, 0.20423555374145508]\n",
            "[1.316491122245789, 1.073440764546394, 1.1976761949062353, 0.8420078158378599, 0.8465766867995264, 0.8003313493728638, 0.8711158382892606, 0.8683207330107691, 0.7482532548904418, 0.6944435071945192, 0.7000897952914238, 0.7612647596001625, 0.7454007494449618, 0.7868634825944898, 0.8455694338679317, 0.9166489326953885, 0.9717262905836105, 0.9625885799527166, 0.9579475793242452, 1.0206879660487171]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFC4-mQZMse",
        "colab_type": "text"
      },
      "source": [
        "## squeeze net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2C4MQMcQbAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaz9jOeSZOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCt69lxiZXZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT6xPT7cZZPY",
        "colab_type": "code",
        "outputId": "bf2d8960-8858-445d-90c3-98fb6ab1eda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.325723\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.302612\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.302475\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.300115\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.286658\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.299698\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.302803\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.302632\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 2.302368\n",
            "\n",
            "Test set: Test loss: 2.3017, Accuracy: 775/5000 (16%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 15.5%\n",
            "Better loss at Epoch 0: loss = 2.3017338275909416%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 2.324977\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 2.300792\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 2.294339\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 2.283263\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 2.296012\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 2.302588\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 2.302586\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 2.302587\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 2.302582\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 2.325611\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 2.302585\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 2.302587\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 2.302586\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 2.302585\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 2.302585\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 2.302553\n",
            "\n",
            "Test set: Test loss: 2.2734, Accuracy: 709/5000 (14%)\n",
            "\n",
            "Better loss at Epoch 2: loss = 2.273392198085785%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 2.316496\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 2.281405\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 2.273034\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 2.268623\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 2.236400\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 2.233911\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 2.228043\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 2.227187\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 2.211815\n",
            "\n",
            "Test set: Test loss: 2.1982, Accuracy: 971/5000 (19%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 19.42%\n",
            "Better loss at Epoch 3: loss = 2.198218114376069%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 2.217087\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 2.155357\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 2.146219\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 2.078742\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 2.001732\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.970168\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.993362\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.929737\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.905587\n",
            "\n",
            "Test set: Test loss: 1.8969, Accuracy: 1562/5000 (31%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 31.24%\n",
            "Better loss at Epoch 4: loss = 1.896902248859406%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.869052\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 1.881193\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.870297\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.868508\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.844030\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 1.852849\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 1.839055\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.816667\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 1.837737\n",
            "\n",
            "Test set: Test loss: 1.7971, Accuracy: 1886/5000 (38%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 37.72%\n",
            "Better loss at Epoch 5: loss = 1.7970805597305304%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 1.801492\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 1.757818\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 1.762176\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 1.763031\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 1.772069\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 1.774624\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 1.737019\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 1.772264\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 1.749656\n",
            "\n",
            "Test set: Test loss: 1.7091, Accuracy: 1994/5000 (40%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 39.88%\n",
            "Better loss at Epoch 6: loss = 1.7090829503536222%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 1.733733\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 1.735876\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 1.706494\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 1.714265\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 1.726371\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 1.679473\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 1.707581\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 1.732523\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 1.669520\n",
            "\n",
            "Test set: Test loss: 1.6736, Accuracy: 2071/5000 (41%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 41.42%\n",
            "Better loss at Epoch 7: loss = 1.6736260128021239%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 1.692945\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 1.700439\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 1.682823\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 1.666204\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 1.644072\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 1.652510\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 1.660393\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 1.654475\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 1.666072\n",
            "\n",
            "Test set: Test loss: 1.6153, Accuracy: 2186/5000 (44%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 43.72%\n",
            "Better loss at Epoch 8: loss = 1.6153090381622315%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 1.614443\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 1.617892\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 1.605730\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 1.649332\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 1.644542\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 1.622847\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 1.614851\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 1.624717\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 1.637401\n",
            "\n",
            "Test set: Test loss: 1.6548, Accuracy: 2104/5000 (42%)\n",
            "\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 1.592470\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 1.599856\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 1.606573\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 1.575769\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 1.584371\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 1.608459\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 1.595571\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 1.590367\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 1.585361\n",
            "\n",
            "Test set: Test loss: 1.5930, Accuracy: 2204/5000 (44%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 44.08%\n",
            "Better loss at Epoch 10: loss = 1.5930318760871887%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 1.551318\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 1.571719\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 1.537206\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 1.568641\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 1.518834\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 1.559902\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 1.591186\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 1.544449\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 1.560886\n",
            "\n",
            "Test set: Test loss: 1.5475, Accuracy: 2308/5000 (46%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 46.16%\n",
            "Better loss at Epoch 11: loss = 1.5475415992736812%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 1.532136\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 1.510316\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 1.523691\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 1.530485\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 1.518530\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 1.554937\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 1.514593\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 1.520366\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 1.528762\n",
            "\n",
            "Test set: Test loss: 1.5633, Accuracy: 2264/5000 (45%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 1.523774\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 1.510164\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 1.506397\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 1.503423\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 1.510160\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 1.471653\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 1.469337\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 1.498701\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 1.503899\n",
            "\n",
            "Test set: Test loss: 1.5416, Accuracy: 2344/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 46.88%\n",
            "Better loss at Epoch 13: loss = 1.5415827131271365%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 1.476331\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 1.487592\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 1.444339\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 1.490611\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 1.478302\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 1.507648\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 1.488622\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 1.469627\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 1.483431\n",
            "\n",
            "Test set: Test loss: 1.5022, Accuracy: 2331/5000 (47%)\n",
            "\n",
            "Better loss at Epoch 14: loss = 1.5022047281265252%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 1.456511\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 1.475792\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 1.432077\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 1.478995\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 1.466645\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 1.486218\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 1.464455\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 1.418781\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 1.442920\n",
            "\n",
            "Test set: Test loss: 1.4891, Accuracy: 2366/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 47.32%\n",
            "Better loss at Epoch 15: loss = 1.4890630710124966%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 1.461142\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 1.435899\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 1.418393\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 1.425529\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 1.425806\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 1.431818\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 1.411001\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 1.441699\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 1.412546\n",
            "\n",
            "Test set: Test loss: 1.4915, Accuracy: 2395/5000 (48%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 47.9%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 1.421916\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 1.402082\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 1.427792\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 1.423746\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 1.415229\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 1.424970\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 1.415739\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 1.405583\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 1.417086\n",
            "\n",
            "Test set: Test loss: 1.5002, Accuracy: 2381/5000 (48%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 1.415697\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 1.413889\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 1.357966\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 1.420519\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 1.367742\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 1.388457\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 1.405183\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 1.396484\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 1.391442\n",
            "\n",
            "Test set: Test loss: 1.4589, Accuracy: 2411/5000 (48%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 48.22%\n",
            "Better loss at Epoch 18: loss = 1.458881777524948%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 1.387722\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 1.365901\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 1.380869\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 1.356926\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 1.369712\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 1.376182\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 1.381415\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 1.357387\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 1.383352\n",
            "\n",
            "Test set: Test loss: 1.4200, Accuracy: 2545/5000 (51%)\n",
            "\n",
            "Better accuracy at Epoch 19: accuracy = 50.9%\n",
            "Better loss at Epoch 19: loss = 1.420024849176407%\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 1.372718\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 1.361082\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 1.373874\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 1.362418\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 1.348045\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 1.338271\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 1.384915\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 1.363169\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 1.362758\n",
            "\n",
            "Test set: Test loss: 1.4028, Accuracy: 2554/5000 (51%)\n",
            "\n",
            "Better accuracy at Epoch 20: accuracy = 51.08%\n",
            "Better loss at Epoch 20: loss = 1.402843114733696%\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 1.346572\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 1.328655\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 1.331934\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 1.322276\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 1.356289\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 1.339387\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 1.356696\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 1.353582\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 1.352183\n",
            "\n",
            "Test set: Test loss: 1.4003, Accuracy: 2525/5000 (50%)\n",
            "\n",
            "Better loss at Epoch 21: loss = 1.4003210854530337%\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 1.359233\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 1.326183\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 1.305344\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 1.325762\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 1.328535\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 1.322239\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 1.333824\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 1.353468\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 1.355572\n",
            "\n",
            "Test set: Test loss: 1.3717, Accuracy: 2593/5000 (52%)\n",
            "\n",
            "Better accuracy at Epoch 22: accuracy = 51.86%\n",
            "Better loss at Epoch 22: loss = 1.371710929870605%\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 1.321416\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 1.331050\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 1.311990\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 1.320488\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 1.341534\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 1.302140\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 1.314165\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 1.323009\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 1.334527\n",
            "\n",
            "Test set: Test loss: 1.4192, Accuracy: 2513/5000 (50%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 1.319751\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 1.315172\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 1.302074\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 1.311730\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 1.284893\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 1.301158\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 1.320465\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 1.297200\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 1.290804\n",
            "\n",
            "Test set: Test loss: 1.3868, Accuracy: 2591/5000 (52%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 1.304387\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 1.280438\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 1.256511\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 1.280899\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 1.294398\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 1.272526\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 1.303082\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 1.289219\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 1.327483\n",
            "\n",
            "Test set: Test loss: 1.3734, Accuracy: 2618/5000 (52%)\n",
            "\n",
            "Better accuracy at Epoch 25: accuracy = 52.36%\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 1.279282\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 1.282780\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 1.254164\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 1.277170\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 1.271511\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 1.306240\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 1.303783\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 1.261134\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 1.319738\n",
            "\n",
            "Test set: Test loss: 1.3771, Accuracy: 2576/5000 (52%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 1.271664\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 1.256830\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 1.262151\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 1.207206\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 1.170981\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 1.161799\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 1.194937\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 1.146814\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 1.188437\n",
            "\n",
            "Test set: Test loss: 1.2723, Accuracy: 2860/5000 (57%)\n",
            "\n",
            "Better accuracy at Epoch 27: accuracy = 57.2%\n",
            "Better loss at Epoch 27: loss = 1.2723421645164485%\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 1.149819\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 1.162696\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 1.148453\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 1.138840\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 1.142330\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 1.096640\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 1.144122\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 1.128408\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 1.162468\n",
            "\n",
            "Test set: Test loss: 1.2232, Accuracy: 2919/5000 (58%)\n",
            "\n",
            "Better accuracy at Epoch 28: accuracy = 58.38%\n",
            "Better loss at Epoch 28: loss = 1.2232313036918643%\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 1.137588\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 1.102909\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 1.091077\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 1.085623\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 1.139427\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 1.120987\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 1.007267\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.969413\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.981890\n",
            "\n",
            "Test set: Test loss: 1.0728, Accuracy: 3174/5000 (63%)\n",
            "\n",
            "Better accuracy at Epoch 29: accuracy = 63.48%\n",
            "Better loss at Epoch 29: loss = 1.0728230684995652%\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.919164\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.915691\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.925626\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.931563\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.963627\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.957094\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.906031\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.979322\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.918921\n",
            "\n",
            "Test set: Test loss: 1.0282, Accuracy: 3281/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 30: accuracy = 65.62%\n",
            "Better loss at Epoch 30: loss = 1.0281804096698763%\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.878813\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.905448\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.873979\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.906624\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.935893\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.920056\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.939421\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.910221\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.929810\n",
            "\n",
            "Test set: Test loss: 0.9854, Accuracy: 3336/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 31: accuracy = 66.72%\n",
            "Better loss at Epoch 31: loss = 0.9854114812612532%\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.872974\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.905644\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.868922\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.912134\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.912545\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.898269\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.892352\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.905906\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.903658\n",
            "\n",
            "Test set: Test loss: 1.0049, Accuracy: 3306/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.854705\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.846689\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.859605\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.870659\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.888759\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.891546\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.890436\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.908615\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.890162\n",
            "\n",
            "Test set: Test loss: 1.0023, Accuracy: 3311/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.852399\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.865266\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.844630\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.848009\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.871479\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.880001\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.872601\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.892915\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.855309\n",
            "\n",
            "Test set: Test loss: 1.0161, Accuracy: 3317/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.862447\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.838522\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.860485\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.867741\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.824767\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.866180\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.839476\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.862075\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.847325\n",
            "\n",
            "Test set: Test loss: 0.9441, Accuracy: 3387/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 35: accuracy = 67.74%\n",
            "Better loss at Epoch 35: loss = 0.9440914404392241%\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.829283\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.839425\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.860451\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.846623\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.863144\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.821017\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.880027\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.872763\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.858857\n",
            "\n",
            "Test set: Test loss: 0.9607, Accuracy: 3396/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 36: accuracy = 67.92%\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.826342\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.811689\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.869224\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.831518\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.845162\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.866667\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.869545\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.875004\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.843886\n",
            "\n",
            "Test set: Test loss: 0.9687, Accuracy: 3355/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.845948\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.814120\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.811422\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.857415\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.839829\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.796377\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.826351\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.867716\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.831090\n",
            "\n",
            "Test set: Test loss: 0.9290, Accuracy: 3434/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 38: accuracy = 68.68%\n",
            "Better loss at Epoch 38: loss = 0.9289505648612972%\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.801609\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.797453\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.795580\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.802187\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.821541\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.791709\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.842965\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.844205\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.851855\n",
            "\n",
            "Test set: Test loss: 1.0089, Accuracy: 3361/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.815181\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.807161\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.787835\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.844665\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.810245\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.839938\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.802850\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.840017\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.815522\n",
            "\n",
            "Test set: Test loss: 0.9414, Accuracy: 3413/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.778206\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.817021\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.794049\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.821249\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.777814\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.814247\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.847024\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.821219\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.812533\n",
            "\n",
            "Test set: Test loss: 0.9926, Accuracy: 3339/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.787901\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.799614\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.751109\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.785668\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.807494\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.799535\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.774326\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.829470\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.829852\n",
            "\n",
            "Test set: Test loss: 0.9153, Accuracy: 3460/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 42: accuracy = 69.2%\n",
            "Better loss at Epoch 42: loss = 0.915341284275055%\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.768273\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.791995\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.799090\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.802765\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.795842\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.788634\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.782247\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.809267\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.785326\n",
            "\n",
            "Test set: Test loss: 0.9633, Accuracy: 3430/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.778452\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.765124\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.774729\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.764528\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.820761\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.785125\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.743202\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.818766\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.780397\n",
            "\n",
            "Test set: Test loss: 1.0143, Accuracy: 3370/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.745604\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.752460\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.769891\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.790004\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.795614\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.768010\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.743651\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.750086\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.785074\n",
            "\n",
            "Test set: Test loss: 0.9783, Accuracy: 3431/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.741207\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.718476\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.786496\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.798124\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.775293\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.777704\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.778001\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.787134\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.809833\n",
            "\n",
            "Test set: Test loss: 0.9657, Accuracy: 3410/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.765516\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.712426\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.746557\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.787826\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.792832\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.760240\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.781296\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.737803\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.775388\n",
            "\n",
            "Test set: Test loss: 0.9163, Accuracy: 3483/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 47: accuracy = 69.66%\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.727223\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.755732\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.751696\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.770899\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.760408\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.792942\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.766143\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.781312\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.772661\n",
            "\n",
            "Test set: Test loss: 0.9572, Accuracy: 3480/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.719609\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.741785\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.754724\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.758432\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.721161\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.748355\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.804194\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.755457\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.769588\n",
            "\n",
            "Test set: Test loss: 0.9453, Accuracy: 3495/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 49: accuracy = 69.9%\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.694374\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.777565\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.727630\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.739218\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.757955\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.761658\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.729613\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.731955\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.760928\n",
            "\n",
            "Test set: Test loss: 0.9709, Accuracy: 3492/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.720462\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.735661\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.768785\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.727269\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.731980\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.718786\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.762641\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.736593\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.756002\n",
            "\n",
            "Test set: Test loss: 0.9605, Accuracy: 3463/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.699863\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.724080\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.731219\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.730663\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.741772\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.732462\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.739494\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.729691\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.757589\n",
            "\n",
            "Test set: Test loss: 0.9563, Accuracy: 3518/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 52: accuracy = 70.36%\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.683506\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.715806\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.742970\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.753207\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.725364\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.711239\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.757277\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.731715\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.737004\n",
            "\n",
            "Test set: Test loss: 0.9944, Accuracy: 3411/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.708748\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.716105\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.706443\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.752651\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.762644\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.787111\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.737829\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.745703\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.729327\n",
            "\n",
            "Test set: Test loss: 0.9584, Accuracy: 3379/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.748302\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.737737\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.729254\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.726213\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.722863\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.743243\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.783721\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.758509\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.760909\n",
            "\n",
            "Test set: Test loss: 0.9240, Accuracy: 3482/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.722851\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.733779\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.727694\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.709992\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.724411\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.734503\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.795671\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.748781\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.733843\n",
            "\n",
            "Test set: Test loss: 0.9604, Accuracy: 3452/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.753935\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.720515\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.747307\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.738410\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.746512\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.755393\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.740327\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.760379\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.732510\n",
            "\n",
            "Test set: Test loss: 0.9806, Accuracy: 3426/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.698270\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.720537\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.716637\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.732558\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.727831\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.738020\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.744568\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.735003\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.752214\n",
            "\n",
            "Test set: Test loss: 1.0019, Accuracy: 3473/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.707926\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.716978\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.722827\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.724621\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.726080\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.726323\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.717872\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.722523\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.740358\n",
            "\n",
            "Test set: Test loss: 0.9539, Accuracy: 3485/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.704156\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.689169\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.733279\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.747622\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.719007\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.768370\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.715039\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.741511\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.744358\n",
            "\n",
            "Test set: Test loss: 0.9708, Accuracy: 3494/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.730041\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.737443\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.698891\n",
            "Train Epoch: 61 [20000/50000 (40%)]\tTrain Loss: 0.700555\n",
            "Train Epoch: 61 [25000/50000 (50%)]\tTrain Loss: 0.681284\n",
            "Train Epoch: 61 [30000/50000 (60%)]\tTrain Loss: 0.740833\n",
            "Train Epoch: 61 [35000/50000 (70%)]\tTrain Loss: 0.747646\n",
            "Train Epoch: 61 [40000/50000 (80%)]\tTrain Loss: 0.722648\n",
            "Train Epoch: 61 [45000/50000 (90%)]\tTrain Loss: 0.786747\n",
            "\n",
            "Test set: Test loss: 0.9704, Accuracy: 3475/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 62: lr = 0.1\n",
            "Train Epoch: 62 [5000/50000 (10%)]\tTrain Loss: 0.766534\n",
            "Train Epoch: 62 [10000/50000 (20%)]\tTrain Loss: 0.739814\n",
            "Train Epoch: 62 [15000/50000 (30%)]\tTrain Loss: 0.726777\n",
            "Train Epoch: 62 [20000/50000 (40%)]\tTrain Loss: 0.749035\n",
            "Train Epoch: 62 [25000/50000 (50%)]\tTrain Loss: 0.760898\n",
            "Train Epoch: 62 [30000/50000 (60%)]\tTrain Loss: 0.793859\n",
            "Train Epoch: 62 [35000/50000 (70%)]\tTrain Loss: 0.728874\n",
            "Train Epoch: 62 [40000/50000 (80%)]\tTrain Loss: 0.741093\n",
            "Train Epoch: 62 [45000/50000 (90%)]\tTrain Loss: 0.738604\n",
            "\n",
            "Test set: Test loss: 0.9889, Accuracy: 3396/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 63: lr = 0.1\n",
            "Train Epoch: 63 [5000/50000 (10%)]\tTrain Loss: 0.721504\n",
            "Train Epoch: 63 [10000/50000 (20%)]\tTrain Loss: 0.728122\n",
            "Train Epoch: 63 [15000/50000 (30%)]\tTrain Loss: 0.732076\n",
            "Train Epoch: 63 [20000/50000 (40%)]\tTrain Loss: 0.730635\n",
            "Train Epoch: 63 [25000/50000 (50%)]\tTrain Loss: 0.707698\n",
            "Train Epoch: 63 [30000/50000 (60%)]\tTrain Loss: 0.703202\n",
            "Train Epoch: 63 [35000/50000 (70%)]\tTrain Loss: 0.729225\n",
            "Train Epoch: 63 [40000/50000 (80%)]\tTrain Loss: 0.714360\n",
            "Train Epoch: 63 [45000/50000 (90%)]\tTrain Loss: 0.747804\n",
            "\n",
            "Test set: Test loss: 0.9857, Accuracy: 3435/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 64: lr = 0.1\n",
            "Train Epoch: 64 [5000/50000 (10%)]\tTrain Loss: 0.701497\n",
            "Train Epoch: 64 [10000/50000 (20%)]\tTrain Loss: 0.672654\n",
            "Train Epoch: 64 [15000/50000 (30%)]\tTrain Loss: 0.720712\n",
            "Train Epoch: 64 [20000/50000 (40%)]\tTrain Loss: 0.726437\n",
            "Train Epoch: 64 [25000/50000 (50%)]\tTrain Loss: 0.726751\n",
            "Train Epoch: 64 [30000/50000 (60%)]\tTrain Loss: 0.724530\n",
            "Train Epoch: 64 [35000/50000 (70%)]\tTrain Loss: 0.721769\n",
            "Train Epoch: 64 [40000/50000 (80%)]\tTrain Loss: 0.737413\n",
            "Train Epoch: 64 [45000/50000 (90%)]\tTrain Loss: 0.802765\n",
            "\n",
            "Test set: Test loss: 0.9604, Accuracy: 3454/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 65: lr = 0.1\n",
            "Train Epoch: 65 [5000/50000 (10%)]\tTrain Loss: 0.763409\n",
            "Train Epoch: 65 [10000/50000 (20%)]\tTrain Loss: 0.706163\n",
            "Train Epoch: 65 [15000/50000 (30%)]\tTrain Loss: 0.730282\n",
            "Train Epoch: 65 [20000/50000 (40%)]\tTrain Loss: 0.696664\n",
            "Train Epoch: 65 [25000/50000 (50%)]\tTrain Loss: 0.720151\n",
            "Train Epoch: 65 [30000/50000 (60%)]\tTrain Loss: 0.698792\n",
            "Train Epoch: 65 [35000/50000 (70%)]\tTrain Loss: 0.693887\n",
            "Train Epoch: 65 [40000/50000 (80%)]\tTrain Loss: 0.710074\n",
            "Train Epoch: 65 [45000/50000 (90%)]\tTrain Loss: 0.730149\n",
            "\n",
            "Test set: Test loss: 0.9645, Accuracy: 3460/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 66: lr = 0.1\n",
            "Train Epoch: 66 [5000/50000 (10%)]\tTrain Loss: 0.711968\n",
            "Train Epoch: 66 [10000/50000 (20%)]\tTrain Loss: 0.704967\n",
            "Train Epoch: 66 [15000/50000 (30%)]\tTrain Loss: 0.670422\n",
            "Train Epoch: 66 [20000/50000 (40%)]\tTrain Loss: 0.709816\n",
            "Train Epoch: 66 [25000/50000 (50%)]\tTrain Loss: 0.722406\n",
            "Train Epoch: 66 [30000/50000 (60%)]\tTrain Loss: 0.722786\n",
            "Train Epoch: 66 [35000/50000 (70%)]\tTrain Loss: 0.727969\n",
            "Train Epoch: 66 [40000/50000 (80%)]\tTrain Loss: 0.700134\n",
            "Train Epoch: 66 [45000/50000 (90%)]\tTrain Loss: 0.732337\n",
            "\n",
            "Test set: Test loss: 0.9783, Accuracy: 3458/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 67: lr = 0.1\n",
            "Train Epoch: 67 [5000/50000 (10%)]\tTrain Loss: 0.692158\n",
            "Train Epoch: 67 [10000/50000 (20%)]\tTrain Loss: 0.716394\n",
            "Train Epoch: 67 [15000/50000 (30%)]\tTrain Loss: 0.709070\n",
            "Train Epoch: 67 [20000/50000 (40%)]\tTrain Loss: 0.705537\n",
            "Train Epoch: 67 [25000/50000 (50%)]\tTrain Loss: 0.715131\n",
            "Train Epoch: 67 [30000/50000 (60%)]\tTrain Loss: 0.705787\n",
            "Train Epoch: 67 [35000/50000 (70%)]\tTrain Loss: 0.691200\n",
            "Train Epoch: 67 [40000/50000 (80%)]\tTrain Loss: 0.727757\n",
            "Train Epoch: 67 [45000/50000 (90%)]\tTrain Loss: 0.739102\n",
            "\n",
            "Test set: Test loss: 0.9362, Accuracy: 3504/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 68: lr = 0.1\n",
            "Train Epoch: 68 [5000/50000 (10%)]\tTrain Loss: 0.693988\n",
            "Train Epoch: 68 [10000/50000 (20%)]\tTrain Loss: 0.640422\n",
            "Train Epoch: 68 [15000/50000 (30%)]\tTrain Loss: 0.698338\n",
            "Train Epoch: 68 [20000/50000 (40%)]\tTrain Loss: 0.700024\n",
            "Train Epoch: 68 [25000/50000 (50%)]\tTrain Loss: 0.713027\n",
            "Train Epoch: 68 [30000/50000 (60%)]\tTrain Loss: 0.744522\n",
            "Train Epoch: 68 [35000/50000 (70%)]\tTrain Loss: 0.739407\n",
            "Train Epoch: 68 [40000/50000 (80%)]\tTrain Loss: 0.723462\n",
            "Train Epoch: 68 [45000/50000 (90%)]\tTrain Loss: 0.733942\n",
            "\n",
            "Test set: Test loss: 0.9887, Accuracy: 3489/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 69: lr = 0.1\n",
            "Train Epoch: 69 [5000/50000 (10%)]\tTrain Loss: 0.655504\n",
            "Train Epoch: 69 [10000/50000 (20%)]\tTrain Loss: 0.696596\n",
            "Train Epoch: 69 [15000/50000 (30%)]\tTrain Loss: 0.697200\n",
            "Train Epoch: 69 [20000/50000 (40%)]\tTrain Loss: 0.695767\n",
            "Train Epoch: 69 [25000/50000 (50%)]\tTrain Loss: 0.704714\n",
            "Train Epoch: 69 [30000/50000 (60%)]\tTrain Loss: 0.702573\n",
            "Train Epoch: 69 [35000/50000 (70%)]\tTrain Loss: 0.724446\n",
            "Train Epoch: 69 [40000/50000 (80%)]\tTrain Loss: 0.719011\n",
            "Train Epoch: 69 [45000/50000 (90%)]\tTrain Loss: 0.725115\n",
            "\n",
            "Test set: Test loss: 0.9779, Accuracy: 3489/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 70: lr = 0.1\n",
            "Train Epoch: 70 [5000/50000 (10%)]\tTrain Loss: 0.680052\n",
            "Train Epoch: 70 [10000/50000 (20%)]\tTrain Loss: 0.702411\n",
            "Train Epoch: 70 [15000/50000 (30%)]\tTrain Loss: 0.705863\n",
            "Train Epoch: 70 [20000/50000 (40%)]\tTrain Loss: 0.726456\n",
            "Train Epoch: 70 [25000/50000 (50%)]\tTrain Loss: 0.700716\n",
            "Train Epoch: 70 [30000/50000 (60%)]\tTrain Loss: 0.717645\n",
            "Train Epoch: 70 [35000/50000 (70%)]\tTrain Loss: 0.703842\n",
            "Train Epoch: 70 [40000/50000 (80%)]\tTrain Loss: 0.692670\n",
            "Train Epoch: 70 [45000/50000 (90%)]\tTrain Loss: 0.704561\n",
            "\n",
            "Test set: Test loss: 0.9696, Accuracy: 3445/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 71: lr = 0.1\n",
            "Train Epoch: 71 [5000/50000 (10%)]\tTrain Loss: 0.671235\n",
            "Train Epoch: 71 [10000/50000 (20%)]\tTrain Loss: 0.668502\n",
            "Train Epoch: 71 [15000/50000 (30%)]\tTrain Loss: 0.666831\n",
            "Train Epoch: 71 [20000/50000 (40%)]\tTrain Loss: 0.664355\n",
            "Train Epoch: 71 [25000/50000 (50%)]\tTrain Loss: 0.690310\n",
            "Train Epoch: 71 [30000/50000 (60%)]\tTrain Loss: 0.669916\n",
            "Train Epoch: 71 [35000/50000 (70%)]\tTrain Loss: 0.714663\n",
            "Train Epoch: 71 [40000/50000 (80%)]\tTrain Loss: 0.734069\n",
            "Train Epoch: 71 [45000/50000 (90%)]\tTrain Loss: 0.717165\n",
            "\n",
            "Test set: Test loss: 0.9637, Accuracy: 3493/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 72: lr = 0.1\n",
            "Train Epoch: 72 [5000/50000 (10%)]\tTrain Loss: 0.685240\n",
            "Train Epoch: 72 [10000/50000 (20%)]\tTrain Loss: 0.689702\n",
            "Train Epoch: 72 [15000/50000 (30%)]\tTrain Loss: 0.644188\n",
            "Train Epoch: 72 [20000/50000 (40%)]\tTrain Loss: 0.686859\n",
            "Train Epoch: 72 [25000/50000 (50%)]\tTrain Loss: 0.718154\n",
            "Train Epoch: 72 [30000/50000 (60%)]\tTrain Loss: 0.730956\n",
            "Train Epoch: 72 [35000/50000 (70%)]\tTrain Loss: 0.676753\n",
            "Train Epoch: 72 [40000/50000 (80%)]\tTrain Loss: 0.755549\n",
            "Train Epoch: 72 [45000/50000 (90%)]\tTrain Loss: 0.735313\n",
            "\n",
            "Test set: Test loss: 0.9771, Accuracy: 3405/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 73: lr = 0.1\n",
            "Train Epoch: 73 [5000/50000 (10%)]\tTrain Loss: 0.674046\n",
            "Train Epoch: 73 [10000/50000 (20%)]\tTrain Loss: 0.657161\n",
            "Train Epoch: 73 [15000/50000 (30%)]\tTrain Loss: 0.653185\n",
            "Train Epoch: 73 [20000/50000 (40%)]\tTrain Loss: 0.673835\n",
            "Train Epoch: 73 [25000/50000 (50%)]\tTrain Loss: 0.691388\n",
            "Train Epoch: 73 [30000/50000 (60%)]\tTrain Loss: 0.672878\n",
            "Train Epoch: 73 [35000/50000 (70%)]\tTrain Loss: 0.673095\n",
            "Train Epoch: 73 [40000/50000 (80%)]\tTrain Loss: 0.730033\n",
            "Train Epoch: 73 [45000/50000 (90%)]\tTrain Loss: 0.731204\n",
            "\n",
            "Test set: Test loss: 0.9855, Accuracy: 3458/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 74: lr = 0.1\n",
            "Train Epoch: 74 [5000/50000 (10%)]\tTrain Loss: 0.618299\n",
            "Train Epoch: 74 [10000/50000 (20%)]\tTrain Loss: 0.658769\n",
            "Train Epoch: 74 [15000/50000 (30%)]\tTrain Loss: 0.696387\n",
            "Train Epoch: 74 [20000/50000 (40%)]\tTrain Loss: 0.697688\n",
            "Train Epoch: 74 [25000/50000 (50%)]\tTrain Loss: 0.694926\n",
            "Train Epoch: 74 [30000/50000 (60%)]\tTrain Loss: 0.699094\n",
            "Train Epoch: 74 [35000/50000 (70%)]\tTrain Loss: 0.710812\n",
            "Train Epoch: 74 [40000/50000 (80%)]\tTrain Loss: 0.713814\n",
            "Train Epoch: 74 [45000/50000 (90%)]\tTrain Loss: 0.746801\n",
            "\n",
            "Test set: Test loss: 1.0241, Accuracy: 3415/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 75: lr = 0.1\n",
            "Train Epoch: 75 [5000/50000 (10%)]\tTrain Loss: 0.707884\n",
            "Train Epoch: 75 [10000/50000 (20%)]\tTrain Loss: 0.711865\n",
            "Train Epoch: 75 [15000/50000 (30%)]\tTrain Loss: 0.698865\n",
            "Train Epoch: 75 [20000/50000 (40%)]\tTrain Loss: 0.686934\n",
            "Train Epoch: 75 [25000/50000 (50%)]\tTrain Loss: 0.686529\n",
            "Train Epoch: 75 [30000/50000 (60%)]\tTrain Loss: 0.683528\n",
            "Train Epoch: 75 [35000/50000 (70%)]\tTrain Loss: 0.708073\n",
            "Train Epoch: 75 [40000/50000 (80%)]\tTrain Loss: 0.708539\n",
            "Train Epoch: 75 [45000/50000 (90%)]\tTrain Loss: 0.729911\n",
            "\n",
            "Test set: Test loss: 0.9642, Accuracy: 3454/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 76: lr = 0.1\n",
            "Train Epoch: 76 [5000/50000 (10%)]\tTrain Loss: 0.670987\n",
            "Train Epoch: 76 [10000/50000 (20%)]\tTrain Loss: 0.789020\n",
            "Train Epoch: 76 [15000/50000 (30%)]\tTrain Loss: 0.780101\n",
            "Train Epoch: 76 [20000/50000 (40%)]\tTrain Loss: 0.750602\n",
            "Train Epoch: 76 [25000/50000 (50%)]\tTrain Loss: 0.682446\n",
            "Train Epoch: 76 [30000/50000 (60%)]\tTrain Loss: 0.685079\n",
            "Train Epoch: 76 [35000/50000 (70%)]\tTrain Loss: 0.702045\n",
            "Train Epoch: 76 [40000/50000 (80%)]\tTrain Loss: 0.708853\n",
            "Train Epoch: 76 [45000/50000 (90%)]\tTrain Loss: 0.724507\n",
            "\n",
            "Test set: Test loss: 0.9243, Accuracy: 3523/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 76: accuracy = 70.46%\n",
            "\n",
            "Train Epoch 77: lr = 0.1\n",
            "Train Epoch: 77 [5000/50000 (10%)]\tTrain Loss: 0.692693\n",
            "Train Epoch: 77 [10000/50000 (20%)]\tTrain Loss: 0.691086\n",
            "Train Epoch: 77 [15000/50000 (30%)]\tTrain Loss: 0.715943\n",
            "Train Epoch: 77 [20000/50000 (40%)]\tTrain Loss: 0.789584\n",
            "Train Epoch: 77 [25000/50000 (50%)]\tTrain Loss: 0.760194\n",
            "Train Epoch: 77 [30000/50000 (60%)]\tTrain Loss: 0.741142\n",
            "Train Epoch: 77 [35000/50000 (70%)]\tTrain Loss: 0.735228\n",
            "Train Epoch: 77 [40000/50000 (80%)]\tTrain Loss: 0.765571\n",
            "Train Epoch: 77 [45000/50000 (90%)]\tTrain Loss: 0.729351\n",
            "\n",
            "Test set: Test loss: 0.9245, Accuracy: 3475/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 78: lr = 0.1\n",
            "Train Epoch: 78 [5000/50000 (10%)]\tTrain Loss: 0.712642\n",
            "Train Epoch: 78 [10000/50000 (20%)]\tTrain Loss: 0.725058\n",
            "Train Epoch: 78 [15000/50000 (30%)]\tTrain Loss: 0.745607\n",
            "Train Epoch: 78 [20000/50000 (40%)]\tTrain Loss: 0.751566\n",
            "Train Epoch: 78 [25000/50000 (50%)]\tTrain Loss: 0.715546\n",
            "Train Epoch: 78 [30000/50000 (60%)]\tTrain Loss: 0.728026\n",
            "Train Epoch: 78 [35000/50000 (70%)]\tTrain Loss: 0.784023\n",
            "Train Epoch: 78 [40000/50000 (80%)]\tTrain Loss: 0.805718\n",
            "Train Epoch: 78 [45000/50000 (90%)]\tTrain Loss: 0.743840\n",
            "\n",
            "Test set: Test loss: 1.0043, Accuracy: 3471/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 79: lr = 0.1\n",
            "Train Epoch: 79 [5000/50000 (10%)]\tTrain Loss: 0.711902\n",
            "Train Epoch: 79 [10000/50000 (20%)]\tTrain Loss: 0.785927\n",
            "Train Epoch: 79 [15000/50000 (30%)]\tTrain Loss: 0.761819\n",
            "Train Epoch: 79 [20000/50000 (40%)]\tTrain Loss: 0.726007\n",
            "Train Epoch: 79 [25000/50000 (50%)]\tTrain Loss: 0.712740\n",
            "Train Epoch: 79 [30000/50000 (60%)]\tTrain Loss: 0.758357\n",
            "Train Epoch: 79 [35000/50000 (70%)]\tTrain Loss: 0.738380\n",
            "Train Epoch: 79 [40000/50000 (80%)]\tTrain Loss: 0.805723\n",
            "Train Epoch: 79 [45000/50000 (90%)]\tTrain Loss: 0.749477\n",
            "\n",
            "Test set: Test loss: 1.0444, Accuracy: 3373/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 80: lr = 0.1\n",
            "Train Epoch: 80 [5000/50000 (10%)]\tTrain Loss: 0.728224\n",
            "Train Epoch: 80 [10000/50000 (20%)]\tTrain Loss: 0.712741\n",
            "Train Epoch: 80 [15000/50000 (30%)]\tTrain Loss: 0.667313\n",
            "Train Epoch: 80 [20000/50000 (40%)]\tTrain Loss: 0.762185\n",
            "Train Epoch: 80 [25000/50000 (50%)]\tTrain Loss: 0.750943\n",
            "Train Epoch: 80 [30000/50000 (60%)]\tTrain Loss: 0.766555\n",
            "Train Epoch: 80 [35000/50000 (70%)]\tTrain Loss: 0.764128\n",
            "Train Epoch: 80 [40000/50000 (80%)]\tTrain Loss: 0.757072\n",
            "Train Epoch: 80 [45000/50000 (90%)]\tTrain Loss: 0.753012\n",
            "\n",
            "Test set: Test loss: 0.9806, Accuracy: 3458/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 81: lr = 0.1\n",
            "Train Epoch: 81 [5000/50000 (10%)]\tTrain Loss: 0.747202\n",
            "Train Epoch: 81 [10000/50000 (20%)]\tTrain Loss: 0.802738\n",
            "Train Epoch: 81 [15000/50000 (30%)]\tTrain Loss: 0.752945\n",
            "Train Epoch: 81 [20000/50000 (40%)]\tTrain Loss: 0.767779\n",
            "Train Epoch: 81 [25000/50000 (50%)]\tTrain Loss: 0.735133\n",
            "Train Epoch: 81 [30000/50000 (60%)]\tTrain Loss: 0.788100\n",
            "Train Epoch: 81 [35000/50000 (70%)]\tTrain Loss: 0.769931\n",
            "Train Epoch: 81 [40000/50000 (80%)]\tTrain Loss: 0.759632\n",
            "Train Epoch: 81 [45000/50000 (90%)]\tTrain Loss: 0.713505\n",
            "\n",
            "Test set: Test loss: 1.0024, Accuracy: 3475/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 82: lr = 0.1\n",
            "Train Epoch: 82 [5000/50000 (10%)]\tTrain Loss: 0.720045\n",
            "Train Epoch: 82 [10000/50000 (20%)]\tTrain Loss: 0.788207\n",
            "Train Epoch: 82 [15000/50000 (30%)]\tTrain Loss: 0.747041\n",
            "Train Epoch: 82 [20000/50000 (40%)]\tTrain Loss: 0.771895\n",
            "Train Epoch: 82 [25000/50000 (50%)]\tTrain Loss: 0.737984\n",
            "Train Epoch: 82 [30000/50000 (60%)]\tTrain Loss: 0.754977\n",
            "Train Epoch: 82 [35000/50000 (70%)]\tTrain Loss: 0.770390\n",
            "Train Epoch: 82 [40000/50000 (80%)]\tTrain Loss: 0.788687\n",
            "Train Epoch: 82 [45000/50000 (90%)]\tTrain Loss: 0.761029\n",
            "\n",
            "Test set: Test loss: 0.9829, Accuracy: 3454/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 83: lr = 0.1\n",
            "Train Epoch: 83 [5000/50000 (10%)]\tTrain Loss: 0.758759\n",
            "Train Epoch: 83 [10000/50000 (20%)]\tTrain Loss: 0.740836\n",
            "Train Epoch: 83 [15000/50000 (30%)]\tTrain Loss: 0.706474\n",
            "Train Epoch: 83 [20000/50000 (40%)]\tTrain Loss: 0.701544\n",
            "Train Epoch: 83 [25000/50000 (50%)]\tTrain Loss: 0.750604\n",
            "Train Epoch: 83 [30000/50000 (60%)]\tTrain Loss: 0.723746\n",
            "Train Epoch: 83 [35000/50000 (70%)]\tTrain Loss: 0.753725\n",
            "Train Epoch: 83 [40000/50000 (80%)]\tTrain Loss: 0.749019\n",
            "Train Epoch: 83 [45000/50000 (90%)]\tTrain Loss: 0.781050\n",
            "\n",
            "Test set: Test loss: 0.9967, Accuracy: 3390/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 84: lr = 0.1\n",
            "Train Epoch: 84 [5000/50000 (10%)]\tTrain Loss: 0.776562\n",
            "Train Epoch: 84 [10000/50000 (20%)]\tTrain Loss: 0.722397\n",
            "Train Epoch: 84 [15000/50000 (30%)]\tTrain Loss: 0.755697\n",
            "Train Epoch: 84 [20000/50000 (40%)]\tTrain Loss: 0.723688\n",
            "Train Epoch: 84 [25000/50000 (50%)]\tTrain Loss: 0.738437\n",
            "Train Epoch: 84 [30000/50000 (60%)]\tTrain Loss: 0.764277\n",
            "Train Epoch: 84 [35000/50000 (70%)]\tTrain Loss: 0.822875\n",
            "Train Epoch: 84 [40000/50000 (80%)]\tTrain Loss: 0.777796\n",
            "Train Epoch: 84 [45000/50000 (90%)]\tTrain Loss: 0.799807\n",
            "\n",
            "Test set: Test loss: 0.9615, Accuracy: 3538/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 84: accuracy = 70.76%\n",
            "\n",
            "Train Epoch 85: lr = 0.1\n",
            "Train Epoch: 85 [5000/50000 (10%)]\tTrain Loss: 0.750804\n",
            "Train Epoch: 85 [10000/50000 (20%)]\tTrain Loss: 0.765975\n",
            "Train Epoch: 85 [15000/50000 (30%)]\tTrain Loss: 0.741563\n",
            "Train Epoch: 85 [20000/50000 (40%)]\tTrain Loss: 0.727346\n",
            "Train Epoch: 85 [25000/50000 (50%)]\tTrain Loss: 0.761402\n",
            "Train Epoch: 85 [30000/50000 (60%)]\tTrain Loss: 0.782297\n",
            "Train Epoch: 85 [35000/50000 (70%)]\tTrain Loss: 0.730564\n",
            "Train Epoch: 85 [40000/50000 (80%)]\tTrain Loss: 0.715545\n",
            "Train Epoch: 85 [45000/50000 (90%)]\tTrain Loss: 0.785449\n",
            "\n",
            "Test set: Test loss: 0.9511, Accuracy: 3482/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 86: lr = 0.1\n",
            "Train Epoch: 86 [5000/50000 (10%)]\tTrain Loss: 0.725042\n",
            "Train Epoch: 86 [10000/50000 (20%)]\tTrain Loss: 0.735292\n",
            "Train Epoch: 86 [15000/50000 (30%)]\tTrain Loss: 0.724572\n",
            "Train Epoch: 86 [20000/50000 (40%)]\tTrain Loss: 0.753818\n",
            "Train Epoch: 86 [25000/50000 (50%)]\tTrain Loss: 0.748133\n",
            "Train Epoch: 86 [30000/50000 (60%)]\tTrain Loss: 0.746113\n",
            "Train Epoch: 86 [35000/50000 (70%)]\tTrain Loss: 0.775366\n",
            "Train Epoch: 86 [40000/50000 (80%)]\tTrain Loss: 0.788235\n",
            "Train Epoch: 86 [45000/50000 (90%)]\tTrain Loss: 0.788653\n",
            "\n",
            "Test set: Test loss: 1.0483, Accuracy: 3431/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 87: lr = 0.1\n",
            "Train Epoch: 87 [5000/50000 (10%)]\tTrain Loss: 0.769670\n",
            "Train Epoch: 87 [10000/50000 (20%)]\tTrain Loss: 0.766822\n",
            "Train Epoch: 87 [15000/50000 (30%)]\tTrain Loss: 0.729652\n",
            "Train Epoch: 87 [20000/50000 (40%)]\tTrain Loss: 0.790870\n",
            "Train Epoch: 87 [25000/50000 (50%)]\tTrain Loss: 0.779351\n",
            "Train Epoch: 87 [30000/50000 (60%)]\tTrain Loss: 0.808545\n",
            "Train Epoch: 87 [35000/50000 (70%)]\tTrain Loss: 0.779532\n",
            "Train Epoch: 87 [40000/50000 (80%)]\tTrain Loss: 0.761603\n",
            "Train Epoch: 87 [45000/50000 (90%)]\tTrain Loss: 0.756061\n",
            "\n",
            "Test set: Test loss: 1.0250, Accuracy: 3399/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 88: lr = 0.1\n",
            "Train Epoch: 88 [5000/50000 (10%)]\tTrain Loss: 0.711094\n",
            "Train Epoch: 88 [10000/50000 (20%)]\tTrain Loss: 0.762703\n",
            "Train Epoch: 88 [15000/50000 (30%)]\tTrain Loss: 0.757294\n",
            "Train Epoch: 88 [20000/50000 (40%)]\tTrain Loss: 0.769582\n",
            "Train Epoch: 88 [25000/50000 (50%)]\tTrain Loss: 0.705855\n",
            "Train Epoch: 88 [30000/50000 (60%)]\tTrain Loss: 0.688753\n",
            "Train Epoch: 88 [35000/50000 (70%)]\tTrain Loss: 0.726377\n",
            "Train Epoch: 88 [40000/50000 (80%)]\tTrain Loss: 0.751264\n",
            "Train Epoch: 88 [45000/50000 (90%)]\tTrain Loss: 0.742674\n",
            "\n",
            "Test set: Test loss: 0.9243, Accuracy: 3507/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 89: lr = 0.1\n",
            "Train Epoch: 89 [5000/50000 (10%)]\tTrain Loss: 0.804422\n",
            "Train Epoch: 89 [10000/50000 (20%)]\tTrain Loss: 0.794280\n",
            "Train Epoch: 89 [15000/50000 (30%)]\tTrain Loss: 0.806907\n",
            "Train Epoch: 89 [20000/50000 (40%)]\tTrain Loss: 0.856819\n",
            "Train Epoch: 89 [25000/50000 (50%)]\tTrain Loss: 0.798633\n",
            "Train Epoch: 89 [30000/50000 (60%)]\tTrain Loss: 0.834632\n",
            "Train Epoch: 89 [35000/50000 (70%)]\tTrain Loss: 0.806183\n",
            "Train Epoch: 89 [40000/50000 (80%)]\tTrain Loss: 0.780245\n",
            "Train Epoch: 89 [45000/50000 (90%)]\tTrain Loss: 0.840381\n",
            "\n",
            "Test set: Test loss: 0.9830, Accuracy: 3454/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 90: lr = 0.1\n",
            "Train Epoch: 90 [5000/50000 (10%)]\tTrain Loss: 0.790072\n",
            "Train Epoch: 90 [10000/50000 (20%)]\tTrain Loss: 0.806058\n",
            "Train Epoch: 90 [15000/50000 (30%)]\tTrain Loss: 0.829089\n",
            "Train Epoch: 90 [20000/50000 (40%)]\tTrain Loss: 0.839316\n",
            "Train Epoch: 90 [25000/50000 (50%)]\tTrain Loss: 0.849827\n",
            "Train Epoch: 90 [30000/50000 (60%)]\tTrain Loss: 0.877623\n",
            "Train Epoch: 90 [35000/50000 (70%)]\tTrain Loss: 0.830139\n",
            "Train Epoch: 90 [40000/50000 (80%)]\tTrain Loss: 0.897879\n",
            "Train Epoch: 90 [45000/50000 (90%)]\tTrain Loss: 0.915729\n",
            "\n",
            "Test set: Test loss: 1.0150, Accuracy: 3433/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 91: lr = 0.1\n",
            "Train Epoch: 91 [5000/50000 (10%)]\tTrain Loss: 0.849911\n",
            "Train Epoch: 91 [10000/50000 (20%)]\tTrain Loss: 0.878801\n",
            "Train Epoch: 91 [15000/50000 (30%)]\tTrain Loss: 0.840030\n",
            "Train Epoch: 91 [20000/50000 (40%)]\tTrain Loss: 0.846129\n",
            "Train Epoch: 91 [25000/50000 (50%)]\tTrain Loss: 0.827650\n",
            "Train Epoch: 91 [30000/50000 (60%)]\tTrain Loss: 0.801066\n",
            "Train Epoch: 91 [35000/50000 (70%)]\tTrain Loss: 0.793449\n",
            "Train Epoch: 91 [40000/50000 (80%)]\tTrain Loss: 0.820832\n",
            "Train Epoch: 91 [45000/50000 (90%)]\tTrain Loss: 0.843950\n",
            "\n",
            "Test set: Test loss: 1.1312, Accuracy: 3265/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 92: lr = 0.1\n",
            "Train Epoch: 92 [5000/50000 (10%)]\tTrain Loss: 0.838651\n",
            "Train Epoch: 92 [10000/50000 (20%)]\tTrain Loss: 0.823459\n",
            "Train Epoch: 92 [15000/50000 (30%)]\tTrain Loss: 0.821291\n",
            "Train Epoch: 92 [20000/50000 (40%)]\tTrain Loss: 0.834781\n",
            "Train Epoch: 92 [25000/50000 (50%)]\tTrain Loss: 0.880955\n",
            "Train Epoch: 92 [30000/50000 (60%)]\tTrain Loss: 0.874431\n",
            "Train Epoch: 92 [35000/50000 (70%)]\tTrain Loss: 0.862380\n",
            "Train Epoch: 92 [40000/50000 (80%)]\tTrain Loss: 0.825599\n",
            "Train Epoch: 92 [45000/50000 (90%)]\tTrain Loss: 0.809429\n",
            "\n",
            "Test set: Test loss: 1.0214, Accuracy: 3389/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 93: lr = 0.1\n",
            "Train Epoch: 93 [5000/50000 (10%)]\tTrain Loss: 0.734285\n",
            "Train Epoch: 93 [10000/50000 (20%)]\tTrain Loss: 0.749889\n",
            "Train Epoch: 93 [15000/50000 (30%)]\tTrain Loss: 0.738298\n",
            "Train Epoch: 93 [20000/50000 (40%)]\tTrain Loss: 0.773135\n",
            "Train Epoch: 93 [25000/50000 (50%)]\tTrain Loss: 0.798750\n",
            "Train Epoch: 93 [30000/50000 (60%)]\tTrain Loss: 0.794057\n",
            "Train Epoch: 93 [35000/50000 (70%)]\tTrain Loss: 0.847729\n",
            "Train Epoch: 93 [40000/50000 (80%)]\tTrain Loss: 0.842368\n",
            "Train Epoch: 93 [45000/50000 (90%)]\tTrain Loss: 0.853515\n",
            "\n",
            "Test set: Test loss: 1.0983, Accuracy: 3275/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 94: lr = 0.1\n",
            "Train Epoch: 94 [5000/50000 (10%)]\tTrain Loss: 0.851091\n",
            "Train Epoch: 94 [10000/50000 (20%)]\tTrain Loss: 0.815561\n",
            "Train Epoch: 94 [15000/50000 (30%)]\tTrain Loss: 0.797618\n",
            "Train Epoch: 94 [20000/50000 (40%)]\tTrain Loss: 0.767541\n",
            "Train Epoch: 94 [25000/50000 (50%)]\tTrain Loss: 0.834276\n",
            "Train Epoch: 94 [30000/50000 (60%)]\tTrain Loss: 0.888509\n",
            "Train Epoch: 94 [35000/50000 (70%)]\tTrain Loss: 0.843271\n",
            "Train Epoch: 94 [40000/50000 (80%)]\tTrain Loss: 0.831516\n",
            "Train Epoch: 94 [45000/50000 (90%)]\tTrain Loss: 0.740824\n",
            "\n",
            "Test set: Test loss: 1.0180, Accuracy: 3381/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 95: lr = 0.1\n",
            "Train Epoch: 95 [5000/50000 (10%)]\tTrain Loss: 0.862533\n",
            "Train Epoch: 95 [10000/50000 (20%)]\tTrain Loss: 0.861263\n",
            "Train Epoch: 95 [15000/50000 (30%)]\tTrain Loss: 0.771636\n",
            "Train Epoch: 95 [20000/50000 (40%)]\tTrain Loss: 0.767790\n",
            "Train Epoch: 95 [25000/50000 (50%)]\tTrain Loss: 0.799876\n",
            "Train Epoch: 95 [30000/50000 (60%)]\tTrain Loss: 0.843198\n",
            "Train Epoch: 95 [35000/50000 (70%)]\tTrain Loss: 0.827001\n",
            "Train Epoch: 95 [40000/50000 (80%)]\tTrain Loss: 0.914898\n",
            "Train Epoch: 95 [45000/50000 (90%)]\tTrain Loss: 0.872735\n",
            "\n",
            "Test set: Test loss: 1.0117, Accuracy: 3481/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 96: lr = 0.1\n",
            "Train Epoch: 96 [5000/50000 (10%)]\tTrain Loss: 0.874718\n",
            "Train Epoch: 96 [10000/50000 (20%)]\tTrain Loss: 0.911833\n",
            "Train Epoch: 96 [15000/50000 (30%)]\tTrain Loss: 0.879485\n",
            "Train Epoch: 96 [20000/50000 (40%)]\tTrain Loss: 0.963723\n",
            "Train Epoch: 96 [25000/50000 (50%)]\tTrain Loss: 0.914838\n",
            "Train Epoch: 96 [30000/50000 (60%)]\tTrain Loss: 0.856759\n",
            "Train Epoch: 96 [35000/50000 (70%)]\tTrain Loss: 0.865528\n",
            "Train Epoch: 96 [40000/50000 (80%)]\tTrain Loss: 1.018575\n",
            "Train Epoch: 96 [45000/50000 (90%)]\tTrain Loss: 0.977877\n",
            "\n",
            "Test set: Test loss: 1.0940, Accuracy: 3302/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 97: lr = 0.1\n",
            "Train Epoch: 97 [5000/50000 (10%)]\tTrain Loss: 0.867558\n",
            "Train Epoch: 97 [10000/50000 (20%)]\tTrain Loss: 1.036423\n",
            "Train Epoch: 97 [15000/50000 (30%)]\tTrain Loss: 1.002412\n",
            "Train Epoch: 97 [20000/50000 (40%)]\tTrain Loss: 0.935947\n",
            "Train Epoch: 97 [25000/50000 (50%)]\tTrain Loss: 0.851782\n",
            "Train Epoch: 97 [30000/50000 (60%)]\tTrain Loss: 0.911152\n",
            "Train Epoch: 97 [35000/50000 (70%)]\tTrain Loss: 0.966595\n",
            "Train Epoch: 97 [40000/50000 (80%)]\tTrain Loss: 0.915944\n",
            "Train Epoch: 97 [45000/50000 (90%)]\tTrain Loss: 0.914481\n",
            "\n",
            "Test set: Test loss: 1.0020, Accuracy: 3432/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 98: lr = 0.1\n",
            "Train Epoch: 98 [5000/50000 (10%)]\tTrain Loss: 0.839981\n",
            "Train Epoch: 98 [10000/50000 (20%)]\tTrain Loss: 0.821880\n",
            "Train Epoch: 98 [15000/50000 (30%)]\tTrain Loss: 0.848898\n",
            "Train Epoch: 98 [20000/50000 (40%)]\tTrain Loss: 0.876659\n",
            "Train Epoch: 98 [25000/50000 (50%)]\tTrain Loss: 0.902736\n",
            "Train Epoch: 98 [30000/50000 (60%)]\tTrain Loss: 0.921345\n",
            "Train Epoch: 98 [35000/50000 (70%)]\tTrain Loss: 0.886563\n",
            "Train Epoch: 98 [40000/50000 (80%)]\tTrain Loss: 0.951558\n",
            "Train Epoch: 98 [45000/50000 (90%)]\tTrain Loss: 0.897878\n",
            "\n",
            "Test set: Test loss: 1.1901, Accuracy: 3136/5000 (63%)\n",
            "\n",
            "\n",
            "Train Epoch 99: lr = 0.1\n",
            "Train Epoch: 99 [5000/50000 (10%)]\tTrain Loss: 0.974632\n",
            "Train Epoch: 99 [10000/50000 (20%)]\tTrain Loss: 0.947470\n",
            "Train Epoch: 99 [15000/50000 (30%)]\tTrain Loss: 0.922767\n",
            "Train Epoch: 99 [20000/50000 (40%)]\tTrain Loss: 0.860740\n",
            "Train Epoch: 99 [25000/50000 (50%)]\tTrain Loss: 0.876464\n",
            "Train Epoch: 99 [30000/50000 (60%)]\tTrain Loss: 0.862060\n",
            "Train Epoch: 99 [35000/50000 (70%)]\tTrain Loss: 0.837964\n",
            "Train Epoch: 99 [40000/50000 (80%)]\tTrain Loss: 0.839090\n",
            "Train Epoch: 99 [45000/50000 (90%)]\tTrain Loss: 0.870205\n",
            "\n",
            "Test set: Test loss: 1.0358, Accuracy: 3371/5000 (67%)\n",
            "\n",
            "CPU times: user 39min 2s, sys: 52 s, total: 39min 53s\n",
            "Wall time: 42min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEpPEK4ZZbPo",
        "colab_type": "code",
        "outputId": "29643a2f-76a8-4f26-89bc-073201915517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhi8XV24AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 25.25911299897005 seconds\n",
            "Best accuracy: 70.76  Best training loss: 0.3689083755016327  Best validation loss: 0.915341284275055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhTWsbg_ejfb",
        "colab_type": "code",
        "outputId": "b21b5f64-8d2c-4f2a-d796-92f28bcec200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "[2.299943208694458, 2.302584409713745, 2.2689270973205566, 2.2405292987823486, 1.8913905620574951, 1.5071830749511719, 1.5770992040634155, 1.6421047449111938, 1.5562281608581543, 1.2674864530563354, 1.4688975811004639, 1.3458105325698853, 1.619601845741272, 1.3046716451644897, 1.403365135192871, 1.3041883707046509, 1.3262447118759155, 1.4430848360061646, 1.969878077507019, 1.5856781005859375, 1.5277986526489258, 1.4632666110992432, 1.3772468566894531, 1.1684536933898926, 1.453433632850647, 1.4331430196762085, 1.1156573295593262, 1.4916654825210571, 0.8227909207344055, 0.9565484523773193, 0.916340172290802, 0.546699047088623, 0.6915007829666138, 0.7833771705627441, 1.0670195817947388, 0.9512470364570618, 0.5745163559913635, 0.5368366837501526, 0.7670887112617493, 0.8164665699005127, 0.9123635292053223, 0.8607751727104187, 0.9274749159812927, 0.8329969048500061, 1.0191471576690674, 0.7217676043510437, 0.6931263208389282, 0.8372833132743835, 0.6234893798828125, 1.0054479837417603, 0.8574786186218262, 0.9506468176841736, 0.5369068384170532, 0.8145015835762024, 0.6357061862945557, 0.6081886887550354, 1.0314364433288574, 0.831519603729248, 0.8477627635002136, 0.6992102861404419, 0.49726149439811707, 0.8838151693344116, 0.8914069533348083, 0.783366858959198, 0.3689083755016327, 0.5936694741249084, 0.975844144821167, 0.9037153124809265, 0.7156571745872498, 0.5687954425811768, 0.5455962419509888, 0.4758591055870056, 0.8993263840675354, 0.5633352398872375, 0.6917872428894043, 0.8002436757087708, 1.1317416429519653, 0.686056911945343, 0.8114426136016846, 0.7684489488601685, 0.8510557413101196, 0.9491961598396301, 0.6910033226013184, 0.6753504872322083, 1.106218695640564, 0.8693298101425171, 0.8821081519126892, 1.110736608505249, 0.816966712474823, 0.8346093893051147, 0.8553116321563721, 0.5540435314178467, 0.8230381011962891, 0.6581259965896606, 0.7746570110321045, 1.0262528657913208, 0.9030080437660217, 0.8714494109153748, 1.0287939310073853, 0.7308042645454407]\n",
            "[2.3017338275909416, 2.3025844097137442, 2.273392198085785, 2.198218114376069, 1.896902248859406, 1.7970805597305304, 1.7090829503536222, 1.6736260128021239, 1.6153090381622315, 1.6547746610641478, 1.5930318760871887, 1.5475415992736812, 1.5633108782768246, 1.5415827131271365, 1.5022047281265252, 1.4890630710124966, 1.49146892786026, 1.5002406406402595, 1.458881777524948, 1.420024849176407, 1.402843114733696, 1.4003210854530337, 1.371710929870605, 1.4192046010494233, 1.3868184006214144, 1.3734155553579332, 1.3771162724494939, 1.2723421645164485, 1.2232313036918643, 1.0728230684995652, 1.0281804096698763, 0.9854114812612532, 1.0048834306001662, 1.0022794020175931, 1.0161184805631636, 0.9440914404392241, 0.9606862694025041, 0.9686857664585112, 0.9289505648612972, 1.0088523080945013, 0.9413778966665269, 0.9925783550739288, 0.915341284275055, 0.9632747304439544, 1.0143395262956616, 0.9782573264837269, 0.9657415407896041, 0.9163190352916717, 0.9571957233548163, 0.9453449010849001, 0.9708879515528679, 0.9604995876550676, 0.9563010215759282, 0.9944482940435411, 0.9584430086612702, 0.9240005755424502, 0.9603924006223679, 0.9806441378593443, 1.0018760120868686, 0.9538986796140674, 0.9707632303237913, 0.9704241564869882, 0.9888776540756228, 0.9856694674491885, 0.9604135221242905, 0.9644897490739821, 0.9782790440320969, 0.9362230199575426, 0.9886548525094988, 0.9779392561316488, 0.9695872670412061, 0.9636594155430794, 0.9771328619122508, 0.9854654353857041, 1.0241219460964208, 0.9641914910078049, 0.9242531400918962, 0.9244909429550172, 1.0043055817484856, 1.0444446462392807, 0.9806122130155563, 1.0023908942937851, 0.9829205536842351, 0.9967226019501685, 0.9614836731553075, 0.9511420682072639, 1.048323236107826, 1.0250487571954727, 0.9242830926179888, 0.983030314445496, 1.0149748158454892, 1.1311764049530029, 1.0214045548439026, 1.0982516652345662, 1.017959160804748, 1.0116557103395463, 1.0940487831830978, 1.00196366250515, 1.190124880671501, 1.0358290868997575]\n",
            "[15.5, 9.6, 14.18, 19.42, 31.24, 37.72, 39.88, 41.42, 43.72, 42.08, 44.08, 46.16, 45.28, 46.88, 46.62, 47.32, 47.9, 47.62, 48.22, 50.9, 51.08, 50.5, 51.86, 50.26, 51.82, 52.36, 51.52, 57.2, 58.38, 63.48, 65.62, 66.72, 66.12, 66.22, 66.34, 67.74, 67.92, 67.1, 68.68, 67.22, 68.26, 66.78, 69.2, 68.6, 67.4, 68.62, 68.2, 69.66, 69.6, 69.9, 69.84, 69.26, 70.36, 68.22, 67.58, 69.64, 69.04, 68.52, 69.46, 69.7, 69.88, 69.5, 67.92, 68.7, 69.08, 69.2, 69.16, 70.08, 69.78, 69.78, 68.9, 69.86, 68.1, 69.16, 68.3, 69.08, 70.46, 69.5, 69.42, 67.46, 69.16, 69.5, 69.08, 67.8, 70.76, 69.64, 68.62, 67.98, 70.14, 69.08, 68.66, 65.3, 67.78, 65.5, 67.62, 69.62, 66.04, 68.64, 62.72, 67.42]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cckvGR2jcWc4"
      },
      "source": [
        "## squeeze net (batch normed) (3x3 ratio 0.125)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EQfBGU9OcWc7",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 112,16),\n",
        "                Fire(128, 16, 112,16),\n",
        "                Fire(128, 32, 224,32),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 224,32),\n",
        "                Fire(256, 48, 336,48),\n",
        "                Fire(384, 48, 336,48),\n",
        "                Fire(384, 64, 448,64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 448,64),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dY8cLN59cWc_",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "__XuxrG3cWdD",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "38ce0e19-8e8c-43c3-bde8-1963bd9331fc",
        "id": "KgNBR-ZIcWdH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.217637\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.025467\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.924834\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.848480\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.789724\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.760014\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.710910\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.655862\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.632373\n",
            "Total train loss: 1.8168\n",
            "\n",
            "Test set: Test loss: 1.5600, Accuracy: 2215/5000 (44%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 44.3%\n",
            "Better loss at Epoch 0: loss = 1.5600179243087768%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.595686\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.533136\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.517072\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.513740\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.508828\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.453362\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.457356\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.441589\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.428663\n",
            "Total train loss: 1.4848\n",
            "\n",
            "Test set: Test loss: 1.3523, Accuracy: 2620/5000 (52%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 52.4%\n",
            "Better loss at Epoch 1: loss = 1.3523385703563688%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.356659\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.355949\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.373135\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.338013\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.317038\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.311713\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.311369\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.285953\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.286549\n",
            "Total train loss: 1.3183\n",
            "\n",
            "Test set: Test loss: 1.2506, Accuracy: 2818/5000 (56%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 56.36%\n",
            "Better loss at Epoch 2: loss = 1.2506290549039845%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.257712\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.208930\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.243178\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.199123\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.210224\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.229567\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.219187\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.200883\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.182622\n",
            "Total train loss: 1.2159\n",
            "\n",
            "Test set: Test loss: 1.1513, Accuracy: 2964/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 59.28%\n",
            "Better loss at Epoch 3: loss = 1.1513058376312264%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.177383\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.139199\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.138608\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.138142\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.132006\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.134750\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.112356\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.117173\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.110052\n",
            "Total train loss: 1.1300\n",
            "\n",
            "Test set: Test loss: 1.0818, Accuracy: 3091/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 61.82%\n",
            "Better loss at Epoch 4: loss = 1.0817698901891706%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.133516\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 1.053860\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.076914\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.063425\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.055975\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 1.063296\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 1.071062\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.074201\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 1.053016\n",
            "Total train loss: 1.0680\n",
            "\n",
            "Test set: Test loss: 1.1073, Accuracy: 3052/5000 (61%)\n",
            "\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 1.017444\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 1.014644\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 1.029176\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 1.048880\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.997591\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 1.015978\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.996352\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.973649\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 1.034360\n",
            "Total train loss: 1.0121\n",
            "\n",
            "Test set: Test loss: 0.9957, Accuracy: 3247/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 64.94%\n",
            "Better loss at Epoch 6: loss = 0.9956662768125536%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.971512\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.986371\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 1.003019\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.958390\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.951855\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.988223\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.993247\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.964569\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.949291\n",
            "Total train loss: 0.9704\n",
            "\n",
            "Test set: Test loss: 0.9448, Accuracy: 3340/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 66.8%\n",
            "Better loss at Epoch 7: loss = 0.9447680926322937%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.889775\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.922563\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.929767\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.917429\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.920987\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.927975\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.929133\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.954964\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.926327\n",
            "Total train loss: 0.9238\n",
            "\n",
            "Test set: Test loss: 0.9617, Accuracy: 3330/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.857134\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.857210\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.892842\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.893951\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.901834\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.888687\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.900048\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.924335\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.862715\n",
            "Total train loss: 0.8862\n",
            "\n",
            "Test set: Test loss: 0.9310, Accuracy: 3372/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 67.44%\n",
            "Better loss at Epoch 9: loss = 0.9309625986218453%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.830148\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.823460\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.852145\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.832040\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.877456\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.845353\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.861635\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.840532\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.884125\n",
            "Total train loss: 0.8506\n",
            "\n",
            "Test set: Test loss: 0.9053, Accuracy: 3443/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 68.86%\n",
            "Better loss at Epoch 10: loss = 0.9052761083841324%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.795885\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.819526\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.805355\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.821356\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.811111\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.821488\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.825548\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.821070\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.825604\n",
            "Total train loss: 0.8164\n",
            "\n",
            "Test set: Test loss: 0.9166, Accuracy: 3420/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.768693\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.775704\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.786583\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.802474\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.780049\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.774682\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.797325\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.804935\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.828682\n",
            "Total train loss: 0.7922\n",
            "\n",
            "Test set: Test loss: 0.8637, Accuracy: 3522/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 70.44%\n",
            "Better loss at Epoch 12: loss = 0.8636670261621473%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.726067\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.725402\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.738999\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.774558\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.749462\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.765246\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.768223\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.757165\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.770566\n",
            "Total train loss: 0.7549\n",
            "\n",
            "Test set: Test loss: 0.8721, Accuracy: 3505/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.735825\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.746898\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.729865\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.753199\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.732071\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.731058\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.744784\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.755170\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.729001\n",
            "Total train loss: 0.7380\n",
            "\n",
            "Test set: Test loss: 0.9264, Accuracy: 3395/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.662618\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.672266\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.707726\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.728639\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.723263\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.705446\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.719576\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.753105\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.734490\n",
            "Total train loss: 0.7144\n",
            "\n",
            "Test set: Test loss: 0.8570, Accuracy: 3534/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 70.68%\n",
            "Better loss at Epoch 15: loss = 0.857000865638256%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.686761\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.670944\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.673593\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.655612\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.688350\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.698316\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.676738\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.705347\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.704437\n",
            "Total train loss: 0.6825\n",
            "\n",
            "Test set: Test loss: 0.8419, Accuracy: 3537/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 70.74%\n",
            "Better loss at Epoch 16: loss = 0.8419499099254606%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.649155\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.647247\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.647843\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.655956\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.686147\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.648216\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.676961\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.692798\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.662748\n",
            "Total train loss: 0.6640\n",
            "\n",
            "Test set: Test loss: 0.8575, Accuracy: 3568/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 71.36%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.632027\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.642785\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.636015\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.614817\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.647118\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.641036\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.636888\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.660004\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.672986\n",
            "Total train loss: 0.6464\n",
            "\n",
            "Test set: Test loss: 0.8618, Accuracy: 3552/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.578956\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.579829\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.648670\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.617440\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.610276\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.646590\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.648448\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.632199\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.645051\n",
            "Total train loss: 0.6234\n",
            "\n",
            "Test set: Test loss: 0.8444, Accuracy: 3604/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 19: accuracy = 72.08%\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.600587\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.568642\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.583128\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.583808\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.593337\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.592897\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.608409\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.627643\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.625110\n",
            "Total train loss: 0.6035\n",
            "\n",
            "Test set: Test loss: 0.8679, Accuracy: 3534/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.554059\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.567695\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.545588\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.570861\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.595182\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.579029\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.613037\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.608991\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.593160\n",
            "Total train loss: 0.5872\n",
            "\n",
            "Test set: Test loss: 0.8627, Accuracy: 3526/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.513836\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.572446\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.546395\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.581044\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.556291\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.563508\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.581489\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.587282\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.582580\n",
            "Total train loss: 0.5658\n",
            "\n",
            "Test set: Test loss: 0.8662, Accuracy: 3559/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.518025\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.520187\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.540851\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.542744\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.534706\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.573446\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.546922\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.585414\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.544632\n",
            "Total train loss: 0.5474\n",
            "\n",
            "Test set: Test loss: 0.8504, Accuracy: 3579/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.497650\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.516445\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.505027\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.540951\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.551455\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.528794\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.528225\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.558256\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.577549\n",
            "Total train loss: 0.5347\n",
            "\n",
            "Test set: Test loss: 0.8635, Accuracy: 3549/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.456597\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.490626\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.487782\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.535074\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.526815\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.522784\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.524794\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.546914\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.553435\n",
            "Total train loss: 0.5187\n",
            "\n",
            "Test set: Test loss: 0.8581, Accuracy: 3613/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 25: accuracy = 72.26%\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.463676\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.456938\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.478754\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.491269\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.513676\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.509476\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.501696\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.518286\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.562068\n",
            "Total train loss: 0.5026\n",
            "\n",
            "Test set: Test loss: 0.8796, Accuracy: 3567/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.465251\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.468529\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.474674\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.465701\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.483474\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.477560\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.491163\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.520799\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.488105\n",
            "Total train loss: 0.4837\n",
            "\n",
            "Test set: Test loss: 0.8593, Accuracy: 3598/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.418026\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.451184\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.468293\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.472224\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.461898\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.476644\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.482316\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.502075\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.477573\n",
            "Total train loss: 0.4705\n",
            "\n",
            "Test set: Test loss: 0.8734, Accuracy: 3619/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 28: accuracy = 72.38%\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.431870\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.428579\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.435223\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.456490\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.453986\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.468305\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.478963\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.481325\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.487378\n",
            "Total train loss: 0.4605\n",
            "\n",
            "Test set: Test loss: 0.9202, Accuracy: 3557/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.419658\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.397880\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.410185\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.462319\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.433926\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.472742\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.425741\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.463005\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.470053\n",
            "Total train loss: 0.4397\n",
            "\n",
            "Test set: Test loss: 0.8713, Accuracy: 3607/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.398614\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.413698\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.428049\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.405660\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.443969\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.442778\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.443119\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.452517\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.472206\n",
            "Total train loss: 0.4352\n",
            "\n",
            "Test set: Test loss: 0.8819, Accuracy: 3587/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.379220\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.407547\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.398324\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.405889\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.402587\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.445608\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.412394\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.420746\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.470682\n",
            "Total train loss: 0.4182\n",
            "\n",
            "Test set: Test loss: 0.9016, Accuracy: 3614/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.355844\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.372020\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.381206\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.402866\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.396595\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.398638\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.403809\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.421101\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.437436\n",
            "Total train loss: 0.4005\n",
            "\n",
            "Test set: Test loss: 0.9099, Accuracy: 3607/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.351922\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.366021\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.378160\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.394346\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.387938\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.418735\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.414181\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.405629\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.391895\n",
            "Total train loss: 0.3930\n",
            "\n",
            "Test set: Test loss: 0.9298, Accuracy: 3560/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.334395\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.345600\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.366821\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.398889\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.413425\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.377900\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.380261\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.395526\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.409554\n",
            "Total train loss: 0.3808\n",
            "\n",
            "Test set: Test loss: 0.8972, Accuracy: 3623/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 35: accuracy = 72.46%\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.318573\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.335270\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.340843\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.367097\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.366363\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.378508\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.381319\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.366746\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.397775\n",
            "Total train loss: 0.3647\n",
            "\n",
            "Test set: Test loss: 0.9489, Accuracy: 3577/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.316245\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.329776\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.341968\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.399564\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.368040\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.366981\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.389994\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.375144\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.391350\n",
            "Total train loss: 0.3650\n",
            "\n",
            "Test set: Test loss: 0.9370, Accuracy: 3601/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.310781\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.303695\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.337844\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.334972\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.336892\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.347079\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.370756\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.375194\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.372292\n",
            "Total train loss: 0.3460\n",
            "\n",
            "Test set: Test loss: 0.9588, Accuracy: 3583/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.307186\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.298853\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.312807\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.321265\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.360883\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.338402\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.337721\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.359910\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.360528\n",
            "Total train loss: 0.3370\n",
            "\n",
            "Test set: Test loss: 1.0152, Accuracy: 3524/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.310648\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.285521\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.296781\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.307401\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.296754\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.335889\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.337819\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.353754\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.351360\n",
            "Total train loss: 0.3217\n",
            "\n",
            "Test set: Test loss: 0.9784, Accuracy: 3572/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.270648\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.280551\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.323567\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.322424\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.328433\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.344005\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.316044\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.324746\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.327735\n",
            "Total train loss: 0.3177\n",
            "\n",
            "Test set: Test loss: 0.9665, Accuracy: 3631/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 41: accuracy = 72.62%\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.255146\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.278178\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.287654\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.309777\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.296641\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.322831\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.323866\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.352315\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.333076\n",
            "Total train loss: 0.3070\n",
            "\n",
            "Test set: Test loss: 0.9678, Accuracy: 3593/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.247661\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.305829\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.275078\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.283874\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.298622\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.302682\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.314111\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.298953\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.300329\n",
            "Total train loss: 0.2944\n",
            "\n",
            "Test set: Test loss: 1.0147, Accuracy: 3593/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.275657\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.259756\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.284132\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.303411\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.286072\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.276696\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.276158\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.285195\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.305992\n",
            "Total train loss: 0.2848\n",
            "\n",
            "Test set: Test loss: 1.0149, Accuracy: 3596/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.256883\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.248247\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.286917\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.284249\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.274513\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.281221\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.294886\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.284915\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.320945\n",
            "Total train loss: 0.2834\n",
            "\n",
            "Test set: Test loss: 0.9884, Accuracy: 3606/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.256077\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.250944\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.273396\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.283776\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.281350\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.279064\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.280899\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.296820\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.303848\n",
            "Total train loss: 0.2806\n",
            "\n",
            "Test set: Test loss: 1.0587, Accuracy: 3527/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.233163\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.223742\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.243447\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.263415\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.291512\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.295489\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.302861\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.289447\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.283284\n",
            "Total train loss: 0.2717\n",
            "\n",
            "Test set: Test loss: 1.0494, Accuracy: 3560/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.233707\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.262785\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.215186\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.241261\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.245143\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.272892\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.279980\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.273244\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.268489\n",
            "Total train loss: 0.2563\n",
            "\n",
            "Test set: Test loss: 1.0758, Accuracy: 3559/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.238818\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.220279\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.253835\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.250024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e8e76cf8595a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     raise RuntimeError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "11f9ba9f-135b-48ed-cbe4-05f0f82b58a0",
        "id": "7FPzwTW8cWdM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKzORcHMzOpcFMzMrM5FwczM6r4B5a5eJyEEGL0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 34.49524099771432 seconds\n",
            "Best accuracy: 72.62  Best training loss: 0.25634893817827104  Best validation loss: 0.8419499099254606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b92c160a-8006-4a66-ec2a-cf31ebe078c1",
        "id": "c3_ls48wcWdP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
            "[1.8168393324613572, 1.4847788677215576, 1.3182759776115418, 1.215897289454937, 1.130046290040016, 1.0679998506903647, 1.0120616449713706, 0.9703730442523957, 0.9238243085443973, 0.8861975490450859, 0.8506102685332299, 0.8163609268069267, 0.7921581084132194, 0.7548786562681198, 0.7379953045845031, 0.7143634339272976, 0.6824760676622391, 0.6640138252973556, 0.6464400825202465, 0.6233637531101703, 0.6035101274549961, 0.5871770976036788, 0.565795158624649, 0.547414325043559, 0.5346717781573535, 0.5186531174182892, 0.5025907101780176, 0.48371973502635957, 0.47048676627874375, 0.46047647750377657, 0.439719740152359, 0.4351905076652765, 0.41816240069270133, 0.4004596504494548, 0.3929821141958237, 0.3807932251095772, 0.36473384764045474, 0.36500578248500826, 0.3459587673544884, 0.33704183156043294, 0.3216800020113587, 0.31771739772707225, 0.3070371732786298, 0.294410960637033, 0.2848191766142845, 0.2833619388565421, 0.28061465814709663, 0.27167759519815443, 0.25634893817827104]\n",
            "[1.5600179243087768, 1.3523385703563688, 1.2506290549039845, 1.1513058376312264, 1.0817698901891706, 1.1072921603918076, 0.9956662768125536, 0.9447680926322937, 0.9617192593216898, 0.9309625986218453, 0.9052761083841324, 0.9166160601377485, 0.8636670261621473, 0.8720985922217369, 0.9264078590273854, 0.857000865638256, 0.8419499099254606, 0.8574570366740228, 0.8617947399616237, 0.8444263979792599, 0.8678978228569028, 0.8627063739299774, 0.8661636638641357, 0.8503899976611139, 0.8634537610411643, 0.8580959451198579, 0.8795851874351504, 0.8592690017819404, 0.8734023559093478, 0.920182045996189, 0.8713012105226515, 0.8819212871789932, 0.9016052794456484, 0.9099073165655137, 0.9297571042180062, 0.8971985423564911, 0.9488551139831541, 0.9369553011655803, 0.9587818926572796, 1.0152495291829111, 0.9783918777108198, 0.9665129432082177, 0.9677662098407741, 1.0147426211833959, 1.0148622101545337, 0.988394903540611, 1.058702290058136, 1.0493665841221806, 1.0758255845308302]\n",
            "[44.3, 52.4, 56.36, 59.28, 61.82, 61.04, 64.94, 66.8, 66.6, 67.44, 68.86, 68.4, 70.44, 70.1, 67.9, 70.68, 70.74, 71.36, 71.04, 72.08, 70.68, 70.52, 71.18, 71.58, 70.98, 72.26, 71.34, 71.96, 72.38, 71.14, 72.14, 71.74, 72.28, 72.14, 71.2, 72.46, 71.54, 72.02, 71.66, 70.48, 71.44, 72.62, 71.86, 71.86, 71.92, 72.12, 70.54, 71.2, 71.18]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xbx40veQt4Cx"
      },
      "source": [
        "## squeeze net (batch normed) (3x3 ratio 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DmgXWXBt4Cz",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 96,32),\n",
        "                Fire(128, 16, 96,32),\n",
        "                Fire(128, 32, 192,64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 192,64),\n",
        "                Fire(256, 48, 288,96),\n",
        "                Fire(384, 48, 288,96),\n",
        "                Fire(384, 64, 384,128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 384,128),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J_bp95Kzt4C2",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bOfREHGMt4C5",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "74b6d522-20dc-43ff-b6b4-b7e2feac9016",
        "id": "K1mnpldyt4C7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.246792\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.032425\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.929960\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.840240\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.805132\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.736134\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.699053\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.671553\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.611074\n",
            "Total train loss: 1.8181\n",
            "\n",
            "Test set: Test loss: 1.5329, Accuracy: 2300/5000 (46%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 46.0%\n",
            "Better loss at Epoch 0: loss = 1.532898788452149%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.548183\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.534670\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.513521\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.476334\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.463104\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.450678\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.420054\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.444682\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.371954\n",
            "Total train loss: 1.4573\n",
            "\n",
            "Test set: Test loss: 1.2863, Accuracy: 2720/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 54.4%\n",
            "Better loss at Epoch 1: loss = 1.28628457069397%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.327110\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.303747\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.294326\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.277456\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.257862\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.256864\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.237331\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.235505\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.295063\n",
            "Total train loss: 1.2720\n",
            "\n",
            "Test set: Test loss: 1.2432, Accuracy: 2796/5000 (56%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 55.92%\n",
            "Better loss at Epoch 2: loss = 1.2432439178228376%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.178767\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.170506\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.197246\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.163070\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.162287\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.146693\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.164654\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.170102\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.149718\n",
            "Total train loss: 1.1605\n",
            "\n",
            "Test set: Test loss: 1.0815, Accuracy: 3116/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 62.32%\n",
            "Better loss at Epoch 3: loss = 1.0815348118543628%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.084665\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.107681\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.070307\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.102879\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.058317\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.073124\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.051447\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.040963\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.071927\n",
            "Total train loss: 1.0700\n",
            "\n",
            "Test set: Test loss: 1.0246, Accuracy: 3175/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 63.5%\n",
            "Better loss at Epoch 4: loss = 1.0245870190858841%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.002528\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.971962\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.021513\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.008178\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.016223\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.986893\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 1.000036\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.000156\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.992933\n",
            "Total train loss: 0.9981\n",
            "\n",
            "Test set: Test loss: 1.0017, Accuracy: 3214/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 64.28%\n",
            "Better loss at Epoch 5: loss = 1.001742744445801%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.941007\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.940837\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.914892\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.919640\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.958630\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.943155\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.927266\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.943512\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.940880\n",
            "Total train loss: 0.9356\n",
            "\n",
            "Test set: Test loss: 0.9544, Accuracy: 3307/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 66.14%\n",
            "Better loss at Epoch 6: loss = 0.9543958294391632%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.895894\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.880546\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.891522\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.894567\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.900923\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.873680\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.870591\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.871439\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.888462\n",
            "Total train loss: 0.8829\n",
            "\n",
            "Test set: Test loss: 0.9037, Accuracy: 3457/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 69.14%\n",
            "Better loss at Epoch 7: loss = 0.9037312316894536%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.801880\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.823153\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.829946\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.836731\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.872617\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.859878\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.813596\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.834412\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.838605\n",
            "Total train loss: 0.8339\n",
            "\n",
            "Test set: Test loss: 0.8721, Accuracy: 3460/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 69.2%\n",
            "Better loss at Epoch 8: loss = 0.8720895239710809%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.807696\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.785098\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.781824\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.783882\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.805869\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.770257\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.809911\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.804679\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.797353\n",
            "Total train loss: 0.7939\n",
            "\n",
            "Test set: Test loss: 0.8649, Accuracy: 3506/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 70.12%\n",
            "Better loss at Epoch 9: loss = 0.8648973527550696%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.738516\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.736765\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.729567\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.762350\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.736646\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.753149\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.772649\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.764506\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.772096\n",
            "Total train loss: 0.7542\n",
            "\n",
            "Test set: Test loss: 0.8345, Accuracy: 3552/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 71.04%\n",
            "Better loss at Epoch 10: loss = 0.8344543573260303%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.704171\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.704797\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.732425\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.719132\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.729969\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.701742\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.705558\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.722946\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.746092\n",
            "Total train loss: 0.7178\n",
            "\n",
            "Test set: Test loss: 0.8364, Accuracy: 3558/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 71.16%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.664217\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.664535\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.664567\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.674664\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.664304\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.714493\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.675055\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.710555\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.709385\n",
            "Total train loss: 0.6819\n",
            "\n",
            "Test set: Test loss: 0.8348, Accuracy: 3561/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 71.22%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.615645\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.640283\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.652732\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.649960\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.669549\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.639142\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.678334\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.650527\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.657800\n",
            "Total train loss: 0.6502\n",
            "\n",
            "Test set: Test loss: 0.8567, Accuracy: 3544/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.600353\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.628006\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.635768\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.589077\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.609639\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.621042\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.641591\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.644864\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.637198\n",
            "Total train loss: 0.6249\n",
            "\n",
            "Test set: Test loss: 0.8113, Accuracy: 3622/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 14: accuracy = 72.44%\n",
            "Better loss at Epoch 14: loss = 0.8113250833749771%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.531384\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.565446\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.565844\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.595148\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.612041\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.624706\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.598361\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.623871\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.630064\n",
            "Total train loss: 0.5958\n",
            "\n",
            "Test set: Test loss: 0.8061, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 72.82%\n",
            "Better loss at Epoch 15: loss = 0.8061066842079161%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.526249\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.575137\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.576195\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.551871\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.583201\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.568133\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.585086\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.581617\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.598733\n",
            "Total train loss: 0.5739\n",
            "\n",
            "Test set: Test loss: 0.7925, Accuracy: 3656/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 73.12%\n",
            "Better loss at Epoch 16: loss = 0.7924620547890665%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.540314\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.539057\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.532147\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.526626\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.570418\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.573935\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.550405\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.584429\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.583392\n",
            "Total train loss: 0.5576\n",
            "\n",
            "Test set: Test loss: 0.7980, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 73.8%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.491253\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.523067\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.508240\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.515609\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.522287\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.534994\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.560196\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.544436\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.548626\n",
            "Total train loss: 0.5314\n",
            "\n",
            "Test set: Test loss: 0.8376, Accuracy: 3637/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.488306\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.487938\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.490082\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.500702\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.509915\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.512919\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.511330\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.516391\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.526409\n",
            "Total train loss: 0.5074\n",
            "\n",
            "Test set: Test loss: 0.8219, Accuracy: 3632/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.444038\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.454300\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.477831\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.470358\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.488102\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.535496\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.504450\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.509303\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.503832\n",
            "Total train loss: 0.4886\n",
            "\n",
            "Test set: Test loss: 0.8068, Accuracy: 3675/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.435172\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.416169\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.455897\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.449524\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.470852\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.479538\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.470780\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.485089\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.481671\n",
            "Total train loss: 0.4642\n",
            "\n",
            "Test set: Test loss: 0.8536, Accuracy: 3640/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.411940\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.407116\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.413309\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.435391\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.466150\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.435304\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.464821\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.460280\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.459500\n",
            "Total train loss: 0.4433\n",
            "\n",
            "Test set: Test loss: 0.8443, Accuracy: 3637/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.389642\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.387757\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.421613\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.445292\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.426841\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.437819\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.467165\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.426472\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.424096\n",
            "Total train loss: 0.4271\n",
            "\n",
            "Test set: Test loss: 0.8331, Accuracy: 3655/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.376564\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.391706\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.392450\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.378450\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.397640\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.427217\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.430723\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.425546\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.453558\n",
            "Total train loss: 0.4108\n",
            "\n",
            "Test set: Test loss: 0.8424, Accuracy: 3622/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.364444\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.372733\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.398594\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.400430\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.425431\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.415324\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.407092\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.417279\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.402716\n",
            "Total train loss: 0.4033\n",
            "\n",
            "Test set: Test loss: 0.8362, Accuracy: 3702/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 25: accuracy = 74.04%\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.337356\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.334033\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.343805\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.396208\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.381168\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.400209\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.393710\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.381741\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.410692\n",
            "Total train loss: 0.3775\n",
            "\n",
            "Test set: Test loss: 0.8564, Accuracy: 3686/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.338130\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.349332\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.342400\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.379391\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.357489\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.383286\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.357681\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.391884\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.412290\n",
            "Total train loss: 0.3706\n",
            "\n",
            "Test set: Test loss: 0.8636, Accuracy: 3679/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.326203\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.335591\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.331329\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.344386\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.362826\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.362107\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.368697\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.356585\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.375981\n",
            "Total train loss: 0.3523\n",
            "\n",
            "Test set: Test loss: 0.8706, Accuracy: 3674/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.304643\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.316967\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.312549\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.340401\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.332587\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.380343\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.346218\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.348330\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.351003\n",
            "Total train loss: 0.3391\n",
            "\n",
            "Test set: Test loss: 0.9118, Accuracy: 3642/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.307039\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.291471\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.305772\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.328596\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.311602\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.336250\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.341898\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.333266\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.352104\n",
            "Total train loss: 0.3246\n",
            "\n",
            "Test set: Test loss: 0.9164, Accuracy: 3675/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.269379\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.287534\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.322538\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.299122\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.327645\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.312627\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.344118\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.339205\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.335806\n",
            "Total train loss: 0.3176\n",
            "\n",
            "Test set: Test loss: 0.9121, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.265043\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.258381\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.292854\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.288339\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.293280\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.341903\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.339327\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.306756\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.320877\n",
            "Total train loss: 0.3015\n",
            "\n",
            "Test set: Test loss: 0.8905, Accuracy: 3704/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 32: accuracy = 74.08%\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.248446\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.232227\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.268166\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.287164\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.295038\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.310248\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.288573\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.300270\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.326378\n",
            "Total train loss: 0.2875\n",
            "\n",
            "Test set: Test loss: 0.8942, Accuracy: 3681/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.239756\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.257585\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.273466\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.255103\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.276837\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.303412\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.285672\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.279540\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.297577\n",
            "Total train loss: 0.2757\n",
            "\n",
            "Test set: Test loss: 0.9472, Accuracy: 3678/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.229077\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.249700\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.255028\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.244828\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.262277\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.290094\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.277440\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.268096\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.277654\n",
            "Total train loss: 0.2662\n",
            "\n",
            "Test set: Test loss: 0.9442, Accuracy: 3642/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.220567\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.221792\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.237787\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.261701\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.264511\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.241307\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.278058\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.262932\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.271448\n",
            "Total train loss: 0.2556\n",
            "\n",
            "Test set: Test loss: 0.9470, Accuracy: 3672/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.228987\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.213380\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.244121\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.256142\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.227886\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.254320\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.284986\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.250686\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.279135\n",
            "Total train loss: 0.2503\n",
            "\n",
            "Test set: Test loss: 0.9778, Accuracy: 3656/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.206438\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.201518\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.220589\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.219260\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.247407\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.247676\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.278224\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.255010\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.271729\n",
            "Total train loss: 0.2427\n",
            "\n",
            "Test set: Test loss: 0.9748, Accuracy: 3647/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.209164\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.191196\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.209645\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.207220\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.213533\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.250368\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.229600\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.226606\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.280373\n",
            "Total train loss: 0.2269\n",
            "\n",
            "Test set: Test loss: 0.9787, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.205181\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.202033\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.211486\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.229792\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.219372\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.242472\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.238962\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.223564\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.240835\n",
            "Total train loss: 0.2257\n",
            "\n",
            "Test set: Test loss: 0.9678, Accuracy: 3663/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.187069\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.179243\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.198092\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.218159\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.201897\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.220778\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.225191\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.258540\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.230353\n",
            "Total train loss: 0.2138\n",
            "\n",
            "Test set: Test loss: 0.9947, Accuracy: 3657/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.173715\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.184673\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.182024\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.198346\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.210380\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.216597\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.213018\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.221639\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.209180\n",
            "Total train loss: 0.2017\n",
            "\n",
            "Test set: Test loss: 0.9990, Accuracy: 3700/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.169303\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.172928\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.168890\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.193213\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.211378\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.179997\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.211464\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.206199\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.229231\n",
            "Total train loss: 0.1949\n",
            "\n",
            "Test set: Test loss: 1.0970, Accuracy: 3606/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.186490\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.170219\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.162870\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.177326\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.184629\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.199244\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.188148\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.212995\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.217239\n",
            "Total train loss: 0.1916\n",
            "\n",
            "Test set: Test loss: 1.0278, Accuracy: 3658/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.150078\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.199579\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.158925\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.180636\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.187308\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.191421\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.198914\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.202883\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.188957\n",
            "Total train loss: 0.1841\n",
            "\n",
            "Test set: Test loss: 1.0541, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.146483\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.150938\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.164140\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.167197\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.197659\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.183910\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.185068\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.211329\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.189025\n",
            "Total train loss: 0.1790\n",
            "\n",
            "Test set: Test loss: 1.0892, Accuracy: 3647/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.138256\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.162602\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.157246\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.172545\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.161303\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.169449\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.168839\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.188448\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.183144\n",
            "Total train loss: 0.1675\n",
            "\n",
            "Test set: Test loss: 1.0871, Accuracy: 3648/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.145177\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.146739\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.155994\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.149943\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.167544\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.207402\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.196739\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.173232\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.162646\n",
            "Total train loss: 0.1669\n",
            "\n",
            "Test set: Test loss: 1.1012, Accuracy: 3667/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.140779\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.143743\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.149999\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.156924\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.162037\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.173479\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.166454\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.182308\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.165489\n",
            "Total train loss: 0.1608\n",
            "\n",
            "Test set: Test loss: 1.0579, Accuracy: 3688/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.130200\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.136710\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.152197\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.141493\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.165156\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.157927\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.171418\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.188313\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.164987\n",
            "Total train loss: 0.1565\n",
            "\n",
            "Test set: Test loss: 1.0958, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.129017\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.134469\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.159495\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.161956\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.170896\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.156059\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.154636\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.154665\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.172562\n",
            "Total train loss: 0.1557\n",
            "\n",
            "Test set: Test loss: 1.0800, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.123348\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.134130\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.143237\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.142756\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.139805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e8e76cf8595a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mstep_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e63a5ce-b88f-4e05-a956-7be36f0de450",
        "id": "NPiDQGqht4C_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3hUZfbA8e9Jr6RTkkBCk94DqIAU\nG2JhUVRQVGyo6+qurrvLukVlV9e2K+ryW0UFG4oFUVQQGwI2IHSkt0BCgBQS0kgyyfv7497EIU5I\nCJlMyvk8T56ZuW3ODWHOvF2MMSillFJVeXk6AKWUUo2TJgillFIuaYJQSinlkiYIpZRSLmmCUEop\n5ZImCKWUUi5pglCqkRGR70RkwBleo4OI5IuId30eW4trvSoi/7Sf9xWR78/0mspzNEEotxOR4SLy\nvYjkiki2/QE42NNxNTQR+UZEbqvhmMuBPGPMeqdtPUVkkf37yxORZSJy7qmuY4w5YIwJMcaU1RTX\n6Rx7Oowxm4Ac+55UE6QJQrmViLQCPgGeByKBOOARoNiTcTVidwJvVLwQkc7Ad8BmoCMQCywEPheR\nc1xdQER8GiDO2poH3OHpIFTdaIJQ7nYWgDHmbWNMmTGmyBjzuf3tEhHxFpGnRSRTRPaKyN0iYio+\n5ERkv4hcUHExEXlYRN50en22XTrJEZGNIjLKaV+YiLwiIukikiYi/6yoRrGPzXf6MRXn1nDNb0Tk\nH3YpKE9EPheR6JriEZFHgRHAf+33+2/VX5SI+AFjgOVOmx8GfjDG/MUYk22MyTPGPIeVRJ6wz0u0\n479VRA4AXzttq/g9dhSRFXbMX4rIrIrfo4tja7rH90TksF2iWSEivU7x7/8NcL6I+J/iGNVIaYJQ\n7rYTKBOR10TkEhGJqLL/duAyYACQBEys7YVFJA74FPgnVunkAWCBiMTYh7wKOIAu9vUvAm4DMMb0\ns6tVQoD7gR3AulpcE+A64GagNeBnH3PKeIwxfwFWAr+x3/c3Lm6pK1BujEl12nYh8J6LY98FholI\noNO2kUAP4GIXx78FrAaisJLODS6OcebyHm1L7FhbA+uwSgkuGWPSgFKgWw3vpxohTRDKrYwxx4Hh\ngAFeAjLs+vQ29iHXADONMQeNMdnAv07j8lOAxcaYxcaYcmPMF0AyMM6+/jjgd8aYAmPMUeAZYJLz\nBURkONYH+hV2rNVe0+m0ucaYncaYIqwP6v41xVPL+wkH8qpsiwbSXRybjvX/N9Jp28P2vRZVuccO\nwGDg78aYEmPMt8CiGmKp7h4xxsyxSzLFWMmmn4iEneJaefa9qSZGE4RyO2PMNmPMVGNMPNAbqx59\npr07FjjodHjKaVw6Abjars7JEZEcrGTUzt7nC6Q77XsR61svACLSHuvD7yZjzM5aXLPCYafnhUDI\naZx7KseA0CrbMqs5vx1Qbp9T4aCL48D6HWcbYwprcWwFl/doVwk+LiJ7ROQ4sN8+JprqhQI5Nbyf\naoQaU2OWagGMMdtF5FV+brhMB9o7HdKhyikFQJDT67ZOzw8Cbxhjbq/6PiLSDqshPNoY43CxPxD4\nEKv0sqQ216yFms6taerk3VZoEmdXzQB8CVwNzK1y7DVYbROFIlLT9dOBSBEJckoS7as5tibXAeOB\nC7CSQxhWkhJXB9vVbn5YVXiqidEShHIrEekuIr8XkXj7dXtgMvCjfci7wL0iEm+3T0yvcokNwCQR\n8RWRqm0UbwKXi8jF9jfbABEZJSLxxph04HPg3yLSSkS8RKSziIy0z50DbDfGPFnl/aq9Zi1ut6Zz\njwCdqjvZGFOClRBGOm1+BDhXRB4VkUgRCRWRe4AbgT/VIiaMMSlYVV0Pi4ifWL2f6tr1NBQr8WZh\nJe7Hajh+JPC1XR2lmhhNEMrd8oChwCoRKcBKDFuA39v7XwKWAhuxGjw/qHL+34DOWN9SH8FqbAXA\nGHMQ69vsg0AG1jf4P/Dz3/WNWN9et9rnv8/P1TWTgAlyck+mEbW4ZrVqce6zwEQROSYiz1VzmRdx\nakA2xuzCqqbqh/WNPR24CrjYGPNdTTE5uR44B+uD/Z/AO9Stq/HrWNWAaVi/1x9PfTjXAy/U4X1U\nIyC6YJBqTEQkEdgH+LqqGmoJROQ7rN5O62s8uO7v8Q5WCeohN75HX+BFY4zL8Rqq8dMEoRoVTRDu\nIdbI9Wys3+1FWO0v57gzCammTxuplWoZ2mJV30UBqcBdmhxUTbQEoZRSyiVtpFZKKeVSs6piio6O\nNomJiZ4OQymlmoy1a9dmGmNiXO1rVgkiMTGR5ORkT4ehlFJNhohUO3uBVjEppZRySROEUkoplzRB\nKKWUcqlZtUEopRpWaWkpqampnDhxwtOhqBoEBAQQHx+Pr69vrc/RBKGUqrPU1FRCQ0NJTEzEaVZZ\n1cgYY8jKyiI1NZWOHTvW+jytYlJK1dmJEyeIiorS5NDIiQhRUVGnXdLTBKGUOiOaHJqGuvw7tfgE\nUVZumLVsNyt2Zng6FKWUalRafILw9hJeXL6Hz7cervlgpVSjkZWVRf/+/enfvz9t27YlLi6u8nVJ\nSUmtrnHzzTezY8epF7ubNWsW8+bNq4+QGT58OBs2bKiXazUEbaQGEqODSckqrPlApVSjERUVVflh\n+/DDDxMSEsIDDzxw0jHGGIwxeHm5/i48d27VlVx/6e677z7zYJuoFl+CAOgQGaQJQqlmYvfu3fTs\n2ZPrr7+eXr16kZ6ezrRp00hKSqJXr17MmDGj8tiKb/QOh4Pw8HCmT59Ov379OOecczh69CgAf/3r\nX5k5c2bl8dOnT2fIkCF069aN77//HoCCggKuuuoqevbsycSJE0lKSqqxpPDmm2/Sp08fevfuzYMP\nPgiAw+HghhtuqNz+3HPWwoPPPPMMPXv2pG/fvkyZMqXef2fV0RIEkBgVzJIthyktK8fXW3OmUnXx\nyMc/sfXQ8Xq9Zs/YVjx0ea/TPm/79u28/vrrJCUlAfD4448TGRmJw+Fg9OjRTJw4kZ49e550Tm5u\nLiNHjuTxxx/n/vvvZ86cOUyfXnWJdKtUsnr1ahYtWsSMGTP47LPPeP7552nbti0LFixg48aNDBw4\n8JTxpaam8te//pXk5GTCwsK44IIL+OSTT4iJiSEzM5PNmzcDkJOTA8CTTz5JSkoKfn5+ldsagn4a\nAglRQZSVG9KOFXk6FKVUPejcuXNlcgB4++23GThwIAMHDmTbtm1s3br1F+cEBgZyySWXADBo0CD2\n79/v8tpXXnnlL4759ttvmTRpEgD9+vWjV69TJ7VVq1YxZswYoqOj8fX15brrrmPFihV06dKFHTt2\ncO+997J06VLCwsIA6NWrF1OmTGHevHmnNdDtTLmtBCEic4DLgKPGmN4u9v8Ba0Hzijh6ADHGmGwR\n2Y+12H0Z4DDGJFU9vz4lRAUDsD+rgMToYHe+lVLNVl2+6btLcPDP/4937drFs88+y+rVqwkPD2fK\nlCkuxwP4+flVPvf29sbhcL3irb+/f43H1FVUVBSbNm1iyZIlzJo1iwULFjB79myWLl3K8uXLWbRo\nEY899hibNm3C29u7Xt/bFXeWIF4Fxla30xjzlDGmvzGmP/BnYLkxJtvpkNH2frcmB4DEqCAAbYdQ\nqhk6fvw4oaGhtGrVivT0dJYuXVrv7zFs2DDeffddADZv3uyyhOJs6NChLFu2jKysLBwOB/Pnz2fk\nyJFkZGRgjOHqq69mxowZrFu3jrKyMlJTUxkzZgxPPvkkmZmZFBY2zGeV20oQxpgV9gL0tTEZeNtd\nsdQkJtSfQF9vTRBKNUMDBw6kZ8+edO/enYSEBIYNG1bv73HPPfdw44030rNnz8qfiuohV+Lj4/nH\nP/7BqFGjMMZw+eWXc+mll7Ju3TpuvfVWjDGICE888QQOh4PrrruOvLw8ysvLeeCBBwgNDa33e3DF\nrWtS2wniE1dVTE7HBGEtot6logQhIvuAY4ABXjTGzD7F+dOAaQAdOnQYlJJS7doXpzR25griwgN5\nZergOp2vVEu0bds2evTo4ekwPM7hcOBwOAgICGDXrl1cdNFF7Nq1Cx+fxtUPyNW/l4isra6mpjFE\nfznwXZXqpeHGmDQRaQ18ISLbjTErXJ1sJ4/ZAElJSXXOdglRQezJKKjr6UqpFiw/P5/zzz8fh8OB\nMYYXX3yx0SWHumgMdzCJKtVLxpg0+/GoiCwEhgAuE0R9SYwKZtn2DMrKDd5eOreMUqr2wsPDWbt2\nrafDqHce7eYqImHASOAjp23BIhJa8Ry4CNji7lg6RAVRUlbO4eM6r71SSoF7u7m+DYwCokUkFXgI\n8AUwxrxgHzYB+NwY41y30wZYaM886AO8ZYz5zF1xVki0u7qmZBUQFx7o7rdTSqlGz529mCbX4phX\nsbrDOm/bC/RzT1TV6xD5c1fXczs39LsrpVTjoyOpbbHhgfh6i3Z1VUopmyYIm7eX0D4yiJQs7cmk\nVFMxevToXwx8mzlzJnfdddcpzwsJCQHg0KFDTJw40eUxo0aNIjk5+ZTXmTlz5kmD1saNG1cvcyU9\n/PDDPP3002d8nTOlCcJJQmQQ+7UEoVSTMXnyZObPn3/Stvnz5zN5co013ADExsby/vvv1/n9qyaI\nxYsXEx4eXufrNTaaIJwkRAVzIKsAdw4eVErVn4kTJ/Lpp59WLhC0f/9+Dh06xIgRIyrHJgwcOJA+\nffrw0Ucf/eL8/fv307u3NY63qKiISZMm0aNHDyZMmEBR0c+Td951112V04U/9NBDADz33HMcOnSI\n0aNHM3r0aAASExPJzMwE4D//+Q+9e/emd+/eldOF79+/nx49enD77bfTq1cvLrroopPex5UNGzZw\n9tln07dvXyZMmMCxY8cq379iCvCKiQKXL19euWjSgAEDyMvLq/PvFhrHOIhGIzEqiIKSMjLzS4gJ\n9fd0OEo1LUumw+HN9XvNtn3gkser3R0ZGcmQIUNYsmQJ48ePZ/78+VxzzTWICAEBASxcuJBWrVqR\nmZnJ2WefzRVXXFHt2sz/+9//CAoKYtu2bWzatOmkKbsfffRRIiMjKSsr4/zzz2fTpk3ce++9/Oc/\n/2HZsmVER0efdK21a9cyd+5cVq1ahTGGoUOHMnLkSCIiIti1axdvv/02L730Etdccw0LFiw45RoP\nN954I88//zwjR47k73//O4888ggzZ87k8ccfZ9++ffj7+1dWaz399NPMmjWLYcOGkZ+fT0BAwOn8\ntn9BSxBOKmZ1PZCt7RBKNRXO1UzO1UvGGB588EH69u3LBRdcQFpaGkeOHKn2OitWrKj8oO7bty99\n+/at3Pfuu+8ycOBABgwYwE8//VTjZHzffvstEyZMIDg4mJCQEK688kpWrlwJQMeOHenfvz9w6mnF\nwVqjIicnh5EjRwJw0003sWLFisoYr7/+et58883KUdvDhg3j/vvv57nnniMnJ+eMR3NrCcJJgj2r\n6/7MQgYlRHo4GqWamFN803en8ePHc99997Fu3ToKCwsZNGgQAPPmzSMjI4O1a9fi6+tLYmKiy2m+\na7Jv3z6efvpp1qxZQ0REBFOnTq3TdSpUTBcO1pThNVUxVefTTz9lxYoVfPzxxzz66KNs3ryZ6dOn\nc+mll7J48WKGDRvG0qVL6d69e51j1RKEk/iIILwE7cmkVBMSEhLC6NGjueWWW05qnM7NzaV169b4\n+vqybNkyaprI87zzzuOtt94CYMuWLWzatAmwpgsPDg4mLCyMI0eOsGTJkspzQkNDXdbzjxgxgg8/\n/JDCwkIKCgpYuHAhI0aMOO17CwsLIyIiorL08cYbbzBy5EjKy8s5ePAgo0eP5oknniA3N5f8/Hz2\n7NlDnz59+NOf/sTgwYPZvn37ab+nMy1BOPHz8SI2PJCUbO3JpFRTMnnyZCZMmHBSj6brr7+eyy+/\nnD59+pCUlFTjN+m77rqLm2++mR49etCjR4/Kkki/fv0YMGAA3bt3p3379idNFz5t2jTGjh1LbGws\ny5Ytq9w+cOBApk6dypAhQwC47bbbGDBgwCmrk6rz2muvceedd1JYWEinTp2YO3cuZWVlTJkyhdzc\nXIwx3HvvvYSHh/O3v/2NZcuW4eXlRa9evSpXyKsrt0733dCSkpJMTf2WazLl5VXkFTv46O76nzNe\nqeZGp/tuWk53um+tYqqiQ1QQB7SKSSmlNEFUlRgVxLHCUnILSz0dilJKeZQmiCoqurqmaFdXpWql\nOVVTN2d1+XfSBFFFRVdXnbRPqZoFBASQlZWlSaKRM8aQlZV12gPntBdTFT9P+60lCKVqEh8fT2pq\nKhkZGZ4ORdUgICCA+Pj40zpHE0QVQX4+tGnlryUIpWrB19eXjh07ejoM5SZaxeRCQmSwJgilVIun\nCcKFhKgg9msVk1KqhdME4UJCVBBH84opLHF4OhSllPIYTRCOEvjsQdi6qHLTz7O6ajWTUqrl0gTh\n7Qtb3ocdiys3JVaMhdB2CKVUC+a2BCEic0TkqIhsqWb/KBHJFZEN9s/fnfaNFZEdIrJbRKa7K0b7\nzSB2ABxaX7mpQ5R2dVVKKXeWIF4FxtZwzEpjTH/7ZwaAiHgDs4BLgJ7AZBHp6cY4rQSRsQOK8wEI\nC/QlIshX16dWSrVobksQxpgVQHYdTh0C7DbG7DXGlADzgfH1GlxVsQMAA4c3VW6y1qfWBKGUark8\n3QZxjohsFJElItLL3hYHHHQ6JtXe5pKITBORZBFJrvNoznbW8n/O1Uza1VUp1dJ5MkGsAxKMMf2A\n54EP63IRY8xsY0ySMSYpJiambpGEtoFWcVUSRDCHcooocZTX7ZpKKdXEeSxBGGOOG2Py7eeLAV8R\niQbSgPZOh8bb29yrSkN1QmQQ5QZSj2k1k1KqZfJYghCRtiIi9vMhdixZwBqgq4h0FBE/YBKwqPor\n1ZPYAZC1G07kApAYrbO6KqVaNrdN1icibwOjgGgRSQUeAnwBjDEvABOBu0TEARQBk4w1Z7BDRH4D\nLAW8gTnGmJ/cFWel2AHWY/pG6Hhe5WA5bYdQSrVUbksQxpjJNez/L/DfavYtBha72uc2FQkibR10\nPI+oYD+igv3YnJrboGEopVRj4eleTI1HUCSEJ1S2Q4gIw7pEs3J3pi6GopRqkTRBOKvSUD28azQZ\necXsOJLnwaCUUsozNEE4ix0AOSlQaI3vG9E1GoBvd2V6MiqllPIITRDOKtoh7FJEu7BAOscEs1IT\nhFKqBdIE4axdP+vRqZppRNcYVu3L4kRpmYeCUkopz9AE4SwwHCI7V0kQ0ZwoLWddyjEPBqaUUg1P\nE0RVsQPg0IbKl0M7ReHjJazcrdVMSqmWRRNEVXED4Xgq5B8FIMTfh4EJEazcVceJAJVSqonSBFFV\nZUP1z6WIEV2i+enQcbILSjwUlFJKNTxNEFW17QsIHFpXuWl412iMge+0mkkp1YJogqjKPwRiup3U\nUN03PpxWAT5azaSUalE0QbhSMaLanmLD28uaduPbXTrthlKq5dAE4UrsAMg/AnnplZuGd43mUO4J\n9mbq7K5KqZZBE4QrVUZUA4zoYq1Wt3KnVjMppVoGTRCutOkN4n1SgugQFURCVBDfakO1UqqF0ATh\nil8QtO5xUoIAGN4lmh/2ZFFaputUK6WaP00Q1Yntf1JDNVjTbhSUlLH+QI4HA1NKqYahCaI6sQOh\nMAtyD1ZuOqdzNF4C32p3V6VUC6AJojouGqrDAn3p1z5c52VSSrUImiCq06YX+IXAsn9Bzs+liBFd\notl4MIfcwlIPBqeUUu6nCaI6Pv4waR4cPwQvnw9p1tQbw7vGUG7g+z1ailBKNW9uSxAiMkdEjorI\nlmr2Xy8im0Rks4h8LyL9nPbtt7dvEJFkd8VYo06j4NbPrWQxdxxs+4QBHcKJDPZjwbpUj4WllFIN\nwZ0liFeBsafYvw8YaYzpA/wDmF1l/2hjTH9jTJKb4qud1t3htq+sKqd3puC7ahY3DO3Al9uOsicj\n36OhKaWUO7ktQRhjVgDZp9j/vTGmYpm2H4F4d8VyxkJaw9RPoOcV8PlfubNgFoE+hle+3efpyJRS\nym0aSxvErcASp9cG+FxE1orItFOdKCLTRCRZRJIzMtzY/dQ3ECa+CsPvI3Dja/wnfiUL1qaSlV/s\nvvdUSikP8niCEJHRWAniT06bhxtjBgKXAHeLyHnVnW+MmW2MSTLGJMXExLg3WC8vuOBhiB/M6LIf\nKHaU88aPKe59T6WU8hCPJggR6Qu8DIw3xmRVbDfGpNmPR4GFwBDPRFiNbuMIyNjIVV2EN35I4URp\nmacjUkqpeuexBCEiHYAPgBuMMTudtgeLSGjFc+AiwGVPKI/pfhkAd8fuJKughIXr0zwckFJK1T93\ndnN9G/gB6CYiqSJyq4jcKSJ32of8HYgC/q9Kd9Y2wLcishFYDXxqjPnMXXHWScxZENWFjpnf0Duu\nFS+v3Et5uS4kpJRqXnzcdWFjzOQa9t8G3OZi+16g3y/PaGS6X4r8MIu7xv6Luz/Yy7IdRzm/RxtP\nR6WUUvXG443UTVa3S6HcwcX+m2kXFsBLK/d6OiKllKpXmiDqKj4Jglvjs3MJtwzryI97s9mSluvp\nqJRSqt5ogqgrL2/oNhZ2fcG1A1sT4u+jpQilVLOiCeJMdLsUSvJodfhHJg1uzyeb0knLKfJ0VEop\nVS80QZyJTiPBNxi2L+bm4R0R4P+W7fZ0VEopVS80QZwJ30DoMgZ2LCaulT/XD+3A/DUH2X00z9OR\nKaXUGdMEcaa6XwZ56ZC+nnvP70qQrzePL9nu6aiUUuqMaYI4U10vAvGG7Z8SFeLPr0d34cttR3VB\nIaVUk6cJ4kwFRULCubB9MQA3D0skLjyQxxZv09HVSqkmTRNEfeh+KWRsg6w9BPh684eLu7El7Tgf\nbdQ5mpRSTZcmiPrQbZz1uMMqRVzRL5Y+cWE89dkOnelVKeVa/lE41riXC9AEUR8iEqBNH9j+KQBe\nXsKD43pwKPcEc77TVeeUatQyd8PCuyDfjQuOAZSVQsr38NUMePE8eLorPNsPfnyhdufv+hK+fAT2\nrYQyh3tjtbltsr4Wp/s4WPGU9UcWEsM5naO4oEcb/m/ZHq5Nak9UiL+nI1RKufL5X2HnEijMhOve\nBZHTv0Z5OayeDRveBG8/8A2yfvyCrLFSJ3Jg3wooPm51amk/FM7/O6Suhc/+BMf2wcWPWTM0VFVW\naiWV75+zXn/7HwiMhG6XWL0oO4+2uty7gZYg6kv3S8GUw/rXKzdNv6Q7RaVlPPvVLg8GppSqVupa\nKznEDoBdn8OqWn6bd3b8ELx5pfVB7+0PgRFQXgb5h+HwFti7DI5sgV4T4Jo34E/74JYlMOL3cO0b\ncPbd1vu+MwVKCk6+dm4avHqplRwG3wZ/3AfXvA5dLoBtn8D8yfBkJ3jnBiuR1DMtQdSXtn2ttoiv\n/wkxPaD7OLq0DuG6IR2Yt+oAN56TSJfWIZ6OUinlbNmj1rfxmz6GD6bBF3+3eiW2q+WKA1s+gE/u\ng7ISuOwZGHTz6ZVAvLxh7GMQ2RGW/BHmjoPr3oHQtlaV0sJp4CiGiXOg91XWOT3HWz9lpbB/pVW1\nfTwdvH1P//5rIMY0n66YSUlJJjk5ueYD3aWkAF69DI5ug6mfQHwSmfnFjHn6Gzq3DuG9O87Bx1sL\nbUo1Cgd+hDkXw4UzYNhvoSALXhgGfsEwbTn4n+IL3YlcWPwH2PQOxA2CCbMhusuZxbPjM3j/Fqvr\nfPdLYdWL0LqnVWI402ufgoisNcYkudqnn1b1yS/Yzv5t4K1rIGsP0SH+/HNCH9YfyOH/vtnj6QiV\nUhW+/icEt4bBt1uvg6PgytmQtceqLnLFGNj2MfxvGGx+H0b+CW5ZWj8f4N3Gws2LrZLBqhdg4A1w\n+1duTQ41qVWCEJHOIuJvPx8lIveKSLh7Q2uiQlrD9QusP6R5E6Egkyv6xTK+fyzPfrWLjQdzPB2h\nUmrfCqt6ZsT9VkNyhY7nWdvWvwlbFvy83RjY8zW8NMZqK/ANtBLD6Afrt2ontj/csdyq8rriebc1\nPtdWbUsQC4AyEekCzAbaA2+5LaqmLrqLVZI4fgjeuhZKCpkxvjetQ/25790NFJXo2AilPMYY+PpR\nCI212gyqGvVniB8MH/8Oju2Hg6vhtcvhjQlQkAHjZ8FdP0D7we6JL7StlagagdomiHJjjAOYADxv\njPkD0M59YTUD7YfAVS9D2lpYcBth/l78++p+7M0o4LHF2zwdnVIt156v4OCPcN7vwTfgl/u9fa3/\nuwCzR8MrF0LGdrjkSbhnLQyYAt4to39PbRNEqYhMBm4CPrG31ViuEpE5InJURLZUs19E5DkR2S0i\nm0RkoNO+m0Rkl/1zUy3jbFx6XA6XPAE7PoVZQzk39xPuODeWN35MYdmOo56OTqmWxxir7SGsAwy4\nsfrjIhJh/H+tdsXz/w73boChd4BPyxrPVNs0eDNwJ/CoMWafiHQE3qjFea8C/wVer2b/JUBX+2co\n8D9gqIhEAg8BSYAB1orIImPMsVrG23gMvQNC2liDWz7+LdODY2gXdjGPvldCv/suJTLYz9MRKtVy\n7FgCh9Zb9fs+Nfzfq+hO2oLVqgRhjNlqjLnXGPO2iEQAocaYJ2px3gog+xSHjAdeN5YfgXARaQdc\nDHxhjMm2k8IXwNjaxNoo9fqV1W3upo+Rdv2ZWvwmi0rvYOOLt2MKsjwdnVItQ2kRLHsMIjpCv8me\njqZJqFUJQkS+Aa6wj18LHBWR74wx95/h+8cBB51ep9rbqtvedIlYDU8dz4MjWzmw8DGGp39M7v+2\nEH7Hp1bDlFLqzKVvgrWvWgt5FWTYP5lQkm/tn/CiWwaVNUe1rWIKM8YcF5HbsL7xPyQim9wZWG2J\nyDRgGkCHDh08HE0ttelJ12lv8OQLs7n3yN8onn0h/rd8bNV7KqXqJnufNTJ683vgF2KVFIKjrP9X\nwTEQFAXRZ1ltg6pWapsgfOyqn2uAv9Tj+6dhdZmtEG9vSwNGVdn+jasLGGNmY3W9JSkpqckMC/f2\nEu6aejMPPGd4Im8GPq+MxfumRRBzlqdDU8q9jqfD6heh3AH+YRDQCgLCwL+V1ShcnAeFWU4/2VCS\nBzHdre6ncUnWB3+F/AxY8f0XCjsAACAASURBVCQkzwUvHxh+vzUyOlCHap2p2iaIGcBS4DtjzBoR\n6QTUxwx0i4DfiMh8rEbqXGNMuogsBR6z2zsALgL+XA/v16iEB/nxu6nXc+P/CXML/0X43LHIDQtr\nPw+MUk1JmQPWvGSNQXAUWR/mjhOnPsc3yPrm7xNgrdpo7DFEER2tZBEcDetet9oXBt5ojWxupT3w\n60utEoQx5j3gPafXe4GrajpPRN7GKglEi0gqVs8kX/saLwCLgXHAbqAQq7cUxphsEfkHsMa+1Axj\nzKkau5usbm1DuePqK7jyLW8Wej9B2KuXIde/Bx3O9nRoStWfg2vg0/vg8GZrJtJxT0FkJ3CUWFNg\nn8i1HovzrRJFUJQ1iZ7zKOeSAji0AVLXQFqyNRo6/7DV02jM3yC6q+fur5mq1WR9IhIPPA8Mszet\nBH5rjEl1Y2ynzeOT9Z2Bp5ZuZ+GyVXwW+W9alWbA5PnQaaSnw1LqzBRmw1ePwNrXrI4YYx+3PtDr\nsuZCVcZAaaFVLaXq7FST9dW2imku1tQaV9uvp9jbLjzz8BTA/Rd2Y+uh41y4azrLWs8k6K1r4fr3\noOMIT4emlCVrD8y/HoqyrfaCgFZOj6HWtNQnjlttCMXHrecFGdZU2Gf/Gkb/2TquvohocnCz2pYg\nNhhj+te0zdOacgkCILeolF/N+g6foiwWhz2Ob16qlSQSh3s6NNXS5RyEuZdY39i7X2onguM/Pxbn\nWe0E/qF2g3OolTwCI6D/ddCur6fvQFWjPkoQWSIyBXjbfj0Z0BFe9Sws0JeXbhzEhFnfc+2Jv/Bu\n6KP4zLsGprxvLWKilCfkHYHXx1vJYOrH2omiBantXEy3YHVxPQykAxOBqW6KqUXr0jqUV28ZzLa8\nAKaU/pWy0Fh4cyKk/ODp0FRjVV5uDQQ78hPsWQYp30PmLig6ZtXTn4nCbGsW07x0qzSryaFFqfOK\nciLyO2PMzHqO54w09SomZ9/tzuTmV9dwTutS5vIwXvlHYMoH0GGop0NTnlZSaC2Nmboa8o9aP6aa\nKeS9fK1BYsHR0GkUDLkdwms5oLQ4zyo5HN4M170LnUfX1x2oRuRUVUxnkiAOGGMa1dDl5pQgAL7c\neoQ731zLmLgyXih7CK/8o3DFc9bi5/XRC0Q1PTkHYf511od25zFWn/+QNvZPaysZlJVYJYqCDCt5\nFGTC8VTYtxIw1trpQ++02raq+zsqKYR5V8OBH+DaN6H7uAa9TdVw3JUgDhpj2td8ZMNpbgkC4OON\nh/jt/PVclggzeRKv9A3Q9WK49OnafxNUzcOBH63VzBzFcNUrcNZFp3d+biqsecWap6goG1r3gqHT\nIDzBel2YbVVLFWZbpZO0dda6CH0muuV2VOOgJYgm7t3kg/zx/U1c3COa/+uyBu9vHrN2jPkLDLmj\nxSxe0qKtewM+uQ/C21tjZGK61f1apUXWesqrXoAjLpZq8bcHqo38o9UDSTVrdU4QIpKHtR7DL3YB\ngcaYRvXJ1FwTBMDrP+zn7x/9xNhebXnukij8lv4Rdi21Gg0vf85ay1Y1P2UO+Pwv1od5p9Fw9Vyr\n62h9MMZaG8Fxwhq1HBRpXVtnOm1R6tzN1RhTj6Na1Jm48ZxEysoNj3y8lV+XlzPrurfw3/kxLPkT\nzB5pra8b2QmiOkFkZ+t5THed/K+pMAbyDkP2Xji2z3rM3mu1NWTthrPvhgtn1G9pUQTiBtZ8nGqx\nGlUJQJ3azcM64uPtxd8+3MK0N9bx4g1XENBptFWnnLEDsvdYK2YVZPx8Urdx1vQGEQn1G0x5mdX1\nMSy+fq/b3BXlWNU6WXusf6/svZBlJ4XSwp+PE2/r3yyiozUBXd9rPBezarHq3AbRGDXnKiZn76w5\nwPQPNnNu5yhevnEwgX7eJx9wItf64NnzNax42vp2et4DcO49Z76mblEOrH8DVs+GnANw8b/gnF+f\n2TUbmjHw7TPW7yemO7TtDW36QOseJ08OV5/vd3CVNR31TwuhrNja7uULkR2t0l5kZ6fnHSGsvVb1\nqAbhlkbqxqilJAiABWtT+cP7GxmcGMmcqYMJ9q+mMJibCp/9GbYtgqguMO7puvVnz9pj1YOvnwel\nBZAwzJoHZ9fnMOx3cMHDTaPrbXk5LH4Akl+B6G5w/JC11gCAeFkf1O36Qfuh0H4wtOnt+oO6OB+O\nbrWqgBzFVo+y8PbWY0C49bs4kQsb34G1c61j/UKtkkD3cda/RVh78PL+5bWVakCaIJqpjzakcf+7\nGxnQPpxXbhpMWNApvnHu+tL6YDy2D866xOo/X1pkVWuUFlk/jmLw9rNKGb6B1qNPgFU3vudr64Oy\n90Q4+07rQ7S8DBb/wfqw7XedNUbDnd96y0qtmUH3LLMGfXW/1Pogr+2HbFkpLLwTtrxvLShzwSPW\nt/ucFKva5/AW6zFtrVV9BtZ6BLEDrWThF/zzMVl7cN1/A6sXUFj7n6uN2vWHpFug91XgH1IPvwil\n6o8miGbs003p/Hb+euIjAnnxhiS6tT1Fv4LSE/DdTKt6SLysJOAb9POjt5/1Ieqwk4XjhPXo5Q19\nJ1kfcqFtTr6mMbDiKWupx64XwdWvumeGzbwj8N5UOPC9taLY4U3WgLCgKCvhdb/UKhn5Bro+v6QQ\n3rvJKvFc8DAMv6/69zLGKnmlrrbWMTi4ynq/coe1fGWb3tC2L7TtY1VP+QZD7gGryi3noP14wPpd\nDbxJG4JVo6YJoplbsz+bX89bR0Gxg6cm9uPSvh5YUSt5Lnx6v/Vt+/r3rC6T9eXganj3Rqv944rn\noe/V1jQQu7+C7Z/CzqVQnAs+gVaSOOtiOGustf4AWFU9b02yRgVf9gwk3Xz6MZQWWckzoFX93ZdS\njYAmiBbgyPET3PnmWtYfyOGuUZ154KJueHs1cJvAtk/g/VuseX9iB0BoO6sqK9T+qeiVU9u2CmOs\n+vvFf4SwOLh2nvWNvSpHCaR8a/Xg2vGZ9W0erBjOugS2fwJHt8GVL1rVPEqpSpogWohiRxmPfLyV\nt1YdYETXaJ6fPIDwIL+GDSLlB1j5tFVFk5dufXt3Ftbeaj/oPMZ6dFXScBRb5654Cta/CV0uhKte\nqt0AMWOsBuEdS2DnZ5CabLWjXPsmdL3gzO9PqWZGE0QL8/bqAzz00U+0CfPnxSlJ9Iz1YLVISYHV\nyJ2XDhnbYe83sHeFVSWEWCPA2/axJ5RLs3oVOY/jOO+PMGp63Xv75GdY7S3BUfVxN0o1O5ogWqB1\nB45x15tryS0q5Ymr+jK+f5ynQ/pZmcOa4mHP17B3GWTuhJC2VjVSq1hoZT+26a1TiCjlZpogWqij\neSf4zbz1rN6fzS3DOvLncd3x9a7tGlFKqZbgVAnCrZ8WIjJWRHaIyG4Rme5i/zMissH+2SkiOU77\nypz2LXJnnM1V69AA5t0+lKnnJjLnu31MeXkVGXnFng5LKdVEuK0EISLewE7gQiAVWANMNsZsreb4\ne4ABxphb7Nf5xpjTGlWkJYjqLVyfyvQFm4kI8uOFGwbRv324p0NSSjUCnipBDAF2G2P2GmNKgPnA\n+FMcPxl4243xtGgTBsSz4K5z8fEWrnnhB+Z+t4/mVL2olKp/7kwQccBBp9ep9rZfEJEEoCPwtdPm\nABFJFpEfReRX1b2JiEyzj0vOyMio7jAF9I4L4+PfDGdE12ge+Xgrt72WTFa+VjkppVxrLC2Wk4D3\njTlp5fUEu9hzHTBTRDq7OtEYM9sYk2SMSYqJiWmIWJu0iGA/Xr4piUeu6MXK3Zlc8uxKvtud6emw\nlFKNkDsTRBrgvGZ1vL3NlUlUqV4yxqTZj3uBb4AB9R9iyyQi3HRuIh/+ehihAT5MeWUVjy/ZTmlZ\nuadDU0o1Iu5MEGuAriLSUUT8sJLAL3ojiUh3IAL4wWlbhIj428+jgWGAy8ZtVXc9Y1vxyT0jmDS4\nAy8s38PEF34gJavA02EppRoJtyUIY4wD+A2wFNgGvGuM+UlEZojIFU6HTgLmm5NbTHsAySKyEVgG\nPF5d7yd1ZgL9vPnXlX343/UD2ZeRz7hnV/L+2lRtwFZK6UA59bNDOUXc984GVu3L5rK+7Xj0V31O\nvcaEUqrJ89hAOdW0xIYH8tbtZ/OHi7vx2ZbDXPLsClbtzfJ0WEopD9EEoU7i7SXcPboLC+46Fz8f\nLya99CNPLd1OiUMbsJVqaTRBKJf6tQ/n03tHMHFgPLOW7eHS51aSvD/b02EppRqQJghVrWB/H566\nuh9zpiZRWFLGxBd+4C8LN5NbVOrp0JRSDUAThKrRmO5t+Py+87h1eEfeXn2AC/+znCWb07Wnk1LN\nnCYIVSvB/j787bKefHj3MKJD/Llr3jpuey2ZnUfyPB2aUspNNEGo09I3PpxFvxnGg+O68+PeLC6e\nuYJ73l7P7qOaKJRqbnQchKqz7IISXlq5l9e+309RaRlX9Ivl3vO70jnmtGZpV0p5kK4op9wqK7+Y\n2Sv38vr3KRQ7yvhV/zgeuLgbseGBng5NKVUDTRCqQWTlFzN7xV5e/X4/InDnyM7ccV5nAv28PR2a\nUqoaOpJaNYioEH/+PK4HX/1+JOf3aMPML3dxwX+W88mmQ9rjSakmSBOEqnfxEUHMum4g70w7m7BA\nX37z1nquffFHtqTlejo0pdRp0ASh3GZopyg+vmc4j03ow+6MfC57/ltunLOab3YcpbxcSxRKNXba\nBqEaRG5RKa9/v583fkzhaF4xnWOCmTqsI1cNjCPIz8fT4SnVYmkjtWo0ShzlLN6czivf7mNzWi6t\nAny4dnB7LusbS9/4METE0yEq1aJoglCNjjGGtSnHeOXbfXyx9QiOckNceCAX92rL2N5tGZQQgbeX\nJgul3E0ThGrUcgpL+HLbUT7bks6KXZmUOMqJDvHnsr7tuHNkZ9qGBXg6RKWaLU0QqsnIL3bw9XYr\nWXyx9Qgiwo1nJ3DXqM5Ehfh7Ojylmh1NEKpJOphdyLNf7eKDdakE+npz6/CO3HZeJ1oF6DKoStUX\nTRCqSdt9NI9nvtjFp5vTCQv05Y6RnbjxnERC/LX3k1JnShOEaha2pOXy7893sGxHBq0CfJg6rCM3\nn5tIRLCfp0NTqsny2FQbIjJWRHaIyG4Rme5i/1QRyRCRDfbPbU77bhKRXfbPTe6MUzUNvePCmHvz\nED66exjndI7iua92MeyJr3n0060cOX7C0+Ep1ey4rQQhIt7ATuBCIBVYA0w2xmx1OmYqkGSM+U2V\ncyOBZCAJMMBaYJAx5tip3lNLEC3LziN5/O+bPSzaeAhvEa4cGMc1g9szoH24jqdQqpZOVYJwZyXu\nEGC3MWavHcR8YDyw9ZRnWS4GvjDGZNvnfgGMBd52U6yqCTqrTSjPXNuf+y44i/8t38PC9anMX3OQ\nTjHBTBwUz5UD4rWLrFJnwJ1VTHHAQafXqfa2qq4SkU0i8r6ItD/NcxGRaSKSLCLJGRkZ9RG3amI6\nRAXxryv7sOYvF/DEVX2ICvbjyc92cO7jX3HTnNUs2ZxOmc79pNRp8/RkfR8DicaYvsAXwGunewFj\nzGxjTJIxJikmJqbeA1RNR2iAL9cO7sB7d57LNw+M4u7RXdh1JI+75q3jomeW88G6VBxl5Z4OU6km\nw50JIg1o7/Q63t5WyRiTZYwptl++DAyq7blKnUpidDC/v6gbK/80hv9eNwBfby/uf3cjY/69nPmr\nD1Di0EShVE3cmSDWAF1FpKOI+AGTgEXOB4hIO6eXVwDb7OdLgYtEJEJEIoCL7G1KnRZvL+GyvrEs\nvncEs28YRHiQL9M/2Mzop7/h1e/2kXei1NMhKtVoua2R2hjjEJHfYH2wewNzjDE/icgMINkYswi4\nV0SuABxANjDVPjdbRP6BlWQAZlQ0WCtVF15ewkW92nJhzzYs35nBf7/ezcMfb+XJpTuYMCCOG85J\noHvbVp4OU6lGRQfKqRZr48Ec3vwxhUUbD1HsKGdwYgRTzk7gkt7t8PPxdPOcUg1DR1IrdQo5hSW8\nl5zKm6tSSMkqJCzQlwt6tOGS3m0Z3jWaAF9vT4eolNtoglCqFsrLDSt3Z/LRhjS+3HqE4yccBPt5\nM7p7ay7p3Y5R3WII1vmfVDPjqYFySjUpXl7CyLNiGHlWDCWOcn7Ym8VnW9L5/KcjfLIpHX8fL847\nK4Zxfdpyfo82Oqusava0BKFUDcrKDWv2Z/PZlsN8tuUwh4+fwNdbGNYl2q6GiqFdqwC8dAU81QRp\nFZNS9aS83LAhNYfPthxm8eZ0Uo8VARDo601idDCdYoLpZD8O6RhFXHighyNW6tQ0QSjlBsYYfjp0\nnA0Hc9iXWcDejHz2ZhZwMLuQcgM+XsLVSe25Z0wXYjVRqEZK2yCUcgMRoXdcGL3jwk7aXuIoZ19m\nAfNWpfD26gMsWJvKdUM78OtRnWndSicPVE2HliCUcqPUY4X89+vdvLc2FV9v4YazE7h2cAfaRwbi\n76PdZ5XnaRWTUh6WklXAs1/t4sP1aZQb8BJoFxZIQlSQ/RPMoIQIkhIidC0L1aA0QSjVSOzPLGBt\nyjFSsgs5kFXA/qxCDmQXkl1QAkCHyCB+NSCOKwfEkRgd7OFoVUugCUKpRi63sJSvth/hg3VpfLcn\nE2NgUEIEEwbEMbZ3W6JD/D0domqmNEEo1YSk5xbx4fpDfLAulV1H8wHo3jaUYV2iGd4lmiEdI3VE\nt6o3miCUaoIqutEu35nBt7syWZtyjJKycny8hP7twznvrBhGdYuhd2yYDtJTdaYJQqlm4ERpGcn7\nj/Ht7ky+253JlkO5GAPRIX6MPKs1o7vHMKJLDGFBOgWIqj0dB6FUMxDg683wrtEM7xoNQGZ+MSt2\nZvDNjgy+2n6EBetS8fYSOkYHkxBp9YxKjA6iQ2QQiVHBdIgM0pKGOi2aIJRqoqJD/LlyYDxXDoyn\nrNyw4WAOy3dmsPNwHvuzCvh+TxZFpWWVx7cO9efiXm0Z27stQztG4uOta16oU9MEoVQz4O0lDEqI\nYFBCROU2YwwZecWkZBey52g+y3dm8P7aVN74MYXwIF8u7NGGsb3bMqyLrnmhXNM2CKVakKKSMpbv\nzGDpT4f5ctsR8k448PfxYminKHuq82g6x4ToYL0WRBuplVK/ULHmxTc7jrJiZwZ7MgoAiAsP5Lyz\nojm7UxSDEyN1osFmThOEUqpGqccKWbEzk+U7j/L97izyih2AlTCSEiNISoxkcGIEXVuH4q2N3c2G\nJgil1GlxlJWz/XAea/Znk7z/GGv2Z3M0rxiAAF8vurdtRa/YVvSKDaNXbCu6tQ3VdowmymMJQkTG\nAs8C3sDLxpjHq+y/H7gNcAAZwC3GmBR7Xxmw2T70gDHmipreTxOEUu5hjOFgdhHJKdlsSTvOT4dy\n2Zp+nLwTVinD20s4q00o/duH0S8+nH7tw+naOkR7SjUBHkkQIuIN7AQuBFKBNcBkY8xWp2NGA6uM\nMYUichcwyhhzrb0v3xgTcjrvqQlCqYZjjCH1WBE/HcplS9pxNqXlsvFgDrlFpYC1yl7vuFa0jwyi\nbasA2oYF0DrUemzbKoDWof46LqMR8NRAuSHAbmPMXjuI+cB4oDJBGGOWOR3/IzDFjfEopeqRiNA+\nMoj2kUGM7d0OsJJGSlYhG1Nz2HAwh82pufy4J4ujecU4yk/+Muq8TGvnmBA624892rXSNo5Gwp0J\nIg446PQ6FRh6iuNvBZY4vQ4QkWSs6qfHjTEfujpJRKYB0wA6dOhwRgErpc6MiJAYHUxidDDj+8dV\nbi8vN2QVlHDk+AkO554gPbeIfZmF7M3MZ3NqLks2p1ORP6KC/bigRxsu7t2GczvrGA1PahQD5URk\nCpAEjHTanGCMSRORTsDXIrLZGLOn6rnGmNnAbLCqmBokYKXUafHyEmJC/YkJ9f/FEq0AxY4yUrIK\n2ZZ+nK+2HWXx5nTeST5IsJ83o7q1Zkz31kSF+OHn7YWvjxe+3l74eguBvt4kRAVricNN3Jkg0oD2\nTq/j7W0nEZELgL8AI40xxRXbjTFp9uNeEfkGGAD8IkEopZo+fx9vzmoTylltQhnfP65yjMbSnw7z\nxdYjfLo5vdpzQ/19GJQYweDESAYnRtI3PkxLHfXEnY3UPliN1OdjJYY1wHXGmJ+cjhkAvA+MNcbs\nctoeARQaY4pFJBr4ARjv3MDtijZSK9X8lJcbdh3Np6DEQamjnNIyQ2lZOSVl5RwvKmX9wRzW7Muu\nXDvDz9uLfu3DGJwYyZCOkQxKiCA0QGe4rY4nu7mOA2ZidXOdY4x5VERmAMnGmEUi8iXQB6j4enDA\nGHOFiJwLvAiUA17ATGPMKzW9nyYIpVqu7IISkvdns2Z/Nqv3H2NLWi5l5QYvgZ6xrRiSGEVSYgSd\nYqyZbYP8GkUNu8fpQDmlVItTUOxg/YEcVu/LYvX+bNYfyKHYUV65PzrEn4SoIBIig4iPCCQ8yI+w\nQF/CAn1pZT9GBPsSE+LfrOem0vUglFItTrC/z0nrZxQ7ythxOI+UrEIOZBeSklVASlYhP+zN4vDx\nE1T3XTnYz5uOMcF0jA6hk90tNzEqmLZhAUQF+zXrwYCaIJRSLYK/jzd948PpGx/+i32OsnLyTjjI\nLSrl+IlScousn6z8EvZlFrAvs4ANB4/xyaZDJyUSL7FKIm3sgX9xEYEM6BBOUkIk8RGBTb7koQlC\nKdXi+Xh7ERHsR0Sw3ymPO1FaZpc+Cjly/ARHj5/gyPFijuSdID33BKv2ZfP6DymAtUBTUmIEgxIi\n6dmuFX4+greXF94ieHlZ05ME+nrTNiwAf5/G2etKE4RSStVSgO/P3XFdKSs37Dicx9qUbJJTjpG8\n/xiLNx+u8boVpY+48EDiIgKJDQskKsSPqGB/+9GP8CC/Bh/voY3USinlRodzT7D7aD6O8nLKjaGs\n3Eok5cZQUOzgUM4J0nIKST1WRFpOEYdyiigt++XnspdAiL8Pvt5eeHtJ5aOPlxAd6s+7d5xTp/i0\nkVoppTykbZg1QWFtVUxLkl1QQlZ+MVlOj3knHDjKy3GUGRzlhrJya0xIiL97Pso1QSilVCPiPC0J\nuK7KarBYPPruSimlGi1NEEoppVzSBKGUUsolTRBKKaVc0gShlFLKJU0QSimlXNIEoZRSyiVNEEop\npVxqVlNtiEgGkFLH06OBzHoMpzFrSfcKer/NXUu6X3fca4IxJsbVjmaVIM6EiCRXNx9Jc9OS7hX0\nfpu7lnS/DX2vWsWklFLKJU0QSimlXNIE8bPZng6gAbWkewW93+auJd1vg96rtkEopZRySUsQSiml\nXNIEoZRSyqUWnyBEZKyI7BCR3SIy3dPx1DcRmSMiR0Vki9O2SBH5QkR22Y8RnoyxPolIexFZJiJb\nReQnEfmtvb3Z3bOIBIjIahHZaN/rI/b2jiKyyv6bfkdE/Dwda30SEW8RWS8in9ivm+39ish+Edks\nIhtEJNne1mB/yy06QYiINzALuAToCUwWkZ6ejarevQqMrbJtOvCVMaYr8JX9urlwAL83xvQEzgbu\ntv9Nm+M9FwNjjDH9gP7AWBE5G3gCeMYY0wU4BtzqwRjd4bfANqfXzf1+Rxtj+juNf2iwv+UWnSCA\nIcBuY8xeY0wJMB8Y7+GY6pUxZgWQXWXzeOA1+/lrwK8aNCg3MsakG2PW2c/zsD5I4miG92ws+fZL\nX/vHAGOA9+3tzeJeK4hIPHAp8LL9WmjG91uNBvtbbukJIg446PQ61d7W3LUxxqTbzw8DbTwZjLuI\nSCIwAFhFM71nu7plA3AU+ALYA+QYYxz2Ic3tb3om8Eeg3H4dRfO+XwN8LiJrRWSava3B/pZ93HVh\n1TQYY4yINLu+ziISAiwAfmeMOW590bQ0p3s2xpQB/UUkHFgIdPdwSG4jIpcBR40xa0VklKfjaSDD\njTFpItIa+EJEtjvvdPffcksvQaQB7Z1ex9vbmrsjItIOwH486uF46pWI+GIlh3nGmA/szc36no0x\nOcAy4BwgXEQqvvw1p7/pYcAVIrIfqzp4DPAszfd+Mcak2Y9Hsb4ADKEB/5ZbeoJYA3S1e0H4AZOA\nRR6OqSEsAm6yn98EfOTBWOqVXSf9CrDNGPMfp13N7p5FJMYuOSAigcCFWG0uy4CJ9mHN4l4BjDF/\nNsbEG2MSsf6vfm2MuZ5mer8iEiwioRXPgYuALTTg33KLH0ktIuOw6jW9gTnGmEc9HFK9EpG3gVFY\n0wQfAR4CPgTeBTpgTY9+jTGmakN2kyQiw4GVwGZ+rqd+EKsdolnds4j0xWqk9Mb6sveuMWaGiHTC\n+oYdCawHphhjij0Xaf2zq5geMMZc1lzv176vhfZLH+AtY8yjIhJFA/0tt/gEoZRSyrWWXsWklFKq\nGpoglFJKuaQJQimllEuaIJRSSrmkCUIppZRLmiBUkyAiRkT+7fT6ARF5uJ6u/aqITKz5yDN+n6tF\nZJuILKuyPVZE3ref97e7XtfXe4aLyK9dvZdSNdEEoZqKYuBKEYn2dCDOnEbw1satwO3GmNHOG40x\nh4wxFQmqP3BaCaKGGMKBygRR5b2UOiVNEKqpcGCtx3tf1R1VSwAikm8/jhKR5SLykYjsFZHHReR6\new2FzSLS2ekyF4hIsojstOf8qZgI7ykRWSMim0TkDqfrrhSRRcBWF/FMtq+/RUSesLf9HRgOvCIi\nT1U5PtE+1g+YAVxrz/9/rT2ado4d83oRGW+fM1VEFonI18BXIhIiIl+JyDr7vStmJX4c6Gxf76mK\n97KvESAic+3j14vIaKdrfyAin4m15sCTp/2vpZoFnaxPNSWzgE2n+YHVD+iBNeX5XuBlY8wQsRYS\nugf4nX1cItY8N52BZSLSBbgRyDXG/H97d+xSVRjGcfz7uOQSLbY0VASJRIWl0hIFDY1RREMEEU1F\n2eweuAVBEERBEDnkPxCakRFBWQlqgdJQtEZBRZBZPg3Pc+H1cEquttz6fbbjveec517kPOc9L/f3\n9pnZGuCxmY3m+3cD+S2RJQAAAjdJREFU2939TXkyM9tArE/QQ6xNMGpmh/MXzgeIX/8+ryvU3b9n\nI+l19/N5vEEiUuJ0xmpMmNlYUcNOd/+Yo4gjGUzYATzJBjaQdXbn8TYXpzwXp/UdZtaVtXbma91E\nEu48MGdmV9y9TD6W/4BGENIy3P0zcAu40MRuz3KNiHkiCrtxgZ8hmkLDsLsvuvtropF0Edk3Jy3i\ntJ8S0dJb8/0T1eaQ+oBxd3+fEdRDwL4m6q06CAxkDeNAOxGxAHCviFgwYNDMpoExIvJ6uRjovcBt\nAHefJWIbGg3ivrt/cvdvxChp0yo+g7QojSCk1VwGJoGbxd9+kDc7ZtYGlEtOlpk8i8X2Ikv//6uZ\nM05cdPvdfaR8IXOAvq6s/KYZcNTd5yo17KnUcAJYD/S4+4JF4mn7Ks5bfm8/0bXiv6QRhLSUvGMe\nZumykm+JRzoAh4iV1Zp1zMzacl5iCzAHjABnLeLDMbPOTNX8kwlgv5l1WCxpexx42EQdX4C1xfYI\n0G8WC1qY2a7f7LeOWCthIecSGnf81eOVHhGNhXy0tJH43CKAGoS0pktEOm3DdeKiPEWsh7CSu/t3\nxMX9LnAmH63cIB6vTObE7jWWuZPOlb4GiAjqKeCFuzcTx/wA2NaYpAYuEg1v2sxe5XadIaDXzGaI\nuZPZrOcDMXfysjo5DlwF2nKfO8CpfyEFVf4epbmKiEgtjSBERKSWGoSIiNRSgxARkVpqECIiUksN\nQkREaqlBiIhILTUIERGp9QvY5lz9WsbzDwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTvxmZlXGid/MrMo48ZuZVZn/B3wdEapT/sc9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 34.292118758153904 seconds\n",
            "Best accuracy: 74.08  Best training loss: 0.15568388721346854  Best validation loss: 0.7924620547890665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15eacef1-0058-48de-af6b-c42d420d3ec0",
        "id": "QEUkgpZTt4DC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
            "[1.8180818259716034, 1.4572993639707565, 1.2719646226167678, 1.1604638813734054, 1.0699988037347794, 0.9980746896266938, 0.9355610894560814, 0.882878409564495, 0.8338646187782287, 0.793876740694046, 0.7541562731266022, 0.7177694869935513, 0.6819172051548957, 0.6501521447002888, 0.6248666300475597, 0.5958057281374931, 0.573871077939868, 0.5576381465196609, 0.5314415907114745, 0.5074381946772337, 0.48857979716360567, 0.46418102568387987, 0.4433420429080725, 0.4271464377641678, 0.4108445409461856, 0.4032571386024356, 0.3775452640801668, 0.3706348918005824, 0.3522555408105254, 0.33906378839164975, 0.3246300146356225, 0.3176314008533955, 0.30148927933722736, 0.2874620287418366, 0.27571548099070786, 0.26615336583927274, 0.2555813882499933, 0.25032045352458954, 0.24266249246522784, 0.22688937573507428, 0.2257178712412715, 0.2137910726778209, 0.2017316782362759, 0.19490773535706102, 0.1916315761320293, 0.18409708750620485, 0.17903121000900865, 0.1674568880479783, 0.1668830525856465, 0.16076305632665752, 0.15653469883091747, 0.15568388721346854]\n",
            "[1.532898788452149, 1.28628457069397, 1.2432439178228376, 1.0815348118543628, 1.0245870190858841, 1.001742744445801, 0.9543958294391632, 0.9037312316894536, 0.8720895239710809, 0.8648973527550696, 0.8344543573260303, 0.8364363476634026, 0.8348170527815817, 0.8567454895377161, 0.8113250833749771, 0.8061066842079161, 0.7924620547890665, 0.7979561829566956, 0.8375952523946761, 0.8219047036767007, 0.8068324905633925, 0.8535580223798753, 0.8443241953849793, 0.833060807585716, 0.8424094116687775, 0.8361525970697404, 0.8564375358819959, 0.8636116054654116, 0.8706138691306111, 0.9117623233795168, 0.9164288830757138, 0.9120613721013068, 0.8904806303977963, 0.894193086326122, 0.9471503749489788, 0.944168705046177, 0.9470449614524837, 0.9777941042184831, 0.9748282599449157, 0.9787426552176475, 0.9677919742465023, 0.9947181347012524, 0.9989621618390081, 1.096989844739437, 1.0277900150418284, 1.0540591537952422, 1.0891827976703643, 1.0870578917860987, 1.1012084904313089, 1.0578787404298784, 1.0958140006661417, 1.0800278621912]\n",
            "[46.0, 54.4, 55.92, 62.32, 63.5, 64.28, 66.14, 69.14, 69.2, 70.12, 71.04, 71.16, 71.22, 70.88, 72.44, 72.82, 73.12, 73.8, 72.74, 72.64, 73.5, 72.8, 72.74, 73.1, 72.44, 74.04, 73.72, 73.58, 73.48, 72.84, 73.5, 73.28, 74.08, 73.62, 73.56, 72.84, 73.44, 73.12, 72.94, 73.28, 73.26, 73.14, 74.0, 72.12, 73.16, 73.28, 72.94, 72.96, 73.34, 73.76, 73.74, 73.06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ys8HGi-i1jN9"
      },
      "source": [
        "## squeeze net (batch normed) (3x3 ratio 0.75)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SsppzcuK1jOA",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 32,96),\n",
        "                Fire(128, 16, 32,96),\n",
        "                Fire(128, 32, 64,192),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 64,192),\n",
        "                Fire(256, 48, 96,288),\n",
        "                Fire(384, 48, 96,288),\n",
        "                Fire(384, 64, 128,384),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 128,384),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FtGuNUML1jOD",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GlByCFM91jOG",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e27dae1b-4cbf-41b2-d8c3-e28d924130e5",
        "id": "y0C3Sf1Q1jOI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.219947\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.958025\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.863865\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.772912\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.728081\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.688621\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.626961\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.592084\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.562367\n",
            "Total train loss: 1.7530\n",
            "\n",
            "Test set: Test loss: 1.4862, Accuracy: 2405/5000 (48%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 48.1%\n",
            "Better loss at Epoch 0: loss = 1.4862018430233004%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.483810\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.475003\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.454590\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.449992\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.398392\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.404679\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.374674\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.363828\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.338126\n",
            "Total train loss: 1.4054\n",
            "\n",
            "Test set: Test loss: 1.2782, Accuracy: 2749/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 54.98%\n",
            "Better loss at Epoch 1: loss = 1.2782037156820303%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.283582\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.253839\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.248822\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.248938\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.244836\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.220628\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.193213\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.203477\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.212028\n",
            "Total train loss: 1.2263\n",
            "\n",
            "Test set: Test loss: 1.1508, Accuracy: 2972/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 59.44%\n",
            "Better loss at Epoch 2: loss = 1.1507763636112212%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.136783\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.153244\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.113896\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.123121\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.093386\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.094671\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.116840\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.117819\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.089131\n",
            "Total train loss: 1.1121\n",
            "\n",
            "Test set: Test loss: 1.0579, Accuracy: 3152/5000 (63%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 63.04%\n",
            "Better loss at Epoch 3: loss = 1.05793753683567%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.014357\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.039282\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.034485\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.980666\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.027865\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.039939\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.019138\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.025914\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.981485\n",
            "Total train loss: 1.0175\n",
            "\n",
            "Test set: Test loss: 1.0144, Accuracy: 3225/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 64.5%\n",
            "Better loss at Epoch 4: loss = 1.0144024962186815%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.955689\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.940299\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.958364\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.938966\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.957851\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.918167\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.947592\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.950116\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.960652\n",
            "Total train loss: 0.9446\n",
            "\n",
            "Test set: Test loss: 1.0121, Accuracy: 3256/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 65.12%\n",
            "Better loss at Epoch 5: loss = 1.0120739215612413%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.872976\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.872270\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.858895\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.888328\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.901991\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.909000\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.879816\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.864297\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.883931\n",
            "Total train loss: 0.8791\n",
            "\n",
            "Test set: Test loss: 0.9529, Accuracy: 3374/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 67.48%\n",
            "Better loss at Epoch 6: loss = 0.9528959494829178%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.805715\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.808455\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.825802\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.829758\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.826053\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.841315\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.850658\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.814971\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.848497\n",
            "Total train loss: 0.8257\n",
            "\n",
            "Test set: Test loss: 0.8893, Accuracy: 3462/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 69.24%\n",
            "Better loss at Epoch 7: loss = 0.8893240210413932%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.754936\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.765250\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.753875\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.781326\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.791260\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.764228\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.750418\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.772958\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.781890\n",
            "Total train loss: 0.7700\n",
            "\n",
            "Test set: Test loss: 0.8791, Accuracy: 3475/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 69.5%\n",
            "Better loss at Epoch 8: loss = 0.8790674993395804%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.681899\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.671411\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.729425\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.717706\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.720614\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.733656\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.750545\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.752910\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.742356\n",
            "Total train loss: 0.7218\n",
            "\n",
            "Test set: Test loss: 0.8739, Accuracy: 3473/5000 (69%)\n",
            "\n",
            "Better loss at Epoch 9: loss = 0.8738784733414651%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.683994\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.655172\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.653996\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.691603\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.660596\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.676237\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.699361\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.700997\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.710650\n",
            "Total train loss: 0.6816\n",
            "\n",
            "Test set: Test loss: 0.8492, Accuracy: 3548/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 70.96%\n",
            "Better loss at Epoch 10: loss = 0.849186798930168%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.587157\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.608430\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.634548\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.633635\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.650858\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.662081\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.650002\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.667636\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.680018\n",
            "Total train loss: 0.6451\n",
            "\n",
            "Test set: Test loss: 0.8394, Accuracy: 3547/5000 (71%)\n",
            "\n",
            "Better loss at Epoch 11: loss = 0.8394116374850273%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.586231\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.594719\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.592366\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.612388\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.604902\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.598752\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.626603\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.631982\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.621004\n",
            "Total train loss: 0.6110\n",
            "\n",
            "Test set: Test loss: 0.8969, Accuracy: 3486/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.539461\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.559998\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.562082\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.571358\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.572179\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.576626\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.599077\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.589546\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.600346\n",
            "Total train loss: 0.5798\n",
            "\n",
            "Test set: Test loss: 0.8567, Accuracy: 3583/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 71.66%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.501870\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.519573\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.515402\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.544255\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.545378\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.553671\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.541967\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.541926\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.562592\n",
            "Total train loss: 0.5389\n",
            "\n",
            "Test set: Test loss: 0.9167, Accuracy: 3537/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.486039\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.476544\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.482882\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.518268\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.539840\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.520326\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.518967\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.546744\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.537117\n",
            "Total train loss: 0.5155\n",
            "\n",
            "Test set: Test loss: 0.8723, Accuracy: 3576/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.449794\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.449618\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.458750\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.505544\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.475391\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.474554\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.486123\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.488294\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.523018\n",
            "Total train loss: 0.4832\n",
            "\n",
            "Test set: Test loss: 0.9081, Accuracy: 3563/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.404197\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.427337\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.461778\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.450325\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.483165\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.479368\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.468073\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.475990\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.510934\n",
            "Total train loss: 0.4661\n",
            "\n",
            "Test set: Test loss: 0.9149, Accuracy: 3577/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.365877\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.406989\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.428240\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.431862\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.443277\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.442695\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.451290\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.445352\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.450389\n",
            "Total train loss: 0.4338\n",
            "\n",
            "Test set: Test loss: 0.8581, Accuracy: 3626/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 72.52%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.364406\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.370936\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.400258\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.406073\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.393532\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.417936\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.427236\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.450685\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.441895\n",
            "Total train loss: 0.4103\n",
            "\n",
            "Test set: Test loss: 0.9005, Accuracy: 3623/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.339797\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.345917\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.369047\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.375996\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.403561\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.397210\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.422333\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.426989\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.403266\n",
            "Total train loss: 0.3907\n",
            "\n",
            "Test set: Test loss: 0.9031, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 20: accuracy = 72.82%\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.314031\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.320879\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.346942\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.347993\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.349929\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.363031\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.406053\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.390336\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.409983\n",
            "Total train loss: 0.3633\n",
            "\n",
            "Test set: Test loss: 0.9225, Accuracy: 3634/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.303109\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.301331\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.329693\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.348241\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.350295\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.362485\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.360818\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.366969\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.386465\n",
            "Total train loss: 0.3494\n",
            "\n",
            "Test set: Test loss: 0.9205, Accuracy: 3608/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.266230\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.284485\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.318995\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.307292\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.315899\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.342358\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.346069\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.354493\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.376032\n",
            "Total train loss: 0.3261\n",
            "\n",
            "Test set: Test loss: 0.9666, Accuracy: 3635/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.254212\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.273993\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.281623\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.314588\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.308817\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.338235\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.330271\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.355958\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.377450\n",
            "Total train loss: 0.3154\n",
            "\n",
            "Test set: Test loss: 1.0022, Accuracy: 3621/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.249221\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.257452\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.268208\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.287622\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.293533\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.320469\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.307691\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.332559\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.324032\n",
            "Total train loss: 0.2964\n",
            "\n",
            "Test set: Test loss: 0.9817, Accuracy: 3615/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.232202\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.255330\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.237363\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.265726\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.262279\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.279450\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.314071\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.301222\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.281107\n",
            "Total train loss: 0.2735\n",
            "\n",
            "Test set: Test loss: 1.0536, Accuracy: 3575/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.258173\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.244456\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.266758\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.243946\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.245978\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.277375\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.262759\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.275042\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.285843\n",
            "Total train loss: 0.2643\n",
            "\n",
            "Test set: Test loss: 1.0452, Accuracy: 3643/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 27: accuracy = 72.86%\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.187169\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.220362\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.234302\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.231202\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.256783\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.251956\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.247904\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.249247\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.286137\n",
            "Total train loss: 0.2467\n",
            "\n",
            "Test set: Test loss: 1.0136, Accuracy: 3595/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.238064\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.207091\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.230349\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.223120\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.233983\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.237558\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.249018\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.269770\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.251976\n",
            "Total train loss: 0.2399\n",
            "\n",
            "Test set: Test loss: 1.0742, Accuracy: 3608/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.186159\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.214204\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.226519\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.221786\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.234149\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.233362\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.255707\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.240883\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.265455\n",
            "Total train loss: 0.2328\n",
            "\n",
            "Test set: Test loss: 1.0232, Accuracy: 3634/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.164627\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.161037\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.174292\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.187808\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.202486\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.241368\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.225159\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.226223\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.218276\n",
            "Total train loss: 0.2028\n",
            "\n",
            "Test set: Test loss: 1.0947, Accuracy: 3647/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 31: accuracy = 72.94%\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.176094\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.163244\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.184243\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.191209\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.207712\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.221228\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.235817\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.212904\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.223814\n",
            "Total train loss: 0.2056\n",
            "\n",
            "Test set: Test loss: 1.0997, Accuracy: 3633/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.151815\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.149508\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.175804\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.186968\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.216029\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.212999\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.230958\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.186854\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.220915\n",
            "Total train loss: 0.1947\n",
            "\n",
            "Test set: Test loss: 1.1089, Accuracy: 3630/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.154499\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.167872\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.185322\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.175439\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.172536\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.183628\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.215305\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.198993\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.176982\n",
            "Total train loss: 0.1815\n",
            "\n",
            "Test set: Test loss: 1.1349, Accuracy: 3608/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.128264\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.142576\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.153796\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.162826\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.178348\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.204000\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.190825\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.202160\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.210857\n",
            "Total train loss: 0.1777\n",
            "\n",
            "Test set: Test loss: 1.1572, Accuracy: 3590/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.135408\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.136443\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.135677\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.163294\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.165180\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.185892\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.167213\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.168474\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.178851\n",
            "Total train loss: 0.1608\n",
            "\n",
            "Test set: Test loss: 1.1896, Accuracy: 3583/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.131054\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.144520\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.144062\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.145847\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.157429\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.178204\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.164353\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.195283\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.188781\n",
            "Total train loss: 0.1611\n",
            "\n",
            "Test set: Test loss: 1.1253, Accuracy: 3588/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.107962\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.123148\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.148206\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.140542\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.123480\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.159089\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.161357\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.172241\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.172403\n",
            "Total train loss: 0.1480\n",
            "\n",
            "Test set: Test loss: 1.2085, Accuracy: 3539/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.144833\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.117466\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.138980\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.141657\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.149283\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.159102\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.153393\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.158031\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.165192\n",
            "Total train loss: 0.1499\n",
            "\n",
            "Test set: Test loss: 1.1728, Accuracy: 3626/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.115640\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.115012\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.122388\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.135408\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.152632\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.153995\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.132246\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.141122\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.148181\n",
            "Total train loss: 0.1374\n",
            "\n",
            "Test set: Test loss: 1.2230, Accuracy: 3610/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.107177\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.096787\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.135681\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.134920\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.138937\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.134710\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.123501\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.144121\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.159456\n",
            "Total train loss: 0.1315\n",
            "\n",
            "Test set: Test loss: 1.2072, Accuracy: 3651/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 41: accuracy = 73.02%\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.097979\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.114749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e8e76cf8595a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Calculating gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "70d76163-9d3b-4df5-fd5b-a47e6e2a7ef9",
        "id": "gBQOyexZ1jOL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3hUZfbA8e9JD6mEBAIkEDoECC1S\npIsCYkEUEZBVWRRxbWvZ1XX9ra6ru+q6igULKuhaQNRVEURsCCggAiK910AgBRJSSJnk/f3xDjFA\nSAJkMknmfJ4nTzJ33nvnzBXvufetYoxBKaWU5/JydwBKKaXcSxOBUkp5OE0ESinl4TQRKKWUh9NE\noJRSHk4TgVJKeThNBEq5iYj8KCLdzvMYzUQkW0S8q7JsJY71log87vw7QUSWne8xlftoIlBVRkT6\nicgyEckUkSPOC90F7o6ruonI9yJycwVlrgCyjDG/lNoWLyJznecvS0QWiciF5R3HGLPPGBNsjCmq\nKK6zKXs2jDHrgAznd1K1kCYCVSVEJBSYB7wIRABNgb8D+e6MqwabArxz4oWItAJ+BNYDLYAmwCfA\nVyLSp6wDiIhPNcRZWe8Bt7o7CHWOjDH6oz/n/QMkAhnlvO8NPAOkAbuA2wED+Djf3wNcXKr8o8C7\npV73BpYBGcCvwKBS74UBbwLJwAHgccDb+d6vQHapH3Ni3wqO+T3wD+zFOQv4CoisKB7gCaAIyHN+\n3ktlnAs/4DgQU2rbO8AXZZR9BVji/DvOGf8kYB+wpNS2E+exhXN7FvANMO3EeSyjbEXf8UPgEJDp\nPGbHUu+9BTxe6nVT53fyd/e/Rf05+x99IlBVZRtQJCJvi8ilIlL/lPdvAS4HumGTxujKHlhEmgLz\nsRf4COB+4GMRiXIWeQtwAK2dxx8K3AxgjOlibHVIMHAvsBVYU4ljAowHJgINsRfv+yuKxxjzV2Ap\ncIfzc+8o4yu1AYqNMUmltl2CvfCeag7QV0QCS20bCHQAhpVR/n1gJdAAm0x/V0aZ0sr8jk4LnLE2\nBNZg7/rLZIw5ABQC7Sr4PFUDaSJQVcIYcwzoh73jfB1IddZ3N3IWGQNMNcbsN8YcAf51FoefgL1b\n/sIYU2yM+RpYBYxwHn8E8EdjTI4xJgV4Dhhb+gAi0g974b7SGesZj1lqt5nGmG3GmOPYC3LXiuKp\n5PcJx96BlxaJfaI5VTL2/9OIUtsedX7X46d8x2bABcDfjDEFxpgfgLkVxHKm74gxZoYxJssYk49N\nKl1EJKycY2U5v5uqZTQRqCpjjNlsjLnJGBMDdMLWc091vt0E2F+q+N6zOHRz4FoRyTjxg006jZ3v\n+QLJpd57DXsXC4CIxGIvcjcaY7ZV4pgnHCr1dy4QfBb7lucoEHLKtrQz7N8YKHbuc8L+MsqBPcdH\njDG5lSh7QpnfUUS8ReRJEdkpIsewVXdgE9aZhGCrylQtU5Mam1QdYozZIiJv8VsDYjIQW6pIs1N2\nyQHqlXodXerv/cA7xphbTv0cEWmMbZCONMY4yng/EPgU+zSyoDLHrISK9q1oSt8dNjRp6qxSAVuf\nfy0w85SyY4DlxphcEano+MlAhIjUK5UMYs9QtiLjgZHAxdgkEIZNRlJWYWd1mR+26k3VMvpEoKqE\niLQXkftEJMb5OhYYB6xwFpkD3CUiMc72gwdPOcRaYKyI+IrIqW0I7wJXiMgw551qgIgMEpEYY0wy\ntpHzPyISKiJeItJKRAY6950BbDHGPH3K553xmJX4uhXtexhoeaadjTEF2Av/wFKb/w5cKCJPiEiE\niISIyJ3ADcADlYgJY8xebBXVoyLi5+xtdK5dOkOwCTYdm6D/WUH5gcB3zmokVctoIlBVJQvoBfwk\nIjnYBLABuM/5/uvAQmwPmzXA/07Z//+AVti7zr9jGz0BMMbsx96dPgSkYu/I/8Rv/35vwN6NbnLu\n/xG/VbOMBUY5B1Kd+OlfiWOeUSX2fR4YLSJHReSFMxzmNUo15BpjtmOrl7pg78CTgWuAYcaYHyuK\nqZTrgT7YC/jjwAecWxfe/2Kr7w5gz+uK8otzPfDqOXyOqgHEGF2YRlU/EYkDdgO+ZVXpeAIR+RHb\nu+iXCguf+2d8gH0iesSFn5EAvGaMKXO8g6r5NBEot9BE4BrOkdxHsOd2KLZ9pI8rk42q/bSxWKm6\nJRpb7dYASAJu0ySgKqJPBEop5eG0sVgppTxcrasaioyMNHFxce4OQymlapXVq1enGWOiynrPZYlA\nRGZg55ZJMcZ0KuP9MGx/7GbOOJ4xxpw6mOY0cXFxrFq1qqrDVUqpOk1Ezjia35VVQ28Bw8t5/3Zg\nkzGmCzAIOyDIz4XxKKWUKoPLEoExZgm2G9sZiwAhYsfNBzvLajdCpZSqZu5sI3gJOzPiQexw9uuM\nMcVujEcppTySOxPBMOz8Mhdhpxb4WkSWOqcIPomITAYmAzRrdupcZUopVyosLCQpKYm8vDx3h6Iq\nISAggJiYGHx9fSu9jzsTwUTgSWMHMuwQkd1Ae+yiGicxxkwHpgMkJibqwAelqlFSUhIhISHExcVR\nagZUVQMZY0hPTycpKYkWLVpUej93jiPYBwwBcC4u0g67hKFSqgbJy8ujQYMGmgRqARGhQYMGZ/30\n5sruo7OwvYEiRSQJeAS7gAjGmFexa6W+JSLrsXOcP2CMSXNVPEqpc6dJoPY4l/9WLksExphxFbx/\nEDspVrXYcugYn/xygDsGtyYkoPJ1Z0opVdd5zBQTSUeO89riXWw7nO3uUJRSZyE9PZ2uXbvStWtX\noqOjadq0acnrgoKCSh1j4sSJbN1a/uJp06ZN47333quKkOnXrx9r166tkmNVh1o3xcS5ahdtl4jd\neiiLHs3ruzkapVRlNWjQoOSi+uijjxIcHMz9999/UhljDMYYvLzKvredObPCSQu4/fbbzz/YWspj\nnghi6gcS5OfN1kOn9U5VStVCO3bsID4+nuuvv56OHTuSnJzM5MmTSUxMpGPHjjz22GMlZU/coTsc\nDsLDw3nwwQfp0qULffr0ISUlBYCHH36YqVOnlpR/8MEH6dmzJ+3atWPZsmUA5OTkcM011xAfH8/o\n0aNJTEys8M7/3XffpXPnznTq1ImHHnoIAIfDwe9+97uS7S+8YBeye+6554iPjychIYEJEyZU+Tk7\nE495IhAR2kaHsOVQlrtDUarW+vvnG9l0sGpvpuKbhPLIFR3Pad8tW7bw3//+l8TERACefPJJIiIi\ncDgcDB48mNGjRxMfH3/SPpmZmQwcOJAnn3ySe++9lxkzZvDgg6cuoW2fMlauXMncuXN57LHH+PLL\nL3nxxReJjo7m448/5tdff6V79+7lxpeUlMTDDz/MqlWrCAsL4+KLL2bevHlERUWRlpbG+vXrAcjI\nyADg6aefZu/evfj5+ZVsqw4e80QA0D46hK2Hs9A1GJSqG1q1alWSBABmzZpF9+7d6d69O5s3b2bT\npk2n7RMYGMill14KQI8ePdizZ0+Zx7766qtPK/PDDz8wduxYALp06ULHjuUnsJ9++omLLrqIyMhI\nfH19GT9+PEuWLKF169Zs3bqVu+66i4ULFxIWFgZAx44dmTBhAu+9995ZDQg7Xx7zRADQrlEIs1bu\nJyUrn0ahAe4OR6la51zv3F0lKCio5O/t27fz/PPPs3LlSsLDw5kwYUKZ/en9/H6b29Lb2xuHo+wp\nzvz9/Sssc64aNGjAunXrWLBgAdOmTePjjz9m+vTpLFy4kMWLFzN37lz++c9/sm7dOry9vav0s8vi\nUU8E7aJDAdtgrJSqW44dO0ZISAihoaEkJyezcOHCKv+Mvn37MmfOHADWr19f5hNHab169WLRokWk\np6fjcDiYPXs2AwcOJDU1FWMM1157LY899hhr1qyhqKiIpKQkLrroIp5++mnS0tLIzc2t8u9QFo96\nImhfqufQgLZlrs+glKqlunfvTnx8PO3bt6d58+b07du3yj/jzjvv5IYbbiA+Pr7k50S1TlliYmL4\nxz/+waBBgzDGcMUVV3DZZZexZs0aJk2ahDEGEeGpp57C4XAwfvx4srKyKC4u5v777yckJKTKv0NZ\nat2axYmJieZ8Fqbp+cQ39G8TxX/GdKnCqJSquzZv3kyHDh3cHUaN4HA4cDgcBAQEsH37doYOHcr2\n7dvx8alZ99Rl/TcTkdXGmMSyytes6KtBu+gQth7WLqRKqbOXnZ3NkCFDcDgcGGN47bXXalwSOBe1\n/xucpXaNQnhnxV6Kig3eXjp/ilKq8sLDw1m9erW7w6hyHtVYDPaJIN9RzJ70HHeHopRSNYLHJYL2\n2nNIKaVO4nGJoE2jYLxEE4FSSp3gcYkgwNebuAZBmgiUUsrJ4xIBQNtGdqoJpVTNN3jw4NMGh02d\nOpXbbrut3P2Cg4MBOHjwIKNHjy6zzKBBg6ioO/rUqVNPGtg1YsSIKpkH6NFHH+WZZ5457+NUBZcl\nAhGZISIpIrKhnDKDRGStiGwUkcWuiuVU7aJD2JOew/GCour6SKXUORo3bhyzZ88+advs2bMZN67c\nta9KNGnShI8++uicP//URPDFF18QHh5+zseriVz5RPAWMPxMb4pIOPAycKUxpiNwrQtjOUn76BCM\nge0p+lSgVE03evRo5s+fX7IIzZ49ezh48CD9+/cv6dffvXt3OnfuzGeffXba/nv27KFTp04AHD9+\nnLFjx9KhQwdGjRrF8ePHS8rddtttJVNYP/LIIwC88MILHDx4kMGDBzN48GAA4uLiSEuzq+o+++yz\ndOrUiU6dOpVMYb1nzx46dOjALbfcQseOHRk6dOhJn1OWtWvX0rt3bxISEhg1ahRHjx4t+fwT01Kf\nmOxu8eLFJQvzdOvWjays87+OuXKpyiUiEldOkfHA/4wx+5zlU1wVy6lOLFKz5VAWCTF1K7Mr5VIL\nHoRD66v2mNGd4dInz/h2REQEPXv2ZMGCBYwcOZLZs2czZswYRISAgAA++eQTQkNDSUtLo3fv3lx5\n5ZVnXLf3lVdeoV69emzevJl169adNI30E088QUREBEVFRQwZMoR169Zx11138eyzz7Jo0SIiIyNP\nOtbq1auZOXMmP/30E8YYevXqxcCBA6lfvz7bt29n1qxZvP7664wZM4aPP/643PUFbrjhBl588UUG\nDhzI3/72N/7+978zdepUnnzySXbv3o2/v39JddQzzzzDtGnT6Nu3L9nZ2QQEnP8Emu5sI2gL1BeR\n70VktYjccKaCIjJZRFaJyKrU1NTz/uDmDYII8PXSBmOlaonS1UOlq4WMMTz00EMkJCRw8cUXc+DA\nAQ4fPnzG4yxZsqTkgpyQkEBCQkLJe3PmzKF79+5069aNjRs3Vjih3A8//MCoUaMICgoiODiYq6++\nmqVLlwLQokULunbtCpQ/1TXY9REyMjIYOHAgADfeeCNLliwpifH666/n3XffLRnB3LdvX+69915e\neOEFMjIyqmRksztHFvsAPYAhQCCwXERWGGO2nVrQGDMdmA52rqHz/WBvL6FNwxC2aYOxUmennDt3\nVxo5ciT33HMPa9asITc3lx49egDw3nvvkZqayurVq/H19SUuLq7Mqacrsnv3bp555hl+/vln6tev\nz0033XROxznhxBTWYKexrqhq6Ezmz5/PkiVL+Pzzz3niiSdYv349Dz74IJdddhlffPEFffv2ZeHC\nhbRv3/6cYwX3PhEkAQuNMTnGmDRgCVBtM8G109XKlKo1goODGTx4ML///e9PaiTOzMykYcOG+Pr6\nsmjRIvbu3VvucQYMGMD7778PwIYNG1i3bh1gp7AOCgoiLCyMw4cPs2DBgpJ9QkJCyqyH79+/P59+\n+im5ubnk5OTwySef0L9//7P+bmFhYdSvX7/kaeKdd95h4MCBFBcXs3//fgYPHsxTTz1FZmYm2dnZ\n7Ny5k86dO/PAAw9wwQUXsGXLlrP+zFO584ngM+AlEfEB/IBewHPV9eHtGoXw0eokjuQUEBHkV/EO\nSim3GjduHKNGjTqpB9H111/PFVdcQefOnUlMTKzwzvi2225j4sSJdOjQgQ4dOpQ8WXTp0oVu3brR\nvn17YmNjT5rCevLkyQwfPpwmTZqwaNGiku3du3fnpptuomfPngDcfPPNdOvWrdxqoDN5++23mTJl\nCrm5ubRs2ZKZM2dSVFTEhAkTyMzMxBjDXXfdRXh4OP/3f//HokWL8PLyomPHjiWrrZ0Pl01DLSKz\ngEFAJHAYeATwBTDGvOos8ydgIlAMvGGMmVrRcc93GuoTlmxL5YYZK3n/ll5c2Cqy4h2U8lA6DXXt\nU2OmoTbGVNjJ1xjzb+DfroqhPKUXqdFEoJTyZB45shggKsSf+vV8teeQUsrjeWwiEBHnIjWaCJSq\nSG1bydCTnct/K49NBGCnpN52KIviYv1HrtSZBAQEkJ6ersmgFjDGkJ6eftaDzDxuhbLS2jYKIaeg\niAMZx4mNqOfucJSqkWJiYkhKSqIqBnMq1wsICCAmJuas9vHoRFB6qglNBEqVzdfXlxYtWrg7DOVC\nHl011K6k55AuZq+U8lyelQjyT24YDvb3IaZ+oI4wVkp5NM9JBBs+hqfiIGPfSZvbR4doF1KllEfz\nnEQQnQDFDth28kpH7aJD2J2WQ75DF6lRSnkmz0kEDVpD/Raw/auTNrdtFIKj2LArNcdNgSmllHt5\nTiIQgbbDYPcSKPht2bn20aEAWj2klPJYnpMIANoMBUce7FlasqllVBC+3qINxkopj+VZiSCuH/gG\nnVQ95OvtRauoYO1CqpTyWJ6VCHz8oeUg2PYVlBou3057DimlPJhnJQKAtkMhcx+k/raqT7voEA5m\n5pF5vNCNgSmllHu4LBGIyAwRSRGRDRWUu0BEHCIy2lWxnKTNUPu7VDfSdo3sCOPtOhOpUsoDufKJ\n4C1geHkFRMQbeAr4qrxyVSq0CUR3PqmdoPScQ0op5WlclgiMMUuAIxUUuxP4GEhxVRxlajMU9q2A\n40cBaBoeSEiAD+uTMqs1DKWUqgnc1kYgIk2BUcArlSg7WURWiciqKpkKt80wMEWw87sTx2dQu4Z8\nvfkwjqLi8z++UkrVIu5sLJ4KPGCMqfDKa4yZboxJNMYkRkVFnf8nxyRCYITtPeR0eUJjjuQUsGxn\n+vkfXymlahF3JoJEYLaI7AFGAy+LyFXV8sle3tD6YtjxNRTbOYYGto0ixN+HeesOVksISilVU7gt\nERhjWhhj4owxccBHwB+MMZ9WWwBth0FuOhxYA0CArzeXxDfiyw2HKHBo9ZBSynO4svvoLGA50E5E\nkkRkkohMEZEprvrMs9LqIhAv2P5bN9LLuzTmWJ6DH3boknxKKc/hsqUqjTHjzqLsTa6K44zqRUBs\nLzue4KKHAejXOoqwQF/m/ZrMRe0bVXtISinlDp43sri0NkPh0Do4lgyAn48Xwzo24qtNh8kr1PUJ\nlFKewbMTQdth9vf20r2HmpCd72DxNq0eUkp5Bs9OBA3jITTmpERwYasGRAT5MW9dshsDU0qp6uPZ\niUDETkK3cxE48gHw8fZieKdovtl0mNwCh5sDVEop1/PsRAB2lHFhDuz9sWTT5QmNOV5YxHdbqnfm\nC6WUcgdNBC0GgE/ASaOMe7VoQFSIP/N+1eohpVTdp4nArx7E9T+pncDbSxjRKZpFW1PIztfqIaVU\n3aaJAGw30iM7IX1nyabLuzQh31HMN5sOuzEwpZRyPU0EYBuMAbZ9WbKpR7P6RIcG6NxDSqk6TxMB\nQP04aNwFVs0smYTOy0u4LKExi7el6hKWSqk6TRPBCX3/COnbYfPnJZsuT2hMYZHhq42H3BiYUkq5\nliaCE+JHQoPWsPQ/YAwAXWPDiakfqIPLlFJ1miaCE7y8od89du6hHd8AduWyyxIa8+OONI7mFLg5\nQKWUcg1NBKV1HmOnnFj6n5JNVyQ0wVFs+FKrh5RSdZQmgtJ8/KDv3bBvOeyxI407NgklrkE97T2k\nlKqzXLkwzQwRSRGRDWd4/3oRWSci60VkmYh0cVUsZ6X77yAoquSpQES4sksTlu9MZ0dKtpuDU0rV\nWT+/Ca/0g09ug19nQ+aBavtoVz4RvAUML+f93cBAY0xn4B/AdBfGUnm+gdDndtj5LRz8BYAbLowj\n0Neb/3y11c3BKaVqvLxjZ1e+qBDm3QPz7wVTbMczfXIrPBcPL/aw7238BHLSXBMvLkwExpglwJFy\n3l9mjDnqfLkCiHFVLGctcRL4h5U8FUQG+3Nz/5Ys2HCItfsz3BycUqrG2vA/eLIZfDABju6tuHzu\nEXhnFKyaYaulpyyFP+2EKT/AsH/anozrPoQPb4J/t4JvH3NJ2DWljWASsMDdQZQICIVek+2YgpQt\nANwyoCURQX48tWALxtm9VCmlShzdA5/fDREtYMe3MK0nLPoXFOSWXT5lM0wfBPtXwqjpcMljtvei\nlxdEd7Y1E+M/gAf2wM3fwpC/QVw/l4Tu9kQgIoOxieCBcspMFpFVIrIqNbWaVg7rdRv41oMfngMg\n2N+HOwa3ZvmudJZud90jmlLKjYyBLfPh7Stg61ncmxYVwkeTAIHffQp3/AztL4PFT9qEsOmzkvFJ\ngD32GxeDIw8mfgFdrjvzsb19ICYR+t8HrS46569WHrcmAhFJAN4ARhpj0s9Uzhgz3RiTaIxJjIqK\nqp7gghpAj4mw/kOb6YHrezejaXggTy/cQnGxPhUoVafs+RHeHAqzx8O+FTDnBtj5XeX2/e5xOLAK\nrnwB6jeHsBgYPQNu+gICwuyx/nslHN5kby5njbPVPrcsshd5N3NbIhCRZsD/gN8ZY7a5K45yXXiH\nfVT78QUA/H28ufeStmw4cIz563W0sVJ1QvI6eHc0vDUCMpPgihfg3s0Q2RZmXw97l5e//87v4Mep\n0OMm6HjVye/F9YXJi2HEM/ZzXukD3zwKna6G338JYU1d9a3OiriqvltEZgGDgEjgMPAI4AtgjHlV\nRN4ArgFOtKg4jDEVpsbExESzatUql8Rcps/vhrWz4I/rICSaomLDiOeXku8o4ut7B+Lr7fbaNaXU\nuTiyC757AjZ8BAHh0P9e6DnZ9hwEyE6BmZfa3zfOhSbdTj9Gdgq80hfqNYBbvrPrm5xJ7hFY8gyE\nNrH1/yKu+V5nICKrz3SNdVkicJVqTwRHdtkuXH1uh6GPA/Dt5sNMensVj1/ViQm9m1dfLEqp81Pk\ngF2L4NdZtt7eyxd632Z77ASGn14+MwlmXAoF2TBxATRs/9t7xcXw3jWwd5mt4mkUX33f4xyUlwj0\ndrYiES2h02j4eQZk7AfgovYNSWxen+e/3c7xgiI3B6iUKpcxtlrmy4fg2Q7w3mhbnXPBzXD3Wrj4\nkbKTANi6/hs+BW9f+O9Ie2N4wvIX7XGG/6vGJ4GKaCKojIv+ChiY90cwBhHhgUvbk5qVz4wfd7s7\nOqVUWbIO2fa9V/rCa/1h5XSI7QnXvQf3bYNLn4KQ6IqP06AV3PAZFOXbZJB5AJJW2z79Ha60nUpq\nOa0aqqyfXoMFf4arXoGu4wGY9NbPrNxzhKV/Hkx4Pb/qj0kpdTJjbI+fla/ZcUDFDoi5ALqMhY5X\nQ72Icz/2gTXw9pU2eRQX2kWspiyFwPpVF78LadVQVbjgFojtDV8+aO80gD8Nb0d2voNXvt9Zwc5K\nKZcqPA5r3rF3/jOH2yqbXlPgjtVw8ze2Guh8kgBA0+5w/RzbbpCxH655s9YkgYr4uDuAWsPLC0ZO\ng1f7wvz74Lp3aR8dyqiuTXlr2R5u6htH47BAd0eplGfJ2A+r3oTVb8PxI9AwHi6fCgljwC+o6j+v\n+YUwcT7kHoVmvar++G6iieBsRLaGwQ/B13+zk0B1upp7LmnLvHXJPD5/M9PGd3d3hErVPcZAVjKk\nbbfLyabv/O3vo3ttN8x2I+wTQFw/13fLbNrDtcd3A00EZ6v37bDxU/jiT9BiALERkdx5UWv+8/U2\nrup6mEviG7k7QqVqD2Ns3fvR3baffW6anWUzN82+zkmDzP22++YJvvVsA26T7tB1gp2eIbyZ+75D\nHaCNxefi8CZ4bYBd53j0mxQ4irnypR84mlvA1/cOJDTA173xKVVbLHsRvnq41Aax9e5BkXaQVr0G\nENoUItvYKRki20BIE1tVq85KeY3F+kRwLhrFw8A/w6InoNPV+LW/jKeuSWDUyz/y5IIt/HNUZ3dH\nqFTNt3kefPV/tgvm4L/ai39gfTuti6pWmlbPVb97oFFnmHcvHD9Kl9hwft+3Be//tI8Vu844f55S\ndU/uEfifc9r2yjr4C/zvFtsT5+rpdsRuUKQmATfRRHCuvH1h5EuQkwoL7aPtvUPbEhsRyF/+t568\nQh1xrDxA7hE7q+a6D+xiLEv+ffJ0y2XJPADvj7XVPmNn/Ta3j3IbTQTno0lX6PdHWPsurH6Ler7e\n/GtUArvTcnj+2+3ujk6pkxkDhzfafvDFxed/vBNJIHWbvaB3HmOnY/7fZCjMK3uf/Gx4/zooyLGL\nroRo54qaQNsIzteAP9sVhj6/G3Ytpt/lzzG6RwzTl+zi8oTGdGwS5u4IlbIWPgQrXrZ/e/tD/Ti7\nmlb9FnZOrYgWENvLrtBXkZx0O91C+nYYNwtaD4F2l0JUO/juH7YX0Nj3Ibjhb/sUF8HHkyBlI4z/\nEBp1dMnXVGdPew1VheIiu9jEon9CaBOyRkxj8IeFRIf58+kf+uKjU1Urd/v5Tbs4ercJttvl0d1w\nxPlzdDcUOpdTDAi3M3H2uvXMA7Jy0u2TQPoOmwROXTVr02fwv1ttnf+42RDdyW7/8iFYMc3Ozd/z\nFtd9V1UmnYa6uiSttnc8GXvZ3vZWLv21D3+6tBO3Dmzl7siUJ9vxLbx3rb1rHzf79AZZY+y8+qmb\nYfk02P4VBDW0SyMmTgQf/9/K5qTZ+XaO7LTHajW47M88uNauwpWXCde8AVkH7Yj8XrfBpU+67ruq\nM9JEUJ3ys2DBA7D2PXb5d2Byzq288ccxxEW6YLi7UhVJ2QJvXgJhsTBpIfiHVLzPvhW2rn/PUgiN\nsV2lu463F/XKJIETjiXD7HE2KYhA60vsE4T2DHILtyQCEZkBXA6kGGM6lfG+AM8DI4Bc4CZjzJqK\njlvjE8EJGz6m+PM/cjyvgLWzyJUAACAASURBVC+Cr+Hq/l3w9qtne0j4lvpdL8IOklGqquWkwesX\n2QnZbvkOwmMrv68xsHsxfPsPuxZvREu7iEvGPhg/G1oOqtxxCnJh3j12dPD4DyqXiJRLuCsRDACy\ngf+eIRGMAO7EJoJewPPGmApncao1iQAgYz+p704kKu3n8sv1ucOuflbNS9epOsyRb+/ek9fCTfPP\nfYF0Y2Dbl/YJ4cgu+yTQcmDVxqqqxXmPLBaRVkCSMSZfRAYBCdgLfMaZ9jHGLBGRuHIOO9J5DAOs\nEJFwEWlsjKk7q8KHxxJ1+9c8M38Ns37YyoMXN+faLpG2Ya7wuP29aS4sfwmKCu1CGZoM1JkU5tl/\nKytftz1uOo6C9pedPr2yMTD3Tti/AkbPPPckAM4J3S6FNsPsv1f/4PP7DqpGqmz30Y+BRBFpDUwH\nPgPex97Nn6umwP5Sr5Oc205LBCIyGZgM0KxZLZtcSoR7RnRnS7rhgW8P06hZWwa0bffb+62G2Mfl\n5S/ZFZAue07nUVEnMwa2LoCFf4Gje6DFQNtjZ+4ddtW8VhfZpNBuhF1yccm/7QCvwQ9Dp6urJgYv\nL00CdVhlE0GxMcYhIqOAF40xL4rIL64MrDRjzHRsAiIxMbF2tW4D3l7C1LFdGf3KMm5/fw2f/OFC\nWjd01pWK2Gohbz/44VlwFNgRy9qgpgBSt9rFkHZ+B5Ht4Hef2Au/MXaaho2f2Nlwt99m6/Cb9baN\nvAljYcD97o5e1RKVvfUsFJFxwI3APOe2851i8wBQuvUqxrmtTgr29+GNGxPx9/Fi0turOJpT8Nub\nIjDkbzDoIfj1fTsys8jhvmCV++VlwsK/wisX2m7Jw5+E2378rc++iJ2nZ+g/4I/r4ObvbN//o3ug\nxQC48gWtZlSVVtlEMBHoAzxhjNktIi2Ad87zs+cCN4jVG8isU+0DZYipX4/XfpdIckYeU95dTYGj\n1DB/ERj0AAx5BDZ8BB9NtE8HyvNs/ARe7GH79HcdD3euht632fmtyiICMT1g2BNwzwa48fOT+/4r\nVYFKJQJjzCZjzF3GmFkiUh8IMcY8Vd4+IjILWA60E5EkEZkkIlNEZIqzyBfALmAH8Drwh3P/GrVH\nj+b1eWp0Z37afYRH5m7gtF5b/e+FYf+EzXNhzg2294fyDI58O+jqw5vsQiuTF8GVL0JwlLsjU3Vc\nZXsNfQ9c6Sy/GkgRkR+NMfeeaR9jzLjyjunsLXR75UOtO0Z1i2FHSjbTFu2kdcMQJvVrcXKBPrfb\nNoMv7rczOl73Hvj4uSdYVT2O7LYJIHktXHinfTI80xOAUlWsslVDYcaYY8DV2C6fvYCLXRdW3Xff\nJe0Y1rERT8zfxMKNh04v0PMWuwj39q/g499rm0FdtnkevDbwt4nahj6uSUBVq8omAh8RaQyM4bfG\nYnUevLyE567rSkJMOHe+/wvLdqSdXihxIgz7l13w49Pb7OR2qu4oKrQNwh9cDw1awq1L7LgApapZ\nZbuPPgYsBH40xvwsIi0BnXD/PNXz8+GtiRdw3WsruOW/q3j/lt50iQ0/uVCfP9iBPN/9w05LccXz\n2hukNsg9AofW2yo+3wDwKfXjGwDHM2zvsKSV0PNW2/tHG3iVm+ikczXA4WN5jH51Gdl5Dubc2oc2\njcqYj+Xbx2Dpf+zsjcP/VfeSwZHdtoE8cVLVDFwqckD2YTh2wHbFjOvn+pWwjiXDlnn2CW7PD2Aq\neILzC4GRL9rBYEq52HnPNSQiMcCLQF/npqXA3caYpCqLspLqYiIA2Juew+hXl+MtwodT+hAbUe/k\nAsbAl3+Bn16x0wMP+Zt7AnWFtB3w9uWQlQwRrWD0DLv6W2XtX2nnwM9MgmMH7cU/69DJF+JmF9rJ\n0gKqeKGg9J32wr9lHiQ555Rq0AY6XG7784PtDVR43P52OH8XFUD7y6GBTlGuqkdVJIKvsVNKnBg7\nMAG43hhzSZVFWUl1NREAbDl0jDGvLiciyI8Pp1xIVMgpVQXG2JXQ1rwNF/1fzRw5mpd5dhfb1G02\nCRQXwcWP2sV9clLhkr9D7z+U/+RzLBm+ecROp+ATAGExENrETp0c1hRCnT9ZybZbZsMOMOF/le+O\neWQ3rJxuq3kKc+xMmoW5UJBt/y7Itk8dAI272ot/hyvtKl1K1TBVkQjWGmO6VrStOtTlRACweu9R\nJrzxE3GRQcye3JuwwFN6jxQX2YbjdR/AxX+HCybVjKl9jYGlz9hZKhPG2uqrUydDO1XKFnj7CsDY\nQVANO9iL7md3wNb50GYojHz59Au3I98uubj431BcCBfeZcdfnGlFLYDt39iuuGFN4Xeflj8lszHw\nyzv2CazYYZdb9A0Cv3p26nC/IOfvetCwo00A4bVsDizlcaoiEXwLzARmOTeNAyYaY4ZUWZSVVNcT\nAcCSbalMevtnEmLCeWdST+r5ndKmX+SwI483z7Wvw2Ihqr29kDbsYP+Oalf+hbEqGQNf/w2WvQAx\nPeHgGqjXAC571l4ky3J4k00CXt42CZS+izYGfn7D9qgJDIdRr/22CMq2r+zcO0d22knWhj1h58qv\njH0r4L0xNnHe8GnZ60Bkp9qnrq3zIa4/XPXK2c3jr1QNVRWJoDm2jaAPYIBlwJ3GmP3l7ugCnpAI\nAL5Yn8wd76+hV4sGvHZDD0IDTnkyKHLAjm/g8AZI2QypWyBtm617PqFeAwiKcv5E2t/1Iu3fDVrZ\nC935Tm5XXGSrXVbPhAtuhkv/DYfXw6e329+drrHbghr8ts+h9Xbhc28/mwTOtDDPoQ3w0e/t9+pz\nO6Rth+0LoUFrGP4UtDmHoSzJ6+Ddq22ymfDxyW0RWxfY6ZvzMu2Art5/0JlgVZ3hkoVpROSPxpip\n5xXZOfCURADwyS9J/OnDdbRuGMzMiRfQOKyCXi9FDjsoKWWznbUy66Ctb89Jd/5OhbxSS0iENYMe\nN0K330FIo7MPsKjQVlOt/xD63WsbsE/U6RcVwg/PweKnbZvBZc/Y3jHJv9ok4FvPJoGKGksLcmwV\nzZq3wS8YBj4Avaac30jrtB3wzlX2gj9uNjTuAgsfsp/RqDNcPR0axZ/78ZWqgVyVCPYZY6q9YtST\nEgHAD9vTmPLuaoL9fZg58QI6NA49vwM6CiA33S5asmqmXY7Qy8cOZEr8PcQNqNxdcGGerZ7a+oW9\ne+5/htlGDm+Ez263Uya3vRT2LbdVMzd+DhEtyt6nLPt+gvpx55awypJ5wCaDjH22DSBjP/S9GwY/\npP35VZ3kqkSw3xhT7ZWnnpYIADYnH2PizJ/JyXfwyoQe9GsTWXUHT9thq3XWvg/Hj9jumz1ugri+\nENm27Ibo/Gy7KPnuJTDiGTsdRnmKHLD8RVj0LwhuBDfNg/rNq+47nKucNHjvWpsYR70KzS90d0RK\nuYw+EdQByZnHmTjzZ3akZPPUNQlc0yOmaj+gMM82Pq+aYe/aTwhtahNCVDv7u0Er2zPowBq46mXo\nMrbyn5F5wA7qqqg3UXU6MW2HLgSk6rhzTgQikoVtHD7tLSDQGFPZKSqqjKcmAoBjeYXc9u5qftyR\nzn2XtOWOi1ojrhhhfGSXbahN22r7+adttQ21hbn2fW8/O+irwxVV/9lKKZc458XrjTE1oIO6OiE0\nwJeZN/XkwY/X8Z+vt3Eg4ziPX9UJH+8q7tkS0fL0LpnFxXAsySaGsBho2L5qP1Mp5TYuvaMXkeHA\n84A38IYx5slT3m8GvA2EO8s8aIz5wpUx1XZ+Pl78Z0wXmoQH8tKiHaRlF/DS+G4E+Lq4asPLyw6a\n0oFTStU5LuskLSLewDTgUiAeGCcip/bJexiYY4zpBowFXnZVPHWJiHD/sHY8NrIj3245zA1vriTz\neKG7w1JK1VKuHC3TE9hhjNlljCkAZgMjTyljgBP9IcOAgy6Mp865oU8cL4ztxi/7j3Lda8tJOZbn\n7pCUUrWQKxNBU6D0yOMk57bSHgUmiEgSdg3jO8s6kIhMFpFVIrIqNTXVFbHWWld0acKMmy5g35Fc\nrnl1GXvSctwdklKqlnH3+PlxwFvGmBhgBPCOiJwWkzFmujEm0RiTGBWlC3mfqn+bKGbd0pvsPAej\nX13GhgOZ7g5JKVWLuDIRHABKDziLcW4rbRIwB8AYsxwIAKpwtJTn6BIbzodTLsTfx5ux01ewfGe6\nu0NSStUSrkwEPwNtRKSFiPhhG4PnnlJmHzAEQEQ6YBOB1v2co9YNg/notj40DgvgxhkrmfurNrko\npSrmskRgjHEAd2DXOt6M7R20UUQeE5ErncXuA24RkV+xU1zfZGrb2pk1TOOwQD6c0oeEmDDumvUL\n//xiM46iYneHpZSqwXTN4jqqwFHMY/M28u6KffRt3YAXx3UnIug8ZuxUStVq5Y0sdndjsXIRPx8v\nHr+qM0+PTuDnPUe54sUfWJ+kjchKqdNpIqjjxiTG8tGUPhhjuObVZXy0OsndISmlahhNBB4gISac\nz+/sR2Lz+tz/4a/87bMNFDi03UApZWki8BANgv357+97MnlAS/67fC9jpy9ntw4+U0qhicCj+Hh7\n8dCIDrw0vhs7UrIZPnUJbyzdRVFx7eowoJSqWpoIPNDlCU34+t6B9G8TxePzNzP61WXsSMlyd1hK\nKTfRROChGoUG8PoNPXh+bFd2p+Uw4oUfePn7HTrmQCkPpInAg4kII7s25et7BjKkfUOe/nIro15e\nxpZDx9wdmlKqGmkiUESF+PPKhB68fH13DmYc54oXf+DfC7eQW+Bwd2hKqWqgiUCVGNG5MV/fO5Ar\nEpowbdFOLnl2CV9uSKa2jT5XSp0dTQTqJBFBfjx7XVfm3NqHkAAfpry7hhtmrGRXara7Q1NKuYgm\nAlWmni0imHdnPx65Ip61+zIYNnUJT3+p1UVK1UWaCNQZ+Xh7MbFvC769fyBXdGnCy9/v5OL/LGb+\nOq0uUqou0USgKtQwJIBnx3Tloyl9CKvnx+3vr+HaV5ezdn+Gu0NTSlUBTQSq0hLjbHXRv67uzJ70\nXK6a9iN3zfqFpKO57g5NKXUedD0CdU6y8x28tngn05fswgCT+rXgD4NaERLg6+7QlFJlcNt6BCIy\nXES2isgOEXnwDGXGiMgmEdkoIu+7Mh5VdYL9fbhvaDsW3T+Iyzs35pXvdzLo39/zzoq9FOroZKVq\nFZc9EYiIN7ANuARIwq5hPM4Ys6lUmTbYxesvMsYcFZGGxpiU8o6rTwQ107qkDB6fv5mVu4/QLKIe\ndw9pw1XdmuLtJe4OTSmF+54IegI7jDG7jDEFwGxg5CllbgGmGWOOAlSUBFTNlRATzgeTe/PmjYmE\nBPhw34e/cslzi5n760GKdXZTpWo0VyaCpsD+Uq+TnNtKawu0FZEfRWSFiAwv60AiMllEVonIqtTU\nVBeFq86XiDCkQyM+v6Mfr07ojo+XcNesXxjxwlK+3HBIu5wqVUO5u9eQD9AGGASMA14XkfBTCxlj\nphtjEo0xiVFRUdUcojpbXl7C8E6NWXD3AJ4f25UCRzFT3l3NFS/9wLIdae4OTyl1ClcmggNAbKnX\nMc5tpSUBc40xhcaY3dg2hTYujElVI28vO7vpV/cM4Jlru5B5vJDxb/zEw5+uJydfRygrVVO4MhH8\nDLQRkRYi4geMBeaeUuZT7NMAIhKJrSra5cKYlBv4eHsxukcMX98zkJv7teC9n/YxbOoSlu9Md3do\nSilcmAiMMQ7gDmAhsBmYY4zZKCKPiciVzmILgXQR2QQsAv5kjNGrQx0V4OvNw5fHM+fWPvh4CeNe\nX8Gjczfq/EVKuZkOKFNucbygiKcXbmHmj3to3qAez1zbhQviItwdllJ1ltsGlCl1JoF+3jxyRUdm\nT+6NMTDmteU8Oncj+4/odBVKVTd9IlBul5Pv4Kkvt/DOir0A9G0VybWJMQzrGE2Ar7ebo1Oqbijv\niUATgaoxDmQc56NVSXy4ej9JR48TGuDDVd2aMiYxlk5Nw9wdnlK1miYCVasUFxtW7Erng1X7WbDh\nEAWOYuIbh3Jz/xaM7KrTVih1LjQRqForM7eQuesO8t6KvWw5lEW7RiH8aVg7hnRoiIgmBKUqSxOB\nqvWKiw0LNhzima+2sjsth8Tm9fnz8Pb0bKE9jZSqDO01pGo9Ly/hsoTGfHXPAP51dWf2H81lzGvL\nmThzJZsOHnN3eErVavpEoGql4wVFvL18Dy8v2kFWvoOh8Y24JD6aAW0iaRga4O7wlKpxtGpI1VmZ\nuYW8tmQnc1YlkZadD0CHxqEMbBvFgLaRJDaPwM9HH3yV0kSg6rziYsPmQ8dYsi2NxdtSWLXnKI5i\nQ5CfN31aRTKxbxx9W0e6O0yl3EYTgfI42fkOlu9MZ/G2FL7edJjDx/Lp27oBfx7Wni6xp810rlSd\np4lAebS8wiLe+2kf0xbt4EhOAZd2iua+oe1o3TDY3aEpVW00ESgFZOUV8uYPu3l9yS6OFxZxbY9Y\n7r64DU3CA90dmlIup4lAqVLSs/OZtmgn767YCwLXJcYytmcsHZvoNBaq7tJEoFQZko7m8sK32/l0\n7UEKHMV0ahrKdYmxXNm1KWGBvu4OT6kq5bZE4FyM/nnAG3jDGPPkGcpdA3wEXGCMKfcqr4lAVbWM\n3AI+/eUAH6xKYnPyMfx9vLi0UzRjLoild4sGeOncRqoOcEsiEBFv7BrEl2DXJv4ZGGeM2XRKuRBg\nPuAH3KGJQLmLMYYNB44xZ9V+Pl17gKw8B80i6jGic2OGd4qmS0yYzm+kaq3yEoGPCz+3J7DDGLPL\nGcRsYCSw6ZRy/wCeAv7kwliUqpCI0DkmjM4xYfz1sg58ueEQH69J4o2lu3h18U6ahAUwtGM0l3aK\nJjEuQmdBVXWGKxNBU2B/qddJQK/SBUSkOxBrjJkvImdMBCIyGZgM0KxZMxeEqtTJAny9uapbU67q\n1pSM3AK+3ZzCgg2HeH/lPt5atofIYD8uiY/mmu5N6dG8vj4pqFrNlYmgXCLiBTwL3FRRWWPMdGA6\n2Koh10am1MnC6/lxTY8YrukRQ06+g++3prJgQzJz1x5g1sp9dI0N55b+LRnWsRE+3jqdhap9XJkI\nDgCxpV7HOLedEAJ0Ar533k1FA3NF5MqK2gmUcpcgfx8uS2jMZQmNyS1w8PHqJN78YTe3v7+GmPqB\nTOrXgjGJsQT5u+0eS6mz5srGYh9sY/EQbAL4GRhvjNl4hvLfA/drY7GqbYqKDd9sPszrS3axau9R\nQgN8GN+rOTde2JzGYTpYTdUMbmksNsY4ROQOYCG2++gMY8xGEXkMWGWMmeuqz1aqOnl7CcM6RjOs\nYzRr9h3ljaW7mL5kJ68u3kmrqCB6NK9f8tMyMli7o6oaRweUKeUC+9Jz+XzdQdbsPcrqfUfJyC0E\nILyeL92b2aQwNL4RbRqFuDlS5Sl0ZLFSbmSMYVdaDqv3HmXN3qOs2nuUHSnZAHSNDWdMYiyXd2lM\naICOZlauo4lAqRomNSufz9YeYM6q/Ww7nE2ArxcjOjXm2sRYerWI0OojVeU0EShVQxljWJeUyZxV\n+5m79iBZ+Q5iIwK5rHMT4puE0j46hBaRQfhqt1R1njQRKFULHC8oYuHGQ8xZtZ+fdh+hqNj+v+nr\nLbSKCqZddAjtokPoEB1KzxYR2kVVnRVNBErVMvmOInam5LDtcBZbDmWx9dAxth7K4mBmHgB+Pl70\nax3JsI6NGNKhEZHB/m6OWNV07pprSCl1jvx9vIlvEkp8k9CTth/LK2RDUibfbklh4cZDfLclBZH1\nJDavz9D4aIZ2bETzBkFuilrVVvpEoFQtZYxhc3IWX206xFcbD7Mp+RgAHZuEMrpHDCO7NiUiyM/N\nUaqaQquGlPIA+4/ksnDjIT5be5D1BzLx9RaGtG/E6B4xDGwXpQ3OHk4TgVIeZsuhY3y0KolP1x4g\nLbuAyGB/RnVrwlXdmtK2UYgmBQ+kiUApD1VYVMz3W1P5aPV+vt2cgqPY4OMlNGtQj5aRQbSMCqZF\nZBAtI4NoERVEVLC/TqldR2ljsVIeytfbi0viG3FJfCPSs/NZtDWVXanZ7ErNYXdaDku2p1HgKC4p\nH1M/kMHtGjK4fRR9WkYS6OftxuhVddFEoJSHaBDsz+geMSdtKyo2HMw4zq60HHakZLN8ZzofrU7i\nnRV78fPxok/LBgxuF8Xg9g21N1IdplVDSqmT5DuKWLn7CIu2pPL91hR2peUA0DQ8kJj6gTQOCyA6\nLJAm4QFEhwbQOCyQ6LAAIoP9tFqpBtM2AqXUOduTlsOirSn8si+DQ5l5HMw8zuFjeRQWnXzt8PPx\noklYAE3CA0t+mobb1+2iQ2gYEuCmb6BAE4FSqooVFxvScwo4lJlHcuZxkp0J4mBGHgczjnMwwyYL\n5ywZ+HoLo3vE8odBrYiNqOfe4D2U2xqLRWQ48Dx2YZo3jDFPnvL+vcDNgANIBX5vjNnrypiUUufP\ny0uICvEnKsSfzjFhZZYpLCrm8LE8Dhw9zrx1yXzw834+XLWfq7s35fbBrbXNoQZx5VKV3tilKi8B\nkrBLVY4zxmwqVWYw8JMxJldEbgMGGWOuK++4+kSgVO10KDOPVxfvZNbKfTiKDSO7NuGOwa1pGRXs\n7tA8grueCHoCO4wxu5xBzAZGAiWJwBizqFT5FcAEF8ajlHKj6LAAHr2yI38Y1IrpS3bx7k97+fSX\nAwzvFE1ksD85+UUcL3TY3wVF5BQ4yC0oIjTAh/bRobRvbGdfbR8dqlNnVDFXJoKmwP5Sr5OAXuWU\nnwQscGE8SqkaoGFoAA9fHs+UQa14feku5vy8HwME+fkQ6OdNkJ83gX7eRIcGEOjnzZGcAr7efJgP\nVv12OWkY4k/7xqF0aBxC15hwujWrT3SYNkafqxoxjkBEJgCJwMAzvD8ZmAzQrFmzaoxMKeUqkcH+\n/OXSDvzl0g4VljXGkJqdz5bkLLYeymLzoWNsSc5i5s50CorsgLjo0AC6NQunW7NwusbWp3PTMB0Q\nV0muTAQHgNhSr2Oc204iIhcDfwUGGmPyyzqQMWY6MB1sG0HVh6qUqslEhIYhATQMCWBA26iS7fmO\nIjYnZ/HLvqP8si+DX/YfZcGGQwB4ewmRwX6EBvgSGuhLSIAPoQHO34G+RIcG0Ld1JK2igjx+/IMr\nE8HPQBsRaYFNAGOB8aULiEg34DVguDEmxYWxKKXqIH8fb7rGhtM1NpyJfe22tOx81u7LYO3+DFKy\n8jh23EFWfiHp2QXsScvhWJ6DrLzCknEQTcMDGdA2kgFtoriwdSRhgb5u/Ebu4dJxBCIyApiK7T46\nwxjzhIg8BqwyxswVkW+AzkCyc5d9xpgryzum9hpSSp0vYwxJR4+zdHsai7elsGxHOln5Dry9hK6x\n4fRrHUlEkB/GGIoNGOc+xoDBEB7oR5fYcFo3DMbbq3Y8TeiAMqWUKkdhUTFr92ewZFsqS7alsu5A\nJpW5NAb7+5AQE1byVNK1WXiZI6iLig35jiLyC4sJDvBxyzTgmgiUUuosZOc7yC8swksEERAE8QLB\ntlccPpbHWmebxNr9GWxJzsLhHEbdMMQfby8h31FMfmER+Y7ikvfATsUR3ziUhJgwEmLC6RITRsso\n1z9ZaCJQSikXOl5QxIaDmazdl8GWQ1l4Cfj7ehHg442/rxf+Pt74+3jh5+NFcmYev+7PYMOBTHIK\nigAI8vOmY9MwOjYJpXFYgLNh3J+Gof5EhQQQGuBz3g3auh6BUkq5UKCfNxfERXBBXESl9ykqNuxK\nzebXpEzWJWXwa1Ims1buI6+w+LSy/j5eNAz154becdwyoGVVhg5oIlBKKbfw9hLaNAqhTaOQknUi\njDFk5TtIOZZPSlYeqVn5pGblk5KVT8qxPBqG+rskFk0ESilVQ4iIHfcQ4EvrhtU3B5OuYK2UUh5O\nE4FSSnk4TQRKKeXhNBEopZSH00SglFIeThOBUkp5OE0ESinl4TQRKKWUh6t1cw2JSCqw9xx3jwTS\nqjCcukzPVeXoeaocPU+V48rz1NwYE1XWG7UuEZwPEVl1pkmX1Mn0XFWOnqfK0fNUOe46T1o1pJRS\nHk4TgVJKeThPSwTT3R1ALaLnqnL0PFWOnqfKcct58qg2AqWUUqfztCcCpZRSp9BEoJRSHs5jEoGI\nDBeRrSKyQ0QedHc8NYWIzBCRFBHZUGpbhIh8LSLbnb/ruzPGmkBEYkVkkYhsEpGNInK3c7ueq1OI\nSICIrBSRX53n6u/O7S1E5Cfn/4MfiIifu2OtCUTEW0R+EZF5ztfVfp48IhGIiDcwDbgUiAfGiUi8\ne6OqMd4Chp+y7UHgW2NMG+Bb52tP5wDuM8bEA72B253/hvRcnS4fuMgY0wXoCgwXkd7AU8BzxpjW\nwFFgkhtjrEnuBjaXel3t58kjEgHQE9hhjNlljCkAZgMj3RxTjWCMWQIcOWXzSOBt599vA1dVa1A1\nkDEm2Rizxvl3FvZ/3KbouTqNsbKdL32dPwa4CPjIuV3PFSAiMcBlwBvO14IbzpOnJIKmwP5Sr5Oc\n21TZGhljkp1/HwIauTOYmkZE4oBuwE/ouSqTs7pjLZACfA3sBDKMMQ5nEf1/0JoK/Bkodr5ugBvO\nk6ckAnWOjO1frH2MnUQkGPgY+KMx5ljp9/Rc/cYYU2SM6QrEYJ/I27s5pBpHRC4HUowxq90di4+7\nA6gmB4DYUq9jnNtU2Q6LSGNjTLKINMbe1Xk8EfHFJoH3jDH/c27Wc1UOY0yGiCwC+gDhIuLjvNvV\n/wehL3CliIwAAoBQ4HnccJ485YngZ6CNszXeDxgLzHVzTDXZXOBG5983Ap+5MZYawVl3+yaw2Rjz\nbKm39FydQkSiRCTc+XcgcAm2TWURMNpZzOPPlTHmL8aYGGNMHPaa9J0x5nrccJ48ZmSxM+tOBbyB\nGcaYJ9wcUo0gIrOAQdjpbw8DjwCfAnOAZtgpv8cYY05tUPYoItIPWAqs57f63Iew7QR6rkoRkQRs\nI6c39mZzjjHmMRFp5ZtedQAABC1JREFUie2oEQH8AkwwxuS7L9KaQ0QGAfcbYy53x3nymESglFKq\nbJ5SNaT+v727CbGqjOM4/v3NogKLXNjGhQ1NiUUvU760kcoIl72LiCBSRK8TBS5mFVEg1iAEURAZ\nRuQiiSA3NeakIkGNNuWMllNg0sJNFFgGTdr8Wzz/G8fD2OWOQtfO77OZufec8zzPnRnO/7zM+T1m\nZmfhQmBm1nAuBGZmDedCYGbWcC4EZmYN50JgXUVSSNpceb1B0vPnqe23JT3Yfs1z7meVpG/zQarq\n+/MlvZ/f9+e/NJ+vPudKemKmvszacSGwbjMF3C9p3n89kCpJnTyF/zDwSESsqL4ZEccjolWI+oGO\nCkGbMcwF/ikEtb7M/pULgXWb05R5W5+tL6gf0Us6mV/vkLRX0oeSjkraJGltZuJPSOqrNHOXpAOS\nvsusl1ZA2pCk/ZLGJT1aaXefpB3ANzOMZ022f0jSS/nec8By4C1JQ7X1e3Pdi4AXgNWSvpa0WtIc\nlbkhRjOb/p7cZr2kHZI+BUYkXSppRNJY9t1K0d0E9GV7Q62+so1LJG3N9b+StKLS9geSPlaZT+Hl\njn9b9r/QlKwhu7C8Box3uGO6CbiWEql9FNgSEctUJpAZAJ7J9XopIWh9wG5JVwPrgBMRsVTSxcBn\nknbm+rcA10fED9XOJM2n5MYvpmTG75R0bz5BeyflKdEDMw00Iv7MgrEkIp7K9jZSIgYeyniGUUm7\nKmO4MSJ+ybOC+yLi1zxr+jwL1WCOsz/b6610+WTpNm6QtCjHujCX9VOSVKeASUmvRkQ1qdcawGcE\n1nUy1fMd4OkONtufcwZMUSKPWzvyCcrOv2V7RExHxPeUgrEIWAmsy9jkLyhRwNfk+qP1IpCWAnsi\n4qcMB9sG3NbBeOtWAoM5hj2UELIFueyTSmyFgI2SxoFdlIjidtHXy4F3ASLiCCUKo1UIRiLiRET8\nQTnrufIcPoNdoHxGYN3qFWAM2Fp57zR58CKpB6hO4VfNYpmuvJ7mzL/zeqZKUHauAxExXF2Q+S+/\nz274HRPwQERM1sZwa20Ma4ErgMURcUrSMUrRmK3qz+0vvE9oJJ8RWFfKI+DtnDlN3zHKpRiAuykz\nX3VqlaSevG9wFTAJDAOPq8RMI2mhpDlt2hkFbpc0T2Uq1DXA3g7G8RtwWeX1MDCQKadIuvks211O\nybA/ldf6W0fw9faq9lEKCHlJaAHlc5sBLgTW3TZTUlFb3qTsfA9S8u1nc7T+I2Un/hHwWF4S2UK5\nLDKWN1jfoM2Rcc5KNkiJDD4IfBkRncQF7waua90sBl6kFLZxSYfz9Uy2AUskTVDubRzJ8fxMubdx\nqH6TGngd6Mlt3gPWO/XTqpw+ambWcD4jMDNrOBcCM7OGcyEwM2s4FwIzs4ZzITAzazgXAjOzhnMh\nMDNruL8BdgojMs2SNq4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Ss3qbkFAwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 34.21395397599998 seconds\n",
            "Best accuracy: 73.02  Best training loss: 0.13145054809655995  Best validation loss: 0.8394116374850273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ac8c751e-3bda-4979-adb3-c5a3a3578f9b",
        "id": "2SWcIsvv1jOO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
            "[1.7530029838085175, 1.4054186010956764, 1.2262543530464172, 1.11206821256876, 1.0174993271231652, 0.9446473330259323, 0.8791469852030277, 0.8256531829833984, 0.7699933918714523, 0.7217608823180198, 0.6816095776855946, 0.6451221434772014, 0.6110327849388123, 0.5798243822902441, 0.5389030829966068, 0.5155020115673542, 0.4831509550511837, 0.4661193103641272, 0.43383051754534246, 0.4102742100507021, 0.39068286488950255, 0.3632538733780384, 0.3494019308239222, 0.3261039031147957, 0.31544896789640187, 0.29637291316315534, 0.2734553378075361, 0.2642798228003085, 0.24665471294894814, 0.23994675743952393, 0.23280917739868165, 0.20280317642912268, 0.20556166363880038, 0.1947154118269682, 0.18145877598784865, 0.1777478300817311, 0.160825694732368, 0.16113818064332008, 0.14800657242536544, 0.14994053562451154, 0.13738201677799225, 0.13145054809655995]\n",
            "[1.4862018430233004, 1.2782037156820303, 1.1507763636112212, 1.05793753683567, 1.0144024962186815, 1.0120739215612413, 0.9528959494829178, 0.8893240210413932, 0.8790674993395804, 0.8738784733414651, 0.849186798930168, 0.8394116374850273, 0.8968599587678907, 0.856685155630112, 0.9167001384496688, 0.8723378539085387, 0.9080953758955, 0.9149216452240945, 0.8581489175558094, 0.9005030125379561, 0.9031431910395626, 0.9225239741802219, 0.9204987940192225, 0.9666363361477854, 1.002236432433128, 0.981727671325207, 1.0536145025491714, 1.045223645865917, 1.0136158359050753, 1.074234887957573, 1.0232071867585182, 1.0947380089759824, 1.0996504640579223, 1.1089048317074772, 1.1348638749122628, 1.1571661728620526, 1.18959703207016, 1.1253026244044306, 1.2085330203175548, 1.1727856773138043, 1.2229560643434523, 1.207243374586105]\n",
            "[48.1, 54.98, 59.44, 63.04, 64.5, 65.12, 67.48, 69.24, 69.5, 69.46, 70.96, 70.94, 69.72, 71.66, 70.74, 71.52, 71.26, 71.54, 72.52, 72.46, 72.82, 72.68, 72.16, 72.7, 72.42, 72.3, 71.5, 72.86, 71.9, 72.16, 72.68, 72.94, 72.66, 72.6, 72.16, 71.8, 71.66, 71.76, 70.78, 72.52, 72.2, 73.02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E_Jb3rKEurhJ"
      },
      "source": [
        "## squeeze net (batch normed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FaEnQ_NOurhM",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dUKazngRurhP",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vUzpAJKLurhS",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d7c91ecb-cb43-4ec4-b2b8-c6e5d9ac55a8",
        "id": "DukMiNHlurhW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.254989\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.008565\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.891325\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.833889\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.778979\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.760738\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.696670\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.656897\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.603798\n",
            "\n",
            "Test set: Test loss: 1.5248, Accuracy: 2335/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 46.7%\n",
            "Better loss at Epoch 0: loss = 1.5248075771331793%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.544287\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.523842\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.489133\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.446984\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.466959\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.422284\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.438918\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.387044\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.362673\n",
            "\n",
            "Test set: Test loss: 1.2889, Accuracy: 2753/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 55.06%\n",
            "Better loss at Epoch 1: loss = 1.2889204400777823%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.331058\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.326261\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.275666\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.267827\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.271760\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.246176\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.243176\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.237823\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.207358\n",
            "\n",
            "Test set: Test loss: 1.1563, Accuracy: 2966/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 59.32%\n",
            "Better loss at Epoch 2: loss = 1.1562607598304753%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.149016\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.146674\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.136036\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.145797\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.158476\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.135565\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.182936\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.134498\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.130234\n",
            "\n",
            "Test set: Test loss: 1.0970, Accuracy: 3055/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 61.1%\n",
            "Better loss at Epoch 3: loss = 1.0970239305496217%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.086423\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.061556\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.043529\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.058078\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.056902\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.086598\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.041702\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.041049\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.048055\n",
            "\n",
            "Test set: Test loss: 1.0004, Accuracy: 3261/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 65.22%\n",
            "Better loss at Epoch 4: loss = 1.0004002201557163%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.011553\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.973156\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.993975\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.987632\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.963459\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.984113\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.973220\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.979989\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.981481\n",
            "\n",
            "Test set: Test loss: 0.9816, Accuracy: 3279/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 65.58%\n",
            "Better loss at Epoch 5: loss = 0.9815838193893432%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.939050\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.905158\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.919150\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.936113\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.920861\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.928371\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.912123\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.918133\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.896320\n",
            "\n",
            "Test set: Test loss: 0.9943, Accuracy: 3228/5000 (65%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.844708\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.833644\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.853859\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.877807\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.868423\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.854173\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.868674\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.895115\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.864086\n",
            "\n",
            "Test set: Test loss: 0.8982, Accuracy: 3411/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 68.22%\n",
            "Better loss at Epoch 7: loss = 0.8981940001249314%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.803997\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.797419\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.798641\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.787468\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.822385\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.823426\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.818008\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.838999\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.816660\n",
            "\n",
            "Test set: Test loss: 0.8874, Accuracy: 3441/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 68.82%\n",
            "Better loss at Epoch 8: loss = 0.8873860114812854%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.738717\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.757535\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.753179\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.773818\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.768542\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.760621\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.770162\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.772445\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.781387\n",
            "\n",
            "Test set: Test loss: 0.8745, Accuracy: 3496/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 69.92%\n",
            "Better loss at Epoch 9: loss = 0.8744727268815041%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.715680\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.709926\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.726690\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.701510\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.721320\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.729139\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.753483\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.755429\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.724130\n",
            "\n",
            "Test set: Test loss: 0.8840, Accuracy: 3511/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 70.22%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.678330\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.665234\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.674479\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.677818\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.706315\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.690150\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.677407\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.745731\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.716806\n",
            "\n",
            "Test set: Test loss: 0.8808, Accuracy: 3523/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 70.46%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.633980\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.615373\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.648935\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.631499\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.684666\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.684991\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.678887\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.658139\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.688995\n",
            "\n",
            "Test set: Test loss: 0.8849, Accuracy: 3499/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.607865\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.631685\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.596975\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.619157\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.616842\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.652842\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.669390\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.635200\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.640939\n",
            "\n",
            "Test set: Test loss: 0.8817, Accuracy: 3510/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.554883\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.587455\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.556974\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.594804\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.633010\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.578218\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.610154\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.598671\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.606157\n",
            "\n",
            "Test set: Test loss: 0.8508, Accuracy: 3564/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 14: accuracy = 71.28%\n",
            "Better loss at Epoch 14: loss = 0.850758360028267%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.516248\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.536805\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.521382\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.562418\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.573980\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.591322\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.566634\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.582960\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.573288\n",
            "\n",
            "Test set: Test loss: 0.9006, Accuracy: 3504/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.522272\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.503166\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.525373\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.547959\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.547879\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.550172\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.529494\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.565496\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.571321\n",
            "\n",
            "Test set: Test loss: 0.8559, Accuracy: 3612/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 72.24%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.469951\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.500623\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.488338\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.524380\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.514004\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.521013\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.528500\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.534971\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.547156\n",
            "\n",
            "Test set: Test loss: 0.8908, Accuracy: 3530/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.432111\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.485083\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.472476\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.469649\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.510874\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.502558\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.487359\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.535217\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.524273\n",
            "\n",
            "Test set: Test loss: 0.8883, Accuracy: 3598/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.412475\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.457383\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.448740\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.458893\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.468077\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.505217\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.479747\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.494021\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.489535\n",
            "\n",
            "Test set: Test loss: 0.8923, Accuracy: 3589/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.410030\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.421417\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.428733\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.448077\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.430700\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.445089\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.464096\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.468402\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.449311\n",
            "\n",
            "Test set: Test loss: 0.8763, Accuracy: 3619/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 20: accuracy = 72.38%\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.367993\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.369024\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.390799\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.435598\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.427996\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.429899\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.431624\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.470983\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.422810\n",
            "\n",
            "Test set: Test loss: 0.9145, Accuracy: 3589/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.344831\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.394948\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.396217\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.402859\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.391545\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.413843\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.428892\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.410824\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.419987\n",
            "\n",
            "Test set: Test loss: 0.9456, Accuracy: 3564/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.317255\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.373532\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.380299\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.374622\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.378776\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.406168\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.419436\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.417310\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.392822\n",
            "\n",
            "Test set: Test loss: 0.9795, Accuracy: 3575/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.327282\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.352977\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.336185\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.347612\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.346250\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.371862\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.377155\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.387698\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.381144\n",
            "\n",
            "Test set: Test loss: 0.9590, Accuracy: 3612/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.314383\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.296668\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.311370\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.356745\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.370717\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.363126\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.359175\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.395623\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.388531\n",
            "\n",
            "Test set: Test loss: 0.9982, Accuracy: 3592/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.298784\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.279404\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.303558\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.312122\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.344328\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.314087\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.349245\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.359022\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.360078\n",
            "\n",
            "Test set: Test loss: 0.9704, Accuracy: 3604/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.270135\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.316583\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.290508\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.318838\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.318203\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.326490\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.326182\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.340902\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.349378\n",
            "\n",
            "Test set: Test loss: 0.9828, Accuracy: 3627/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 27: accuracy = 72.54%\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.251439\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.281292\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.291654\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.306843\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.268875\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.329461\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.314306\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.327964\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.318180\n",
            "\n",
            "Test set: Test loss: 1.0059, Accuracy: 3573/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.244076\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.232233\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.241493\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.242215\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.264135\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.290688\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.334428\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.320323\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.331881\n",
            "\n",
            "Test set: Test loss: 1.0122, Accuracy: 3604/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.257591\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.231177\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.248152\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.259082\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.270403\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.281447\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.266797\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.286189\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.296238\n",
            "\n",
            "Test set: Test loss: 1.0179, Accuracy: 3603/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.206632\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.221339\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.241582\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.245495\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.237727\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.264792\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.284880\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.288987\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.303934\n",
            "\n",
            "Test set: Test loss: 1.0492, Accuracy: 3601/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.213159\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.232979\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.246914\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.227635\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.237946\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.263874\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.263421\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.251729\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.266908\n",
            "\n",
            "Test set: Test loss: 1.0729, Accuracy: 3602/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.189734\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.198645\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.223538\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.222220\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.245758\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.263024\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.235288\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.254024\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.268875\n",
            "\n",
            "Test set: Test loss: 1.1151, Accuracy: 3593/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.196718\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.202858\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.207185\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.228209\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.226835\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.244128\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.243283\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.219035\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.254967\n",
            "\n",
            "Test set: Test loss: 1.1099, Accuracy: 3588/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.201298\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.179753\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.187641\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.205856\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.215354\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.213383\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.229702\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.254046\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.252653\n",
            "\n",
            "Test set: Test loss: 1.0874, Accuracy: 3601/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.165171\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.169024\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.183904\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.181756\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.178772\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.173084\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.203071\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.198170\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.209100\n",
            "\n",
            "Test set: Test loss: 1.1255, Accuracy: 3621/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.180873\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.173883\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.215504\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.199268\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.209579\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.184052\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.214737\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.215265\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.207395\n",
            "\n",
            "Test set: Test loss: 1.1398, Accuracy: 3593/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.153828\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.149507\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.167420\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.178638\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.168891\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.209742\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.194499\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.212034\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.223182\n",
            "\n",
            "Test set: Test loss: 1.1284, Accuracy: 3607/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.151823\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.172665\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.173289\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.167123\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.180674\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.163076\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.168937\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.224193\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.185385\n",
            "\n",
            "Test set: Test loss: 1.1341, Accuracy: 3567/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.146117\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.143583\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.173846\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.181072\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.155006\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.171343\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.187745\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.173594\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.190002\n",
            "\n",
            "Test set: Test loss: 1.1852, Accuracy: 3579/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.147300\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.149856\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.162650\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.185583\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.172294\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.162874\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.171608\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.174126\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.162101\n",
            "\n",
            "Test set: Test loss: 1.1877, Accuracy: 3607/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.133985\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.146752\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.133519\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.148001\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.172292\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.172057\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.156436\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.167921\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.166977\n",
            "\n",
            "Test set: Test loss: 1.1788, Accuracy: 3598/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.150573\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.150498\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.140053\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.142522\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.155788\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.156286\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.152603\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.162036\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.167274\n",
            "\n",
            "Test set: Test loss: 1.2172, Accuracy: 3582/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.119995\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.116369\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.142095\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.136562\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.133767\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.134596\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.137990\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.140328\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.156994\n",
            "\n",
            "Test set: Test loss: 1.2521, Accuracy: 3570/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.135879\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.120837\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.126019\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.131224\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.139542\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.146596\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.132028\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.141532\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.132301\n",
            "\n",
            "Test set: Test loss: 1.2518, Accuracy: 3592/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.124778\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.113149\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.122081\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.124497\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.135471\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.123967\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.138712\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.137670\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.149558\n",
            "\n",
            "Test set: Test loss: 1.2526, Accuracy: 3628/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 46: accuracy = 72.56%\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.122531\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.117256\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.105628\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.124528\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.144911\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.125992\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.119619\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.128066\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.149010\n",
            "\n",
            "Test set: Test loss: 1.2524, Accuracy: 3602/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.122863\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.118386\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.099171\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.100075\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.119265\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.114388\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.123521\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.120222\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.153096\n",
            "\n",
            "Test set: Test loss: 1.2311, Accuracy: 3607/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.087940\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.116004\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.108428\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.110766\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.135673\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.133478\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.127377\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.122147\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.112352\n",
            "\n",
            "Test set: Test loss: 1.2532, Accuracy: 3592/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.099113\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.093528\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.101741\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.112792\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.105728\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.118928\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.149378\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.127064\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.108852\n",
            "\n",
            "Test set: Test loss: 1.2576, Accuracy: 3618/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.097983\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.095175\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.109617\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.123423\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.094250\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.102538\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.131183\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.100478\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.125400\n",
            "\n",
            "Test set: Test loss: 1.2805, Accuracy: 3603/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.093156\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.098958\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.089617\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.099800\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.105046\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.116655\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.101351\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.137480\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.110380\n",
            "\n",
            "Test set: Test loss: 1.2251, Accuracy: 3686/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 52: accuracy = 73.72%\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.084812\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.093313\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.096175\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.097530\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.082958\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.117374\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.109987\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.110984\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.113433\n",
            "\n",
            "Test set: Test loss: 1.2757, Accuracy: 3642/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.090158\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.077233\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.092595\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.095252\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.095613\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.116961\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.099766\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.093685\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.086758\n",
            "\n",
            "Test set: Test loss: 1.3463, Accuracy: 3576/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.094344\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.092152\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.094721\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.099350\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.087251\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.095496\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.106950\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.096987\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.099727\n",
            "\n",
            "Test set: Test loss: 1.3054, Accuracy: 3624/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.072034\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.085075\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.085781\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.091348\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.098396\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.092755\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.100320\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.095096\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.090536\n",
            "\n",
            "Test set: Test loss: 1.3197, Accuracy: 3626/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.067269\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.072435\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.078881\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.085700\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.094464\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.083921\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.084028\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.094560\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.088517\n",
            "\n",
            "Test set: Test loss: 1.3309, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.076858\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.062949\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.066968\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.092681\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.105448\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.092161\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.101585\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.101749\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.093169\n",
            "\n",
            "Test set: Test loss: 1.3407, Accuracy: 3613/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.069358\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.085839\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.091860\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.101082\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.083145\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.080637\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.102776\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.092457\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.110338\n",
            "\n",
            "Test set: Test loss: 1.2948, Accuracy: 3643/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.063645\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.069303\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.084897\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.060700\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.073289\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.070119\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.075756\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.091737\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.087895\n",
            "\n",
            "Test set: Test loss: 1.3004, Accuracy: 3681/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.056677\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.065638\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.058766\n",
            "Train Epoch: 61 [20000/50000 (40%)]\tTrain Loss: 0.074799\n",
            "Train Epoch: 61 [25000/50000 (50%)]\tTrain Loss: 0.079399\n",
            "Train Epoch: 61 [30000/50000 (60%)]\tTrain Loss: 0.067571\n",
            "Train Epoch: 61 [35000/50000 (70%)]\tTrain Loss: 0.077472\n",
            "Train Epoch: 61 [40000/50000 (80%)]\tTrain Loss: 0.097238\n",
            "Train Epoch: 61 [45000/50000 (90%)]\tTrain Loss: 0.081051\n",
            "\n",
            "Test set: Test loss: 1.3383, Accuracy: 3661/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 62: lr = 0.1\n",
            "Train Epoch: 62 [5000/50000 (10%)]\tTrain Loss: 0.066498\n",
            "Train Epoch: 62 [10000/50000 (20%)]\tTrain Loss: 0.060977\n",
            "Train Epoch: 62 [15000/50000 (30%)]\tTrain Loss: 0.065125\n",
            "Train Epoch: 62 [20000/50000 (40%)]\tTrain Loss: 0.073083\n",
            "Train Epoch: 62 [25000/50000 (50%)]\tTrain Loss: 0.079001\n",
            "Train Epoch: 62 [30000/50000 (60%)]\tTrain Loss: 0.077819\n",
            "Train Epoch: 62 [35000/50000 (70%)]\tTrain Loss: 0.079226\n",
            "Train Epoch: 62 [40000/50000 (80%)]\tTrain Loss: 0.085903\n",
            "Train Epoch: 62 [45000/50000 (90%)]\tTrain Loss: 0.068719\n",
            "\n",
            "Test set: Test loss: 1.3496, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 63: lr = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mstep_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e86c8b8f-0fe2-4799-96e8-e49799ac88da",
        "id": "ckrez8zdurha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWBwiVEQBoPBYHCJURAGg8FgcMn/A7wMVbvrR1zCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOxdxvMk5pwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 37.18875810782537 seconds\n",
            "Best accuracy: 73.72  Best training loss: 0.020710553973913193  Best validation loss: 0.850758360028267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d5b4e62a-fde1-4276-a0a8-ed0c359bb162",
        "id": "C8mICEuMurhd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]\n",
            "[1.6649585962295532, 1.513968825340271, 1.158576250076294, 0.8647364974021912, 1.0302220582962036, 0.8714985847473145, 0.7354030609130859, 1.0790632963180542, 0.8174920678138733, 0.5057325959205627, 0.686977744102478, 0.5849288702011108, 0.6560469269752502, 0.5848870873451233, 0.7902881503105164, 0.6683538556098938, 0.7632835507392883, 0.2575758993625641, 0.5791089534759521, 0.5804818868637085, 0.42531609535217285, 0.4527164399623871, 0.31680378317832947, 0.3272586166858673, 0.5007224678993225, 0.2454374134540558, 0.19363906979560852, 0.3956010341644287, 0.3267644941806793, 0.3780527114868164, 0.2296539843082428, 0.16767479479312897, 0.25428780913352966, 0.17200492322444916, 0.37030982971191406, 0.06840630620718002, 0.1671062409877777, 0.0960548147559166, 0.12348693609237671, 0.25269484519958496, 0.09033506363630295, 0.16092988848686218, 0.1312336027622223, 0.11296965926885605, 0.11194934695959091, 0.04551532864570618, 0.10051455348730087, 0.12731944024562836, 0.10249125212430954, 0.22633588314056396, 0.27270758152008057, 0.1948392689228058, 0.06700392067432404, 0.19208000600337982, 0.0985332578420639, 0.3866937756538391, 0.020710553973913193, 0.05295496806502342, 0.03812599182128906, 0.11851844936609268, 0.054147977381944656, 0.04246842488646507, 0.06668123602867126]\n",
            "[1.5248075771331793, 1.2889204400777823, 1.1562607598304753, 1.0970239305496217, 1.0004002201557163, 0.9815838193893432, 0.9942547810077669, 0.8981940001249314, 0.8873860114812854, 0.8744727268815041, 0.8840092676877973, 0.8807765081524845, 0.884877855181694, 0.8816783308982848, 0.850758360028267, 0.900597366690636, 0.8558677050471307, 0.8908464485406874, 0.8882989764213559, 0.8922531601786612, 0.8763105767965319, 0.9145175781846042, 0.9456301161646848, 0.9794919770956039, 0.9589641067385672, 0.9982126539945606, 0.9703693449497222, 0.9828382140398025, 1.0058842360973361, 1.012242673635483, 1.0178754380345347, 1.049204885959625, 1.0729285645484925, 1.1150964733958242, 1.109875584244728, 1.0874085769057271, 1.125464936494827, 1.1398225420713426, 1.1283725318312647, 1.134064757227898, 1.1851830860972405, 1.1876564157009117, 1.178773237764836, 1.2171819877624515, 1.2520802727341651, 1.2518309515714645, 1.2525804698467258, 1.252393800020218, 1.2310997369885446, 1.2531659656763083, 1.257634807229042, 1.2804955881834028, 1.225134716629982, 1.2757165080308914, 1.3462548696994785, 1.3054404222965235, 1.3196881395578386, 1.3309243184328077, 1.3407176494598387, 1.294849689602852, 1.3004332894086836, 1.3382762339711194, 1.3495629167556762]\n",
            "[46.7, 55.06, 59.32, 61.1, 65.22, 65.58, 64.56, 68.22, 68.82, 69.92, 70.22, 70.46, 69.98, 70.2, 71.28, 70.08, 72.24, 70.6, 71.96, 71.78, 72.38, 71.78, 71.28, 71.5, 72.24, 71.84, 72.08, 72.54, 71.46, 72.08, 72.06, 72.02, 72.04, 71.86, 71.76, 72.02, 72.42, 71.86, 72.14, 71.34, 71.58, 72.14, 71.96, 71.64, 71.4, 71.84, 72.56, 72.04, 72.14, 71.84, 72.36, 72.06, 73.72, 72.84, 71.52, 72.48, 72.52, 73.06, 72.26, 72.86, 73.62, 73.22, 72.82]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CyZN4Sx5Q7_W"
      },
      "source": [
        "## squeeze net (batch normed in fire module)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nVHdHWqvQ7_a",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                #nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ZujICcOQ7_e",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5W-KhZa9Q7_g",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0470cb64-84d1-4c68-b1a6-110d15a03cd1",
        "id": "UxXSMGujQ7_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.275788\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.035182\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.900537\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.819399\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.787741\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.740649\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.682548\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.656160\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.648021\n",
            "\n",
            "Test set: Test loss: 1.5243, Accuracy: 2279/5000 (46%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 45.58%\n",
            "Better loss at Epoch 0: loss = 1.5243100726604462%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.571942\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.574569\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.493208\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.480752\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.469916\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.454353\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.452217\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.414874\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.403121\n",
            "\n",
            "Test set: Test loss: 1.3452, Accuracy: 2635/5000 (53%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 52.7%\n",
            "Better loss at Epoch 1: loss = 1.3452409231662752%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.347069\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.329830\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.322165\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.339251\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.317399\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.308763\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.275732\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.268064\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.275715\n",
            "\n",
            "Test set: Test loss: 1.2164, Accuracy: 2877/5000 (58%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 57.54%\n",
            "Better loss at Epoch 2: loss = 1.2163545393943787%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.205988\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.203660\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.196310\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.198763\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.167980\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.183701\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.183998\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.174374\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.146437\n",
            "\n",
            "Test set: Test loss: 1.1032, Accuracy: 3031/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 60.62%\n",
            "Better loss at Epoch 3: loss = 1.1031716549396522%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.098117\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.067181\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.067824\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.102236\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.077096\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.064754\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.057304\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.094995\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.062076\n",
            "\n",
            "Test set: Test loss: 1.0323, Accuracy: 3172/5000 (63%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 63.44%\n",
            "Better loss at Epoch 4: loss = 1.0323396480083469%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.964751\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 1.020998\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 1.025119\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 1.008348\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 1.005637\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.968638\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.988631\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.986239\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.994888\n",
            "\n",
            "Test set: Test loss: 1.0179, Accuracy: 3209/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 64.18%\n",
            "Better loss at Epoch 5: loss = 1.0179479336738584%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.921789\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.925104\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.941949\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.916485\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.928316\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.923800\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.898366\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.922986\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.918730\n",
            "\n",
            "Test set: Test loss: 0.9558, Accuracy: 3364/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 67.28%\n",
            "Better loss at Epoch 6: loss = 0.9558254581689839%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.855538\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.850252\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.864215\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.855430\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.875674\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.878050\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.827870\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.875257\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.885901\n",
            "\n",
            "Test set: Test loss: 0.9273, Accuracy: 3393/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 67.86%\n",
            "Better loss at Epoch 7: loss = 0.9272959387302401%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.799396\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.779619\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.793518\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.809297\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.810961\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.797007\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.815304\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.826435\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.826418\n",
            "\n",
            "Test set: Test loss: 0.9767, Accuracy: 3321/5000 (66%)\n",
            "\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.732581\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.772556\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.742380\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.763270\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.746767\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.756731\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.786263\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.780498\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.759800\n",
            "\n",
            "Test set: Test loss: 0.8551, Accuracy: 3524/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 70.48%\n",
            "Better loss at Epoch 9: loss = 0.8551315557956698%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.684213\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.692004\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.672483\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.716999\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.754469\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.712814\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.740963\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.715803\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.738058\n",
            "\n",
            "Test set: Test loss: 0.8602, Accuracy: 3529/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 70.58%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.648247\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.654483\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.671629\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.703383\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.662738\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.657389\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.675877\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.687840\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.697307\n",
            "\n",
            "Test set: Test loss: 0.8382, Accuracy: 3563/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 71.26%\n",
            "Better loss at Epoch 11: loss = 0.8382148900628088%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.618743\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.601326\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.642081\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.610262\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.667616\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.639943\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.649821\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.666158\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.669538\n",
            "\n",
            "Test set: Test loss: 0.8666, Accuracy: 3551/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.591967\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.584093\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.577702\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.592723\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.634603\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.604694\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.611585\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.605604\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.624835\n",
            "\n",
            "Test set: Test loss: 0.8242, Accuracy: 3589/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 71.78%\n",
            "Better loss at Epoch 13: loss = 0.8242081516981126%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.544366\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.553971\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.588704\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.566339\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.572478\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.580510\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.566259\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.595454\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.601373\n",
            "\n",
            "Test set: Test loss: 0.8477, Accuracy: 3562/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.518462\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.534938\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.538992\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.530755\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.564304\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.561963\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.568070\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.558033\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.536086\n",
            "\n",
            "Test set: Test loss: 0.8550, Accuracy: 3577/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.454384\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.492092\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.479330\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.491389\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.535919\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.528569\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.523169\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.521881\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.516043\n",
            "\n",
            "Test set: Test loss: 0.8813, Accuracy: 3568/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.463765\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.453118\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.484694\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.490199\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.480078\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.502224\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.501678\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.513512\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.500600\n",
            "\n",
            "Test set: Test loss: 0.8464, Accuracy: 3610/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 72.2%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.419300\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.427793\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.451113\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.471969\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.474513\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.459418\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.469860\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.462687\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.493490\n",
            "\n",
            "Test set: Test loss: 0.8642, Accuracy: 3644/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 72.88%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.402601\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.414682\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.415630\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.453249\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.425750\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.453944\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.450708\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.473069\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.467171\n",
            "\n",
            "Test set: Test loss: 0.8642, Accuracy: 3640/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.385377\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.399106\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.422629\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.415927\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.428788\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.447620\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.446016\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.444279\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.450302\n",
            "\n",
            "Test set: Test loss: 0.8548, Accuracy: 3643/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.360913\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.392449\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.399797\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.390950\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.378283\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.417605\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.390738\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.421162\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.426525\n",
            "\n",
            "Test set: Test loss: 0.9065, Accuracy: 3591/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.342039\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.357905\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.338013\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.386236\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.381361\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.376201\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.389329\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.384688\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.396242\n",
            "\n",
            "Test set: Test loss: 0.8705, Accuracy: 3670/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 22: accuracy = 73.4%\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.322757\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.350900\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.337178\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.360263\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.350416\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.362978\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.361430\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.357727\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.374181\n",
            "\n",
            "Test set: Test loss: 0.9489, Accuracy: 3621/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.310863\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.311263\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.309177\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.337765\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.349240\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.380688\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.356552\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.359787\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.372114\n",
            "\n",
            "Test set: Test loss: 0.9376, Accuracy: 3634/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.285584\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.266616\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.303907\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.325390\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.326337\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.318522\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.333362\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.372538\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.362851\n",
            "\n",
            "Test set: Test loss: 0.9395, Accuracy: 3666/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.261071\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.258298\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.276725\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.298723\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.299269\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.312458\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.324567\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.318660\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.337095\n",
            "\n",
            "Test set: Test loss: 0.9498, Accuracy: 3654/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.255970\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.262940\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.263347\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.266286\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.297469\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.278227\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.337091\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.312225\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.287935\n",
            "\n",
            "Test set: Test loss: 1.0006, Accuracy: 3615/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.217959\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.261817\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.256573\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.271837\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.272986\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.288583\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.300104\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.295047\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.285586\n",
            "\n",
            "Test set: Test loss: 0.9776, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.248567\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.244860\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.256240\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.266353\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.281205\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.257556\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.275301\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.271350\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.287453\n",
            "\n",
            "Test set: Test loss: 0.9861, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 29: accuracy = 73.7%\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.207889\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.201075\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.243955\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.251286\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.253713\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.269771\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.255259\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.283775\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.296602\n",
            "\n",
            "Test set: Test loss: 0.9770, Accuracy: 3700/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 30: accuracy = 74.0%\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.200638\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.193933\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.213691\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.203332\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.240922\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.247187\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.230374\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.259096\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.270848\n",
            "\n",
            "Test set: Test loss: 1.0189, Accuracy: 3630/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.214014\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.188343\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.209021\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.219906\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.219892\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.243833\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.227944\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.247799\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.263478\n",
            "\n",
            "Test set: Test loss: 1.0143, Accuracy: 3654/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Calculating gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "24c68142-975c-482c-ad19-5f0302489b39",
        "id": "shhVMl2UQ7_n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESkwSvwiIgXm/wErUsCoC4mZYgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 34.52487746060626 seconds\n",
            "Best accuracy: 74.0  Best training loss: 0.2210252732038498  Best validation loss: 0.8242081516981126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c3cd2fbe-9f51-4d88-b77a-c77b12441878",
        "id": "mimNpWkKQ7_s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
            "[1.6375187635421753, 1.6945152282714844, 0.8258227705955505, 1.0054441690444946, 1.1372429132461548, 1.0158512592315674, 0.8369529843330383, 0.8901993036270142, 0.878383457660675, 0.9477666616439819, 0.652128279209137, 0.8509068489074707, 1.040832281112671, 0.4873678982257843, 0.4660559892654419, 0.34873801469802856, 0.37246420979499817, 0.49928709864616394, 0.40859001874923706, 0.4186958074569702, 0.45320525765419006, 0.33774489164352417, 0.4745442867279053, 0.24109862744808197, 0.22999568283557892, 0.27735060453414917, 0.2891758978366852, 0.31933245062828064, 0.2360219955444336, 0.34744197130203247, 0.31519293785095215, 0.30355703830718994, 0.2210252732038498]\n",
            "[1.5243100726604462, 1.3452409231662752, 1.2163545393943787, 1.1031716549396522, 1.0323396480083469, 1.0179479336738584, 0.9558254581689839, 0.9272959387302401, 0.9766502523422239, 0.8551315557956698, 0.8602081745862963, 0.8382148900628088, 0.8665885454416276, 0.8242081516981126, 0.8476556080579759, 0.854965708255768, 0.8812793996930123, 0.8463913616538049, 0.8642362391948699, 0.8642498278617858, 0.8547804021835326, 0.9065471035242081, 0.8704702186584466, 0.9489273744821544, 0.9375735142827032, 0.9395139983296394, 0.9498112824559217, 1.0005782005190849, 0.9775870040059091, 0.9860983976721767, 0.9770144182443616, 1.01888631939888, 1.0143009093403816]\n",
            "[45.58, 52.7, 57.54, 60.62, 63.44, 64.18, 67.28, 67.86, 66.42, 70.48, 70.58, 71.26, 71.02, 71.78, 71.24, 71.54, 71.36, 72.2, 72.88, 72.8, 72.86, 71.82, 73.4, 72.42, 72.68, 73.32, 73.08, 72.3, 73.06, 73.7, 74.0, 72.6, 73.08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FuptxTy8PlpA"
      },
      "source": [
        "## squeeze net (batch normed in main)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eN_FbamyPlpC",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9P4QmSmiPlpG",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d19dVgb9PlpK",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a9a5ca8c-cbda-4c24-8f11-25774701bd45",
        "id": "iXtRyuQnPlpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.322997\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 2.302572\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 9.6%\n",
            "Better loss at Epoch 0: loss = 2.3025844097137442%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 2.325610\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 2.302584\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 2.302584\n",
            "\n",
            "Test set: Test loss: 2.3026, Accuracy: 480/5000 (10%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 2.325610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#   plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#reshape for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Clear gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e86c8b8f-0fe2-4799-96e8-e49799ac88da",
        "id": "Z7vfAQyTPlpv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWBwiVEQBoPBYHCJURAGg8FgcMn/A7wMVbvrR1zCAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOxdxvMk5pwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 37.18875810782537 seconds\n",
            "Best accuracy: 73.72  Best training loss: 0.020710553973913193  Best validation loss: 0.850758360028267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d5b4e62a-fde1-4276-a0a8-ed0c359bb162",
        "id": "Swzmb7_5Plp0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62]\n",
            "[1.6649585962295532, 1.513968825340271, 1.158576250076294, 0.8647364974021912, 1.0302220582962036, 0.8714985847473145, 0.7354030609130859, 1.0790632963180542, 0.8174920678138733, 0.5057325959205627, 0.686977744102478, 0.5849288702011108, 0.6560469269752502, 0.5848870873451233, 0.7902881503105164, 0.6683538556098938, 0.7632835507392883, 0.2575758993625641, 0.5791089534759521, 0.5804818868637085, 0.42531609535217285, 0.4527164399623871, 0.31680378317832947, 0.3272586166858673, 0.5007224678993225, 0.2454374134540558, 0.19363906979560852, 0.3956010341644287, 0.3267644941806793, 0.3780527114868164, 0.2296539843082428, 0.16767479479312897, 0.25428780913352966, 0.17200492322444916, 0.37030982971191406, 0.06840630620718002, 0.1671062409877777, 0.0960548147559166, 0.12348693609237671, 0.25269484519958496, 0.09033506363630295, 0.16092988848686218, 0.1312336027622223, 0.11296965926885605, 0.11194934695959091, 0.04551532864570618, 0.10051455348730087, 0.12731944024562836, 0.10249125212430954, 0.22633588314056396, 0.27270758152008057, 0.1948392689228058, 0.06700392067432404, 0.19208000600337982, 0.0985332578420639, 0.3866937756538391, 0.020710553973913193, 0.05295496806502342, 0.03812599182128906, 0.11851844936609268, 0.054147977381944656, 0.04246842488646507, 0.06668123602867126]\n",
            "[1.5248075771331793, 1.2889204400777823, 1.1562607598304753, 1.0970239305496217, 1.0004002201557163, 0.9815838193893432, 0.9942547810077669, 0.8981940001249314, 0.8873860114812854, 0.8744727268815041, 0.8840092676877973, 0.8807765081524845, 0.884877855181694, 0.8816783308982848, 0.850758360028267, 0.900597366690636, 0.8558677050471307, 0.8908464485406874, 0.8882989764213559, 0.8922531601786612, 0.8763105767965319, 0.9145175781846042, 0.9456301161646848, 0.9794919770956039, 0.9589641067385672, 0.9982126539945606, 0.9703693449497222, 0.9828382140398025, 1.0058842360973361, 1.012242673635483, 1.0178754380345347, 1.049204885959625, 1.0729285645484925, 1.1150964733958242, 1.109875584244728, 1.0874085769057271, 1.125464936494827, 1.1398225420713426, 1.1283725318312647, 1.134064757227898, 1.1851830860972405, 1.1876564157009117, 1.178773237764836, 1.2171819877624515, 1.2520802727341651, 1.2518309515714645, 1.2525804698467258, 1.252393800020218, 1.2310997369885446, 1.2531659656763083, 1.257634807229042, 1.2804955881834028, 1.225134716629982, 1.2757165080308914, 1.3462548696994785, 1.3054404222965235, 1.3196881395578386, 1.3309243184328077, 1.3407176494598387, 1.294849689602852, 1.3004332894086836, 1.3382762339711194, 1.3495629167556762]\n",
            "[46.7, 55.06, 59.32, 61.1, 65.22, 65.58, 64.56, 68.22, 68.82, 69.92, 70.22, 70.46, 69.98, 70.2, 71.28, 70.08, 72.24, 70.6, 71.96, 71.78, 72.38, 71.78, 71.28, 71.5, 72.24, 71.84, 72.08, 72.54, 71.46, 72.08, 72.06, 72.02, 72.04, 71.86, 71.76, 72.02, 72.42, 71.86, 72.14, 71.34, 71.58, 72.14, 71.96, 71.64, 71.4, 71.84, 72.56, 72.04, 72.14, 71.84, 72.36, 72.06, 73.72, 72.84, 71.52, 72.48, 72.52, 73.06, 72.26, 72.86, 73.62, 73.22, 72.82]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LxfbOdOWVIRm"
      },
      "source": [
        "## squeeze net (batch normed) (removed 2 fire layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZMQWRvHzVIRq",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.BatchNorm2d(96),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(384, 48, 192, 192),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(384, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PHPY3nVlVIRv",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "#optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OAwjDPenVIR3",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "32fcefb5-20a2-4c82-a966-1e6b533ea74a",
        "id": "EskXhohmVIR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.184084\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.929844\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.819292\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.741655\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.706633\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.681803\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.605038\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.587396\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.529746\n",
            "\n",
            "Test set: Test loss: 1.4513, Accuracy: 2442/5000 (49%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 48.84%\n",
            "Better loss at Epoch 0: loss = 1.4512908875942232%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.497452\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.466453\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.408232\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.409695\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.386167\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.343840\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.320887\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.346189\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.289250\n",
            "\n",
            "Test set: Test loss: 1.2303, Accuracy: 2856/5000 (57%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 57.12%\n",
            "Better loss at Epoch 1: loss = 1.230314158201218%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.245990\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.256596\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.214742\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.218319\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.204280\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.199459\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.192545\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.163139\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.197833\n",
            "\n",
            "Test set: Test loss: 1.1213, Accuracy: 3053/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 61.06%\n",
            "Better loss at Epoch 2: loss = 1.1212938576936726%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.084931\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.121492\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.109987\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.094818\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.101165\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.089629\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.100273\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.036532\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.105370\n",
            "\n",
            "Test set: Test loss: 1.0364, Accuracy: 3179/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 63.58%\n",
            "Better loss at Epoch 3: loss = 1.0364125180244443%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.999072\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.018061\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.002516\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.031973\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.020956\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.016716\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.016129\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.021329\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.002685\n",
            "\n",
            "Test set: Test loss: 0.9866, Accuracy: 3279/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 65.58%\n",
            "Better loss at Epoch 4: loss = 0.9866232055425642%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.948168\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.939341\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.943918\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.954053\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.937293\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.971225\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.959646\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.958383\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.923795\n",
            "\n",
            "Test set: Test loss: 0.9761, Accuracy: 3269/5000 (65%)\n",
            "\n",
            "Better loss at Epoch 5: loss = 0.9760835993289948%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.875036\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.888551\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.876865\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.902152\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.890894\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.904777\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.904916\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.914034\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.897548\n",
            "\n",
            "Test set: Test loss: 0.9363, Accuracy: 3398/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 67.96%\n",
            "Better loss at Epoch 6: loss = 0.9363281095027927%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.817871\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.819128\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.826431\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.837108\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.838185\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.880538\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.845669\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.833864\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.858048\n",
            "\n",
            "Test set: Test loss: 0.8882, Accuracy: 3420/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 68.4%\n",
            "Better loss at Epoch 7: loss = 0.8882440918684006%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.771141\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.776605\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.802094\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.804487\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.781074\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.778782\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.789408\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.787964\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.812648\n",
            "\n",
            "Test set: Test loss: 0.8800, Accuracy: 3462/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 69.24%\n",
            "Better loss at Epoch 8: loss = 0.8800209221243855%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.704460\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.704977\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.738670\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.753057\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.753290\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.777962\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.728129\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.791201\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.768822\n",
            "\n",
            "Test set: Test loss: 0.8955, Accuracy: 3457/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.685680\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.687537\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.716093\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.684735\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.723149\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.724052\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.752798\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.685739\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.725940\n",
            "\n",
            "Test set: Test loss: 0.8869, Accuracy: 3479/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 69.58%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.648091\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.644734\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.672643\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.690556\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.673769\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.702812\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.651910\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.683386\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.713213\n",
            "\n",
            "Test set: Test loss: 0.8854, Accuracy: 3464/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.612734\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.611434\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.634689\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.645924\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.659536\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.637783\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.661408\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.691803\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.679425\n",
            "\n",
            "Test set: Test loss: 0.8905, Accuracy: 3464/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.581508\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.575575\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.601498\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.622088\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.621084\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.611357\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.625268\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.625113\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.624611\n",
            "\n",
            "Test set: Test loss: 0.8609, Accuracy: 3526/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 70.52%\n",
            "Better loss at Epoch 13: loss = 0.8609130695462225%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.549458\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.558830\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.580517\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.577105\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.600751\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.573793\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.585839\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.577364\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.598605\n",
            "\n",
            "Test set: Test loss: 0.8883, Accuracy: 3504/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.522772\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.510918\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.533524\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.566422\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.553674\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.562067\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.548416\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.561516\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.598543\n",
            "\n",
            "Test set: Test loss: 0.8763, Accuracy: 3511/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.498301\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.513135\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.511922\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.521399\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.552252\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.557127\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.545707\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.579673\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.569499\n",
            "\n",
            "Test set: Test loss: 0.9106, Accuracy: 3510/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.479951\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.474761\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.510154\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.469292\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.523257\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.512952\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.522313\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.519323\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.531797\n",
            "\n",
            "Test set: Test loss: 0.8778, Accuracy: 3585/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 71.7%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.429651\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.453477\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.490525\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.466981\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.475649\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.509849\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.511023\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.502292\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.505909\n",
            "\n",
            "Test set: Test loss: 0.8934, Accuracy: 3568/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.420553\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.422570\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.448520\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.459244\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.485374\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.477662\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.491210\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.483540\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.482135\n",
            "\n",
            "Test set: Test loss: 0.9012, Accuracy: 3544/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.394262\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.404775\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.431585\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.445244\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.433171\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.456548\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.448817\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.454703\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.486641\n",
            "\n",
            "Test set: Test loss: 0.9279, Accuracy: 3580/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.378642\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.391367\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.422156\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.400787\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.422297\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.429348\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.437291\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.440935\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.450878\n",
            "\n",
            "Test set: Test loss: 0.9642, Accuracy: 3540/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.383192\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.403660\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.405935\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.381713\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.405093\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.424208\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.463900\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.426106\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.418207\n",
            "\n",
            "Test set: Test loss: 0.9719, Accuracy: 3520/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.342547\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.378256\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.363470\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.370165\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.401915\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.406089\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.411808\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.404676\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.421438\n",
            "\n",
            "Test set: Test loss: 0.9787, Accuracy: 3477/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.322766\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.336072\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.347186\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.353869\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.358591\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.376994\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.402898\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.403838\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.418188\n",
            "\n",
            "Test set: Test loss: 0.9601, Accuracy: 3529/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.347328\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.318085\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.327777\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.335944\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.380045\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.358861\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.370100\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.375668\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.371599\n",
            "\n",
            "Test set: Test loss: 0.9764, Accuracy: 3564/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.297778\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.285395\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.324987\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.324446\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.345949\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.327344\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.336607\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.357488\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.364103\n",
            "\n",
            "Test set: Test loss: 1.0113, Accuracy: 3534/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.284433\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.294341\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.314833\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.285873\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.335059\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.352765\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.347981\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.340484\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.367629\n",
            "\n",
            "Test set: Test loss: 0.9798, Accuracy: 3585/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.263725\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.254550\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.309254\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.318530\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.311321\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.341276\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.349110\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.340763\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.345361\n",
            "\n",
            "Test set: Test loss: 0.9796, Accuracy: 3577/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.246169\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.269234\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.291550\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.315588\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.304218\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.334858\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.318968\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.328920\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.323965\n",
            "\n",
            "Test set: Test loss: 1.0423, Accuracy: 3533/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.254025\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.250296\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.247479\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.296513\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.305955\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.291483\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.315988\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.304477\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.309013\n",
            "\n",
            "Test set: Test loss: 1.0873, Accuracy: 3516/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.269634\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.237370\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.257276\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.264159\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.270276\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.296272\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.298012\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.321882\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.281046\n",
            "\n",
            "Test set: Test loss: 1.0377, Accuracy: 3534/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.223166\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.222078\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.238827\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.267651\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.265277\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.273903\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.264948\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.281343\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.289321\n",
            "\n",
            "Test set: Test loss: 1.0729, Accuracy: 3566/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.236815\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.227035\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.246268\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.240565\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.257285\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.252007\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.261591\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.281028\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.279596\n",
            "\n",
            "Test set: Test loss: 1.0793, Accuracy: 3530/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.220118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8892740e-ee93-4370-8074-30d5fb50e057",
        "id": "s5Kl8V91VISD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncv8HJELfM9/a6G4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 29.232933152676516 seconds\n",
            "Best accuracy: 71.7  Best training loss: 0.14262917637825012  Best validation loss: 0.8609130695462225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "42bef4b4-0aa7-46d9-99bf-1b6a01d9f59f",
        "id": "ehexviO-VISJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
            "[1.5017831325531006, 1.206808090209961, 1.11640465259552, 0.9419748783111572, 0.981797993183136, 1.0940289497375488, 1.0751595497131348, 1.2481834888458252, 0.7877085208892822, 0.740404486656189, 0.6367399096488953, 0.636405885219574, 0.7885422706604004, 0.760142982006073, 0.40021881461143494, 0.49494704604148865, 0.49913445115089417, 0.5290067195892334, 0.45177963376045227, 0.45988601446151733, 0.6052672863006592, 0.3741489052772522, 0.39897099137306213, 0.4879230260848999, 0.3772616684436798, 0.6696516275405884, 0.26215508580207825, 0.3734387159347534, 0.1892787516117096, 0.198419451713562, 0.28234049677848816, 0.3823551535606384, 0.14262917637825012, 0.29431331157684326]\n",
            "[1.4512908875942232, 1.230314158201218, 1.1212938576936726, 1.0364125180244443, 0.9866232055425642, 0.9760835993289948, 0.9363281095027927, 0.8882440918684006, 0.8800209221243855, 0.8954645860195155, 0.8869282919168473, 0.8854312765598296, 0.8905455899238585, 0.8609130695462225, 0.8882868593931199, 0.8762759312987329, 0.9106329011917114, 0.8777963423728944, 0.893355422616005, 0.9012181228399275, 0.9278731474280352, 0.9642206996679306, 0.9719120573997497, 0.9787364530563357, 0.9601302757859227, 0.9764237287640571, 1.0113333132863045, 0.9797576153278352, 0.9796304324269294, 1.0422743719816212, 1.087254037559033, 1.037725213766098, 1.0728737273812294, 1.0792823150753972]\n",
            "[48.84, 57.12, 61.06, 63.58, 65.58, 65.38, 67.96, 68.4, 69.24, 69.14, 69.58, 69.28, 69.28, 70.52, 70.08, 70.22, 70.2, 71.7, 71.36, 70.88, 71.6, 70.8, 70.4, 69.54, 70.58, 71.28, 70.68, 71.7, 71.54, 70.66, 70.32, 70.68, 71.32, 70.6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "20ODMZClJ4tN"
      },
      "source": [
        "## squeeze skip residuals (batch normed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PliKiJ2nJ4tP",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "        self.features4 = nn.Sequential(\n",
        "            Fire(384, 64, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        \n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "\n",
        "        x = self.features4(x)\n",
        "\n",
        "        residual4 = x\n",
        "        x = self.block4(x)\n",
        "        x += residual4\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y1cJx4t9J4tT",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JfK_Kg6UJ4tW",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a681d780-3499-4b09-cd2b-e0f53d579afb",
        "id": "5kuA1voIJ4ta",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.222965\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.939407\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.807879\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.728481\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.652416\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.607269\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.571071\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.504610\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.455477\n",
            "\n",
            "Test set: Test loss: 1.3356, Accuracy: 2731/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 54.62%\n",
            "Better loss at Epoch 0: loss = 1.3355705612897872%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.356978\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.368610\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.343431\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.323758\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.314335\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.247087\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.264141\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.267696\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.261534\n",
            "\n",
            "Test set: Test loss: 1.1672, Accuracy: 2948/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 58.96%\n",
            "Better loss at Epoch 1: loss = 1.1671756452322006%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.167207\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.148102\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.156774\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.137999\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.142440\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.141630\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.128670\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.116582\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.088528\n",
            "\n",
            "Test set: Test loss: 1.0380, Accuracy: 3224/5000 (64%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 64.48%\n",
            "Better loss at Epoch 2: loss = 1.0379716897010804%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.063154\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.047021\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.027103\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.013623\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.033072\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.002478\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.019153\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.002833\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.001714\n",
            "\n",
            "Test set: Test loss: 0.9809, Accuracy: 3291/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 65.82%\n",
            "Better loss at Epoch 3: loss = 0.9809377205371861%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.938723\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.911909\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.945937\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.948210\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.948164\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.919546\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.930408\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.934383\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.956674\n",
            "\n",
            "Test set: Test loss: 0.9075, Accuracy: 3440/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 68.8%\n",
            "Better loss at Epoch 4: loss = 0.9075101101398466%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.864063\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.859135\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.870228\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.853432\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.895756\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.863173\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.872269\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.884508\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.878067\n",
            "\n",
            "Test set: Test loss: 0.9010, Accuracy: 3443/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 68.86%\n",
            "Better loss at Epoch 5: loss = 0.9009622794389723%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.805682\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.793151\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.807198\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.806269\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.825858\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.777383\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.823166\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.833555\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.786643\n",
            "\n",
            "Test set: Test loss: 0.9328, Accuracy: 3374/5000 (67%)\n",
            "\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.706586\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.752996\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.739728\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.783034\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.756382\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.764425\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.767665\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.744859\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.766214\n",
            "\n",
            "Test set: Test loss: 0.8869, Accuracy: 3442/5000 (69%)\n",
            "\n",
            "Better loss at Epoch 7: loss = 0.8868947577476499%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.679594\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.688444\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.712591\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.698960\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.721691\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.726322\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.730487\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.727245\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.727766\n",
            "\n",
            "Test set: Test loss: 0.8381, Accuracy: 3538/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 70.76%\n",
            "Better loss at Epoch 8: loss = 0.8381421715021133%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.635775\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.653158\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.646406\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.650614\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.660451\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.681183\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.680580\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.675693\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.653846\n",
            "\n",
            "Test set: Test loss: 0.8120, Accuracy: 3604/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 72.08%\n",
            "Better loss at Epoch 9: loss = 0.8119879665970803%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.597880\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.603130\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.646145\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.606644\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.650523\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.647364\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.607896\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.627872\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.654379\n",
            "\n",
            "Test set: Test loss: 0.8209, Accuracy: 3604/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.550040\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.574083\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.581872\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.594443\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.556894\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.597403\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.602342\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.618298\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.611448\n",
            "\n",
            "Test set: Test loss: 0.8225, Accuracy: 3621/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 72.42%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.523993\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.528909\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.544514\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.539602\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.563798\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.565250\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.566198\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.590735\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.569695\n",
            "\n",
            "Test set: Test loss: 0.7865, Accuracy: 3672/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 73.44%\n",
            "Better loss at Epoch 12: loss = 0.7864684224128718%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.501674\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.508237\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.504855\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.520578\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.494842\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.528101\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.553391\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.544421\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.586956\n",
            "\n",
            "Test set: Test loss: 0.7879, Accuracy: 3670/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.463460\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.465001\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.470931\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.479623\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.499259\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.513031\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.501717\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.529661\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.515777\n",
            "\n",
            "Test set: Test loss: 0.8283, Accuracy: 3667/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.437602\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.470311\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.457637\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.454981\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.478769\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.470473\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.471210\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.496497\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.520543\n",
            "\n",
            "Test set: Test loss: 0.8268, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 73.7%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.424386\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.425838\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.429769\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.450920\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.452463\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.417020\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.462475\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.438496\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.486263\n",
            "\n",
            "Test set: Test loss: 0.8266, Accuracy: 3685/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.376884\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.397937\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.400767\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.410091\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.426084\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.430114\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.442574\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.443787\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.445486\n",
            "\n",
            "Test set: Test loss: 0.8419, Accuracy: 3671/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.345018\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.357959\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.382467\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.413080\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.383883\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.400368\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.431345\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.435552\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.428006\n",
            "\n",
            "Test set: Test loss: 0.8419, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 18: accuracy = 74.02%\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.361527\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.359488\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.358611\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.373480\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.377206\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.382855\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.391740\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.426863\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.409938\n",
            "\n",
            "Test set: Test loss: 0.8303, Accuracy: 3695/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.339809\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.310664\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.316780\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.341153\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.368168\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.368496\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.387331\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.386506\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.378955\n",
            "\n",
            "Test set: Test loss: 0.8566, Accuracy: 3660/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.302993\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.290127\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.310604\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.332687\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.359011\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.369050\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.351384\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.350830\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.373793\n",
            "\n",
            "Test set: Test loss: 0.8643, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.299095\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.289081\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.325643\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.306981\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.340390\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.329046\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.340700\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.332202\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.349783\n",
            "\n",
            "Test set: Test loss: 0.9091, Accuracy: 3618/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.275153\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.257415\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.310379\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.334483\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.314025\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.311892\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.323154\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.322447\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.343275\n",
            "\n",
            "Test set: Test loss: 0.8636, Accuracy: 3679/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.254039\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.263476\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.288078\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.305031\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.283865\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.272878\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.288864\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.312710\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.331167\n",
            "\n",
            "Test set: Test loss: 0.8848, Accuracy: 3731/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 24: accuracy = 74.62%\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.231487\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.250840\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.275061\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.274786\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.283274\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.268526\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.297154\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.281158\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.314123\n",
            "\n",
            "Test set: Test loss: 0.9191, Accuracy: 3678/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.218664\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.226952\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.245103\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.268128\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.272010\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.297891\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.266755\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.279313\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.284175\n",
            "\n",
            "Test set: Test loss: 0.9776, Accuracy: 3642/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.231964\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.230425\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.254819\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.252030\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.247442\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.292519\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.277164\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.273325\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.297300\n",
            "\n",
            "Test set: Test loss: 0.9214, Accuracy: 3716/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.207569\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.203389\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.222099\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.233046\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.243542\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.247873\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.248437\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.262868\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.249463\n",
            "\n",
            "Test set: Test loss: 0.9553, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.213866\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.193634\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.232897\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.248100\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.234996\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.248398\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.243712\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.257924\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.241396\n",
            "\n",
            "Test set: Test loss: 0.9591, Accuracy: 3693/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.189379\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.181629\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.189667\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.202087\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.207052\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.205354\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.232161\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.222352\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.239291\n",
            "\n",
            "Test set: Test loss: 1.0004, Accuracy: 3697/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.192741\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.188092\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.203991\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.212943\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.211389\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.205551\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.237077\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.216934\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.258466\n",
            "\n",
            "Test set: Test loss: 0.9790, Accuracy: 3700/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.180088\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.174437\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.195413\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.196430\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.191064\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.193654\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.213182\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.202931\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.202375\n",
            "\n",
            "Test set: Test loss: 0.9719, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.183533\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.195305\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.176093\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.183864\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.183314\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.191266\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.225984\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.219194\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.221897\n",
            "\n",
            "Test set: Test loss: 1.0134, Accuracy: 3686/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.160825\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.165193\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.183991\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.148140\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.175966\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.178691\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.186339\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.192050\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.191580\n",
            "\n",
            "Test set: Test loss: 1.0282, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.148046\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.149595\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.177201\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.173759\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.187000\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.163771\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.186939\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.183108\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.188970\n",
            "\n",
            "Test set: Test loss: 1.0491, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.151071\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.153605\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.163887\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.176064\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.190895\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.158814\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.163832\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.173317\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.173809\n",
            "\n",
            "Test set: Test loss: 1.0022, Accuracy: 3735/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 36: accuracy = 74.7%\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.142681\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.150802\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.148525\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.168683\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.149530\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.160004\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.166463\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.166892\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.178669\n",
            "\n",
            "Test set: Test loss: 1.0069, Accuracy: 3692/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.145416\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.132420\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.160408\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.164962\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.148247\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.161881\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.170079\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.184188\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.158468\n",
            "\n",
            "Test set: Test loss: 1.0358, Accuracy: 3682/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.112777\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.117894\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.124952\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.143258\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.144861\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.154254\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.165614\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.165431\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.153552\n",
            "\n",
            "Test set: Test loss: 1.0686, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.103164\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.123002\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.124433\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.153476\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.133093\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.129544\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.129453\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.147979\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.148499\n",
            "\n",
            "Test set: Test loss: 1.0827, Accuracy: 3708/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.114467\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.108929\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.123983\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.155909\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.147103\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.138757\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.149077\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.148273\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.147354\n",
            "\n",
            "Test set: Test loss: 1.0710, Accuracy: 3664/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.115453\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.115557\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.113351\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.129530\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.152906\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.128144\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.142262\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.139757\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.151431\n",
            "\n",
            "Test set: Test loss: 1.0536, Accuracy: 3702/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.099003\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.100544\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.106358\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.118908\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.119996\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.126402\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.134904\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.128408\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.128966\n",
            "\n",
            "Test set: Test loss: 1.1246, Accuracy: 3686/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.118123\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.099582\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.111434\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.129508\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.109602\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.123905\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.143259\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.139548\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.144460\n",
            "\n",
            "Test set: Test loss: 1.0748, Accuracy: 3688/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.115876\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.107457\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.105890\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.102297\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.118814\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.114686\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.118355\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.110701\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.123301\n",
            "\n",
            "Test set: Test loss: 1.1138, Accuracy: 3715/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.109891\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.106452\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.106140\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.102053\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.099952\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.112025\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.112903\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.115685\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.135367\n",
            "\n",
            "Test set: Test loss: 1.0690, Accuracy: 3723/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.109018\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.087269\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.100199\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.103467\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.124413\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.096321\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.113019\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.106750\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.131618\n",
            "\n",
            "Test set: Test loss: 1.1492, Accuracy: 3666/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.101962\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.095933\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.105912\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.090332\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.089677\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.106097\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.110126\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.128062\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.116345\n",
            "\n",
            "Test set: Test loss: 1.0854, Accuracy: 3730/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.082919\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.089496\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.086147\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.098902\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.114078\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.098766\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.112775\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.110388\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.108089\n",
            "\n",
            "Test set: Test loss: 1.1234, Accuracy: 3698/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.096001\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.079742\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.099598\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.091893\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.084605\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.104219\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.107987\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.104419\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.103910\n",
            "\n",
            "Test set: Test loss: 1.1149, Accuracy: 3711/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.078755\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.084627\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.106385\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.090881\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.088575\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.094477\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.116279\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.093271\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.104278\n",
            "\n",
            "Test set: Test loss: 1.0759, Accuracy: 3735/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.074684\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.094888\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.092208\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.091164\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.070862\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.074504\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.098898\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.091219\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.087529\n",
            "\n",
            "Test set: Test loss: 1.1090, Accuracy: 3716/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.088756\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.086093\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.095952\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.087299\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.083945\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.090766\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.084510\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.092908\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.095290\n",
            "\n",
            "Test set: Test loss: 1.1438, Accuracy: 3736/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 53: accuracy = 74.72%\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.075844\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.075596\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.085831\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.080164\n",
            "Train Epoch: 54 [25000/50000 (50%)]\tTrain Loss: 0.089333\n",
            "Train Epoch: 54 [30000/50000 (60%)]\tTrain Loss: 0.094915\n",
            "Train Epoch: 54 [35000/50000 (70%)]\tTrain Loss: 0.109652\n",
            "Train Epoch: 54 [40000/50000 (80%)]\tTrain Loss: 0.102946\n",
            "Train Epoch: 54 [45000/50000 (90%)]\tTrain Loss: 0.095057\n",
            "\n",
            "Test set: Test loss: 1.1512, Accuracy: 3702/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 55: lr = 0.1\n",
            "Train Epoch: 55 [5000/50000 (10%)]\tTrain Loss: 0.065655\n",
            "Train Epoch: 55 [10000/50000 (20%)]\tTrain Loss: 0.081329\n",
            "Train Epoch: 55 [15000/50000 (30%)]\tTrain Loss: 0.082641\n",
            "Train Epoch: 55 [20000/50000 (40%)]\tTrain Loss: 0.083306\n",
            "Train Epoch: 55 [25000/50000 (50%)]\tTrain Loss: 0.087909\n",
            "Train Epoch: 55 [30000/50000 (60%)]\tTrain Loss: 0.079822\n",
            "Train Epoch: 55 [35000/50000 (70%)]\tTrain Loss: 0.079263\n",
            "Train Epoch: 55 [40000/50000 (80%)]\tTrain Loss: 0.088100\n",
            "Train Epoch: 55 [45000/50000 (90%)]\tTrain Loss: 0.106636\n",
            "\n",
            "Test set: Test loss: 1.1530, Accuracy: 3702/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 56: lr = 0.1\n",
            "Train Epoch: 56 [5000/50000 (10%)]\tTrain Loss: 0.089711\n",
            "Train Epoch: 56 [10000/50000 (20%)]\tTrain Loss: 0.081498\n",
            "Train Epoch: 56 [15000/50000 (30%)]\tTrain Loss: 0.073791\n",
            "Train Epoch: 56 [20000/50000 (40%)]\tTrain Loss: 0.069459\n",
            "Train Epoch: 56 [25000/50000 (50%)]\tTrain Loss: 0.077945\n",
            "Train Epoch: 56 [30000/50000 (60%)]\tTrain Loss: 0.090745\n",
            "Train Epoch: 56 [35000/50000 (70%)]\tTrain Loss: 0.083522\n",
            "Train Epoch: 56 [40000/50000 (80%)]\tTrain Loss: 0.084231\n",
            "Train Epoch: 56 [45000/50000 (90%)]\tTrain Loss: 0.085826\n",
            "\n",
            "Test set: Test loss: 1.1557, Accuracy: 3727/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 57: lr = 0.1\n",
            "Train Epoch: 57 [5000/50000 (10%)]\tTrain Loss: 0.065283\n",
            "Train Epoch: 57 [10000/50000 (20%)]\tTrain Loss: 0.066232\n",
            "Train Epoch: 57 [15000/50000 (30%)]\tTrain Loss: 0.087988\n",
            "Train Epoch: 57 [20000/50000 (40%)]\tTrain Loss: 0.072052\n",
            "Train Epoch: 57 [25000/50000 (50%)]\tTrain Loss: 0.081152\n",
            "Train Epoch: 57 [30000/50000 (60%)]\tTrain Loss: 0.079991\n",
            "Train Epoch: 57 [35000/50000 (70%)]\tTrain Loss: 0.089491\n",
            "Train Epoch: 57 [40000/50000 (80%)]\tTrain Loss: 0.090130\n",
            "Train Epoch: 57 [45000/50000 (90%)]\tTrain Loss: 0.102340\n",
            "\n",
            "Test set: Test loss: 1.1813, Accuracy: 3680/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 58: lr = 0.1\n",
            "Train Epoch: 58 [5000/50000 (10%)]\tTrain Loss: 0.072256\n",
            "Train Epoch: 58 [10000/50000 (20%)]\tTrain Loss: 0.065752\n",
            "Train Epoch: 58 [15000/50000 (30%)]\tTrain Loss: 0.083485\n",
            "Train Epoch: 58 [20000/50000 (40%)]\tTrain Loss: 0.070467\n",
            "Train Epoch: 58 [25000/50000 (50%)]\tTrain Loss: 0.071576\n",
            "Train Epoch: 58 [30000/50000 (60%)]\tTrain Loss: 0.083571\n",
            "Train Epoch: 58 [35000/50000 (70%)]\tTrain Loss: 0.075545\n",
            "Train Epoch: 58 [40000/50000 (80%)]\tTrain Loss: 0.063384\n",
            "Train Epoch: 58 [45000/50000 (90%)]\tTrain Loss: 0.071640\n",
            "\n",
            "Test set: Test loss: 1.1490, Accuracy: 3710/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 59: lr = 0.1\n",
            "Train Epoch: 59 [5000/50000 (10%)]\tTrain Loss: 0.082273\n",
            "Train Epoch: 59 [10000/50000 (20%)]\tTrain Loss: 0.084108\n",
            "Train Epoch: 59 [15000/50000 (30%)]\tTrain Loss: 0.074842\n",
            "Train Epoch: 59 [20000/50000 (40%)]\tTrain Loss: 0.089347\n",
            "Train Epoch: 59 [25000/50000 (50%)]\tTrain Loss: 0.071525\n",
            "Train Epoch: 59 [30000/50000 (60%)]\tTrain Loss: 0.079914\n",
            "Train Epoch: 59 [35000/50000 (70%)]\tTrain Loss: 0.070746\n",
            "Train Epoch: 59 [40000/50000 (80%)]\tTrain Loss: 0.064835\n",
            "Train Epoch: 59 [45000/50000 (90%)]\tTrain Loss: 0.075974\n",
            "\n",
            "Test set: Test loss: 1.1364, Accuracy: 3742/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 59: accuracy = 74.84%\n",
            "\n",
            "Train Epoch 60: lr = 0.1\n",
            "Train Epoch: 60 [5000/50000 (10%)]\tTrain Loss: 0.061994\n",
            "Train Epoch: 60 [10000/50000 (20%)]\tTrain Loss: 0.071273\n",
            "Train Epoch: 60 [15000/50000 (30%)]\tTrain Loss: 0.058797\n",
            "Train Epoch: 60 [20000/50000 (40%)]\tTrain Loss: 0.061162\n",
            "Train Epoch: 60 [25000/50000 (50%)]\tTrain Loss: 0.076987\n",
            "Train Epoch: 60 [30000/50000 (60%)]\tTrain Loss: 0.076470\n",
            "Train Epoch: 60 [35000/50000 (70%)]\tTrain Loss: 0.082915\n",
            "Train Epoch: 60 [40000/50000 (80%)]\tTrain Loss: 0.082577\n",
            "Train Epoch: 60 [45000/50000 (90%)]\tTrain Loss: 0.076946\n",
            "\n",
            "Test set: Test loss: 1.1784, Accuracy: 3714/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 61: lr = 0.1\n",
            "Train Epoch: 61 [5000/50000 (10%)]\tTrain Loss: 0.049857\n",
            "Train Epoch: 61 [10000/50000 (20%)]\tTrain Loss: 0.052798\n",
            "Train Epoch: 61 [15000/50000 (30%)]\tTrain Loss: 0.069690\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mlower_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztz2rVGhJ4td",
        "outputId": "08005122-b8be-46f3-cdca-68982f7bb6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlageAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 37.710669455885224 seconds\n",
            "Best accuracy: 74.84  Best training loss: 0.017938118427991867  Best validation loss: 0.7864684224128718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdkyFgNpJ4th",
        "outputId": "3889c8c8-7a15-4e94-ae3b-811020d6324f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]\n",
            "[1.3012902736663818, 0.9138363003730774, 1.1499232053756714, 0.9705672264099121, 0.9113034009933472, 0.9625198841094971, 1.0122523307800293, 0.7827902436256409, 0.7167970538139343, 0.5407299995422363, 0.6973969340324402, 0.8412118554115295, 0.6487481594085693, 0.5783197283744812, 0.5823440551757812, 0.686017632484436, 0.4730595350265503, 0.39966002106666565, 0.5659844875335693, 0.42069530487060547, 0.3219113051891327, 0.4126985967159271, 0.27082499861717224, 0.23645071685314178, 0.20238718390464783, 0.2999936044216156, 0.367158979177475, 0.16423523426055908, 0.18740898370742798, 0.17556366324424744, 0.29907235503196716, 0.32482269406318665, 0.26091331243515015, 0.18017655611038208, 0.49880069494247437, 0.20392565429210663, 0.3048231601715088, 0.14135976135730743, 0.107121542096138, 0.17010736465454102, 0.04462776333093643, 0.027420319616794586, 0.11568228900432587, 0.11870969086885452, 0.2413611263036728, 0.10023804754018784, 0.10078440606594086, 0.20030954480171204, 0.07238242030143738, 0.157309889793396, 0.04193588346242905, 0.017938118427991867, 0.14639942348003387, 0.06026065722107887, 0.031255293637514114, 0.15024223923683167, 0.027542009949684143, 0.060750607401132584, 0.11827138066291809, 0.0282464399933815, 0.05818812549114227]\n",
            "[1.3355705612897872, 1.1671756452322006, 1.0379716897010804, 0.9809377205371861, 0.9075101101398466, 0.9009622794389723, 0.9327556407451629, 0.8868947577476499, 0.8381421715021133, 0.8119879665970803, 0.8209498891234401, 0.8224937200546267, 0.7864684224128718, 0.7878993535041808, 0.8282684496045108, 0.8268084129691124, 0.826619722247124, 0.8418616095185281, 0.8418848302960392, 0.8303363651037214, 0.8565733239054684, 0.8642947486042976, 0.9091101521253587, 0.8635572454333306, 0.8847794458270071, 0.9190759813785553, 0.9775989240407944, 0.9214038115739823, 0.9553181457519531, 0.9591401764750479, 1.0003685492277146, 0.9789652684330938, 0.9719157367944719, 1.013360830545426, 1.0282396489381789, 1.0491041487455366, 1.002194338738918, 1.0068723329901694, 1.0358420789241791, 1.068607770502567, 1.0827375951409335, 1.0709620600938794, 1.0535998731851581, 1.1245580112934113, 1.0748208910226822, 1.113815067112446, 1.0689627069234853, 1.149198778271675, 1.0854199573397634, 1.1234374880790714, 1.1149457472562794, 1.0758880341053005, 1.108952785730362, 1.1437532165646553, 1.151155003607273, 1.1530080997943875, 1.1556613236665723, 1.1812514555454257, 1.1489699295163156, 1.1364217397570615, 1.1784060195088388]\n",
            "[54.62, 58.96, 64.48, 65.82, 68.8, 68.86, 67.48, 68.84, 70.76, 72.08, 72.08, 72.42, 73.44, 73.4, 73.34, 73.7, 73.7, 73.42, 74.02, 73.9, 73.2, 73.92, 72.36, 73.58, 74.62, 73.56, 72.84, 74.32, 73.06, 73.86, 73.94, 74.0, 73.92, 73.72, 73.68, 73.82, 74.7, 73.84, 73.64, 73.06, 74.16, 73.28, 74.04, 73.72, 73.76, 74.3, 74.46, 73.32, 74.6, 73.96, 74.22, 74.7, 74.32, 74.72, 74.04, 74.04, 74.54, 73.6, 74.2, 74.84, 74.28]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEiz6VX09hl_"
      },
      "source": [
        "## squeeze skip residuals (batch normed) (removed 2 fire layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzk5ZPnS9hmD",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(384, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZE9ZdFDe9hmJ",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fb3-O2nJ9hmO",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3d8f5e74-1c3e-4535-c7ca-2977394e0519",
        "id": "Xk6H1MhM9hmT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.172409\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.857365\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.731581\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.689869\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.616273\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.586640\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.526734\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.475570\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.461383\n",
            "\n",
            "Test set: Test loss: 1.3331, Accuracy: 2730/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 54.6%\n",
            "Better loss at Epoch 0: loss = 1.3330789291858671%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.392462\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.363953\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.330135\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.296302\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.301391\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.259874\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.245365\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.228268\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.223746\n",
            "\n",
            "Test set: Test loss: 1.1809, Accuracy: 2954/5000 (59%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 59.08%\n",
            "Better loss at Epoch 1: loss = 1.1809074890613558%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.188202\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.135655\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.154818\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.147122\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.115360\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.128430\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.102729\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.094555\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.105656\n",
            "\n",
            "Test set: Test loss: 1.0411, Accuracy: 3168/5000 (63%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 63.36%\n",
            "Better loss at Epoch 2: loss = 1.0411449831724167%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.028549\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.053528\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.011872\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.035347\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.014946\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.019178\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.015609\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.019363\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 0.981411\n",
            "\n",
            "Test set: Test loss: 0.9966, Accuracy: 3245/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 64.9%\n",
            "Better loss at Epoch 3: loss = 0.9965701150894164%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.942209\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.935355\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.943236\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.909469\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.960795\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.951934\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.910964\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.924446\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.936926\n",
            "\n",
            "Test set: Test loss: 0.9338, Accuracy: 3352/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 67.04%\n",
            "Better loss at Epoch 4: loss = 0.9337832677364345%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.885851\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.860319\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.874714\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.873561\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.875780\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.865155\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.856831\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.870344\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.882076\n",
            "\n",
            "Test set: Test loss: 0.8938, Accuracy: 3448/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 68.96%\n",
            "Better loss at Epoch 5: loss = 0.89382829785347%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.822003\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.768589\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.816639\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.793289\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.811335\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.826883\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.830824\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.839566\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.822854\n",
            "\n",
            "Test set: Test loss: 0.8072, Accuracy: 3602/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 72.04%\n",
            "Better loss at Epoch 6: loss = 0.8071753495931627%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.735683\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.770482\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.750464\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.739677\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.754420\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.761047\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.781939\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.766107\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.760586\n",
            "\n",
            "Test set: Test loss: 0.7905, Accuracy: 3612/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 72.24%\n",
            "Better loss at Epoch 7: loss = 0.7904717135429384%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.733729\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.712463\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.727874\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.740255\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.714100\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.742809\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.696927\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.728893\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.718518\n",
            "\n",
            "Test set: Test loss: 0.8003, Accuracy: 3636/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 72.72%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.666127\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.661660\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.688546\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.675014\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.696666\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.665698\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.668736\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.712033\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.681781\n",
            "\n",
            "Test set: Test loss: 0.7953, Accuracy: 3644/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 72.88%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.623362\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.644129\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.610313\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.642095\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.672862\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.627948\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.646676\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.645638\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.674282\n",
            "\n",
            "Test set: Test loss: 0.7776, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "Better loss at Epoch 10: loss = 0.7775668901205064%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.585868\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.591425\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.601348\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.632579\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.615898\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.601639\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.651134\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.641142\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.636069\n",
            "\n",
            "Test set: Test loss: 0.7573, Accuracy: 3676/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 73.52%\n",
            "Better loss at Epoch 11: loss = 0.7572807887196539%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.578124\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.533345\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.559728\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.569373\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.587900\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.577309\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.592772\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.569643\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.594094\n",
            "\n",
            "Test set: Test loss: 0.7924, Accuracy: 3665/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.528321\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.516985\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.516772\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.539548\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.568687\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.573342\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.569287\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.573678\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.577016\n",
            "\n",
            "Test set: Test loss: 0.7691, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 73.92%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.474980\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.495118\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.494488\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.548992\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.522444\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.554642\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.544638\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.561026\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.528064\n",
            "\n",
            "Test set: Test loss: 0.7965, Accuracy: 3682/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.464190\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.482503\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.481737\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.490273\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.515729\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.520076\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.504392\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.526038\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.529424\n",
            "\n",
            "Test set: Test loss: 0.8161, Accuracy: 3652/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.454896\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.447345\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.474621\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.455663\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.480169\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.518135\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.477448\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.483144\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.511657\n",
            "\n",
            "Test set: Test loss: 0.7750, Accuracy: 3758/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 75.16%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.422975\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.416984\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.425021\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.432967\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.490331\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.464004\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.504484\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.494230\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.495138\n",
            "\n",
            "Test set: Test loss: 0.7708, Accuracy: 3717/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.410512\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.410849\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.410517\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.410444\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.450798\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.439607\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.424714\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.465187\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.444130\n",
            "\n",
            "Test set: Test loss: 0.7906, Accuracy: 3726/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.393584\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.395739\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.385379\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.379736\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.429538\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.390847\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.432206\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.428047\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.424888\n",
            "\n",
            "Test set: Test loss: 0.8173, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.364471\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.366098\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.372740\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.392658\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.414998\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.414054\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.384898\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.418324\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.420162\n",
            "\n",
            "Test set: Test loss: 0.8042, Accuracy: 3741/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.342366\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.351462\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.358878\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.384746\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.389039\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.362662\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.367424\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.401278\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.432786\n",
            "\n",
            "Test set: Test loss: 0.8221, Accuracy: 3700/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.332658\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.344632\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.336216\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.348772\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.347336\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.382279\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.386290\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.391553\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.374924\n",
            "\n",
            "Test set: Test loss: 0.8549, Accuracy: 3694/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.303805\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.313224\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.314577\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.318954\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.368570\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.348111\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.356156\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.367847\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.370001\n",
            "\n",
            "Test set: Test loss: 0.8080, Accuracy: 3749/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.294537\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.287581\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.308878\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.317366\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.307557\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.332026\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.343383\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.360064\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.351718\n",
            "\n",
            "Test set: Test loss: 0.8125, Accuracy: 3733/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.262601\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.276033\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.295712\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.328344\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.311850\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.313907\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.326697\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.310620\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.333408\n",
            "\n",
            "Test set: Test loss: 0.8600, Accuracy: 3712/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.283163\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.246181\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.284043\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.300743\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.293465\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.313786\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.294326\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.295899\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.320839\n",
            "\n",
            "Test set: Test loss: 0.8636, Accuracy: 3746/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.252431\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.267393\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.255856\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.281602\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.265172\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.303421\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.306476\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.299692\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.308574\n",
            "\n",
            "Test set: Test loss: 0.8609, Accuracy: 3747/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.265460\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.247599\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.242319\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.262823\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.266080\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.276965\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.277274\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.304247\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.304956\n",
            "\n",
            "Test set: Test loss: 0.8875, Accuracy: 3739/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.229720\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.239508\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.252562\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.244870\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.273285\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.249807\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.276215\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.282225\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.288088\n",
            "\n",
            "Test set: Test loss: 0.8481, Accuracy: 3771/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 29: accuracy = 75.42%\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.198220\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.210907\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.252650\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.239751\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.235686\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.265120\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.286893\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.270991\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.272185\n",
            "\n",
            "Test set: Test loss: 0.9392, Accuracy: 3724/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.210795\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.215961\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.227508\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.239319\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.241169\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.249956\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.249220\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.264554\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.271698\n",
            "\n",
            "Test set: Test loss: 0.9078, Accuracy: 3738/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.208243\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.208241\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.204800\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.239755\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.214724\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.236247\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.245372\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.258500\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.250188\n",
            "\n",
            "Test set: Test loss: 0.9784, Accuracy: 3670/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.223273\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.198317\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.200545\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.192265\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.234299\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.212079\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.239931\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.230142\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.229050\n",
            "\n",
            "Test set: Test loss: 0.9459, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.180665\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.181611\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.205937\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.210251\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.227052\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.213496\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.234126\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.219978\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.188687\n",
            "\n",
            "Test set: Test loss: 0.9306, Accuracy: 3766/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.168905\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.194391\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.197984\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.184810\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.211753\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.213296\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.212973\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.220246\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.242531\n",
            "\n",
            "Test set: Test loss: 0.9851, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.175692\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.172491\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.183704\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.200407\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.214167\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.210673\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.223900\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.208007\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.222449\n",
            "\n",
            "Test set: Test loss: 0.9335, Accuracy: 3767/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.171576\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.183819\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.165755\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.179152\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.168381\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.189582\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.222556\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.188315\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.209591\n",
            "\n",
            "Test set: Test loss: 0.9643, Accuracy: 3767/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.160910\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.154778\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.172098\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.166513\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.182453\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.188883\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.178332\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.186460\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.178967\n",
            "\n",
            "Test set: Test loss: 0.9744, Accuracy: 3761/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.156243\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.160860\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.160482\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.173871\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.166323\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.176018\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.184398\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.193527\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.182097\n",
            "\n",
            "Test set: Test loss: 1.0748, Accuracy: 3684/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.160380\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.160012\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.164414\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.183753\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.165897\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.161507\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.164238\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.173592\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.182121\n",
            "\n",
            "Test set: Test loss: 1.0035, Accuracy: 3730/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.135569\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.149963\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.159275\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.159261\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.177443\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.180134\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.158066\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.169725\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.168903\n",
            "\n",
            "Test set: Test loss: 1.0168, Accuracy: 3731/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.140336\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.134878\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.138233\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.153465\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.142023\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.177619\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.161338\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.173141\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.173653\n",
            "\n",
            "Test set: Test loss: 0.9680, Accuracy: 3768/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.136039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9001a69423c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs\\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accura...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Calculate softmax and cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-e34ac7bada91>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mresidual3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-e34ac7bada91>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         return torch.cat([\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand1x1_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbne1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand3x3_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbne3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         ], 1)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "584c9f67-dd38-40c7-946f-1173c6b03baf",
        "id": "s493TGuK9hmZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkYRR4hcRSRglfhGRhPn/uvPL02JqwLYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 29.94700846632563 seconds\n",
            "Best accuracy: 75.42  Best training loss: 0.053065892308950424  Best validation loss: 0.7572807887196539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6e392c7d-9b0f-4dad-f861-6a82ad56ff7d",
        "id": "-p80sMGT9hmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]\n",
            "[1.453267216682434, 1.3715189695358276, 0.9433684349060059, 1.1872323751449585, 0.87949538230896, 1.1622833013534546, 0.7353365421295166, 0.5349129438400269, 0.9624720215797424, 0.6736298203468323, 0.5184442400932312, 0.7947319746017456, 0.6541752815246582, 0.7366088628768921, 0.656165361404419, 0.43318861722946167, 0.6291453838348389, 0.22965003550052643, 0.43593940138816833, 0.746277928352356, 0.5608310103416443, 0.47886717319488525, 0.24985690414905548, 0.22937259078025818, 0.27845925092697144, 0.4262692928314209, 0.40763846039772034, 0.33204469084739685, 0.16318246722221375, 0.20171955227851868, 0.14139777421951294, 0.3243696093559265, 0.3531259298324585, 0.23891416192054749, 0.1317232847213745, 0.35347408056259155, 0.15939438343048096, 0.25912660360336304, 0.053065892308950424, 0.07181815803050995, 0.10535714775323868, 0.37263479828834534, 0.11476952582597733]\n",
            "[1.3330789291858671, 1.1809074890613558, 1.0411449831724167, 0.9965701150894164, 0.9337832677364345, 0.89382829785347, 0.8071753495931627, 0.7904717135429384, 0.8002735531330104, 0.7953236964344975, 0.7775668901205064, 0.7572807887196539, 0.7924278256297113, 0.7690848097205166, 0.796518820822239, 0.8161039716005324, 0.7749586042761801, 0.7707912451028825, 0.7905562949180602, 0.8173462560772897, 0.8041719266772273, 0.8221198606491089, 0.8549380207061766, 0.8079620522260667, 0.8125419801473618, 0.8600166055560114, 0.863572476506233, 0.8608951592445375, 0.887480640411377, 0.8480622401833534, 0.9392410510778426, 0.9077968230843543, 0.9783748421072961, 0.9458663102984426, 0.9306067878007888, 0.9851320025324818, 0.9335350272059443, 0.9643007251620297, 0.9744199278950689, 1.0747977006435394, 1.0034786126017572, 1.0168356496095654, 0.9680164691805841]\n",
            "[54.6, 59.08, 63.36, 64.9, 67.04, 68.96, 72.04, 72.24, 72.72, 72.88, 72.82, 73.52, 73.3, 73.92, 73.64, 73.04, 75.16, 74.34, 74.52, 74.02, 74.82, 74.0, 73.88, 74.98, 74.66, 74.24, 74.92, 74.94, 74.78, 75.42, 74.48, 74.76, 73.4, 74.06, 75.32, 74.06, 75.34, 75.34, 75.22, 73.68, 74.6, 74.62, 75.36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OSGfs8oSP60A"
      },
      "source": [
        "## squeeze skip residuals (batch normed) (removed 4 fire layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w5jbCdNkP60F",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(256, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9j7lacWZP60K",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VOIDL8QCP60O",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bf92d9d6-8521-4128-b530-868fae024483",
        "id": "9-Pu2rYpP60R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.144730\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.867799\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.770182\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.710893\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.653480\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.591216\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.539549\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.526393\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.507454\n",
            "\n",
            "Test set: Test loss: 1.3506, Accuracy: 2675/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 53.5%\n",
            "Better loss at Epoch 0: loss = 1.3505789524316782%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.445414\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.375812\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.362993\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.379682\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.348981\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.320452\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.327400\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.302468\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.306610\n",
            "\n",
            "Test set: Test loss: 1.2756, Accuracy: 2733/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 54.66%\n",
            "Better loss at Epoch 1: loss = 1.2756081318855288%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.223973\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.220124\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.231839\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.206868\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.229043\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.182624\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.188578\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.190526\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.151846\n",
            "\n",
            "Test set: Test loss: 1.1294, Accuracy: 2995/5000 (60%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 59.9%\n",
            "Better loss at Epoch 2: loss = 1.1293651217222216%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.141181\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.154761\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.143837\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.116411\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.125219\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.092930\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.082952\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.079570\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.093227\n",
            "\n",
            "Test set: Test loss: 1.1018, Accuracy: 3076/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 61.52%\n",
            "Better loss at Epoch 3: loss = 1.1017858368158338%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 1.056404\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 1.028798\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 1.056249\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 1.054665\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 1.040433\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 1.084193\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 1.026134\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 1.044932\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 1.042404\n",
            "\n",
            "Test set: Test loss: 1.0122, Accuracy: 3261/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 65.22%\n",
            "Better loss at Epoch 4: loss = 1.0122067934274677%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 1.015565\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.991403\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.991689\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.966496\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.977633\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 1.015537\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.996037\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 1.017610\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 1.014070\n",
            "\n",
            "Test set: Test loss: 0.9559, Accuracy: 3347/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 66.94%\n",
            "Better loss at Epoch 5: loss = 0.9558628690242766%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.936297\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.955454\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.980284\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.942304\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.958355\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.960559\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.945236\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.952587\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.928443\n",
            "\n",
            "Test set: Test loss: 0.9443, Accuracy: 3386/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 67.72%\n",
            "Better loss at Epoch 6: loss = 0.9442830532789231%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.909888\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.882479\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.915187\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.911563\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.937776\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.911513\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.884589\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.929308\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.916094\n",
            "\n",
            "Test set: Test loss: 0.9067, Accuracy: 3404/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 68.08%\n",
            "Better loss at Epoch 7: loss = 0.9067271530628199%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.850642\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.880785\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.870032\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.885126\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.872128\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.895853\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.854394\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.882244\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.869328\n",
            "\n",
            "Test set: Test loss: 0.9225, Accuracy: 3392/5000 (68%)\n",
            "\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.821023\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.844010\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.827381\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.846962\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.842742\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.869172\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.825822\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.829047\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.855474\n",
            "\n",
            "Test set: Test loss: 0.8765, Accuracy: 3481/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 69.62%\n",
            "Better loss at Epoch 9: loss = 0.8765052825212475%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.785804\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.791517\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.792818\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.817517\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.819894\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.804007\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.826442\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.814795\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.836441\n",
            "\n",
            "Test set: Test loss: 0.8443, Accuracy: 3516/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 70.32%\n",
            "Better loss at Epoch 10: loss = 0.8443277543783189%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.781356\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.773273\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.786862\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.751984\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.790733\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.785700\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.799293\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.814243\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.784516\n",
            "\n",
            "Test set: Test loss: 0.8307, Accuracy: 3543/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 70.86%\n",
            "Better loss at Epoch 11: loss = 0.830682610869408%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.764226\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.716745\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.756143\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.742729\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.788846\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.738907\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.754351\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.789283\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.771593\n",
            "\n",
            "Test set: Test loss: 0.8958, Accuracy: 3447/5000 (69%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.728981\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.690933\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.706877\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.727092\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.762285\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.752909\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.740549\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.779299\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.733845\n",
            "\n",
            "Test set: Test loss: 0.8414, Accuracy: 3534/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.692682\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.695042\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.693934\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.708717\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.704596\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.692272\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.738567\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.757761\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.725752\n",
            "\n",
            "Test set: Test loss: 0.8635, Accuracy: 3543/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.643344\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.656645\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.680305\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.700749\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.693642\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.716189\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.718380\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.687774\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.699390\n",
            "\n",
            "Test set: Test loss: 0.8069, Accuracy: 3586/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 15: accuracy = 71.72%\n",
            "Better loss at Epoch 15: loss = 0.8069479709863662%\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.631512\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.668873\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.622626\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.650069\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.656523\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.649459\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.692540\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.661231\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.694134\n",
            "\n",
            "Test set: Test loss: 0.8162, Accuracy: 3622/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 72.44%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.625661\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.647062\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.622042\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.640319\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.670686\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.668692\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.653951\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.662754\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.633914\n",
            "\n",
            "Test set: Test loss: 0.8249, Accuracy: 3611/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.597897\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.607018\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.625963\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.630125\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.645917\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.613693\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.654404\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.659602\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.662350\n",
            "\n",
            "Test set: Test loss: 0.8458, Accuracy: 3565/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.580233\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.571709\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.605192\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.603893\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.594902\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.614997\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.631795\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.607880\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.641324\n",
            "\n",
            "Test set: Test loss: 0.8497, Accuracy: 3574/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.565695\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.565576\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.607250\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.588512\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.583074\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.620694\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.620090\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.585025\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.632448\n",
            "\n",
            "Test set: Test loss: 0.8097, Accuracy: 3651/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 20: accuracy = 73.02%\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.532079\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.562906\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.574018\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.591729\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.544418\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.610279\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.605113\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.603166\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.587216\n",
            "\n",
            "Test set: Test loss: 0.8305, Accuracy: 3600/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.536803\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.563847\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.565176\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.577117\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.553636\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.575282\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.578844\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.616443\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.569095\n",
            "\n",
            "Test set: Test loss: 0.8408, Accuracy: 3585/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.519141\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.497117\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.530811\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.556954\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.577080\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.545728\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.568250\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.555053\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.571963\n",
            "\n",
            "Test set: Test loss: 0.8369, Accuracy: 3625/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.499486\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.504088\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.518772\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.541447\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.538374\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.540181\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.560851\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.552513\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.546076\n",
            "\n",
            "Test set: Test loss: 0.8426, Accuracy: 3610/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.487342\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.497465\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.489441\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.519726\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.513604\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.553742\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.545636\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.527799\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.541207\n",
            "\n",
            "Test set: Test loss: 0.8498, Accuracy: 3605/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.469072\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.468576\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.514411\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.495271\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.511672\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.520924\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.521630\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.533116\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.507505\n",
            "\n",
            "Test set: Test loss: 0.8692, Accuracy: 3592/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.436409\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.462219\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.491461\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.490358\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.505288\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.502870\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.485045\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.545690\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.514777\n",
            "\n",
            "Test set: Test loss: 0.8929, Accuracy: 3561/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.440759\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.439122\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.477918\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.467493\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.491443\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.506938\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.495812\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.491655\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.527131\n",
            "\n",
            "Test set: Test loss: 0.8655, Accuracy: 3609/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.440426\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.439212\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.479128\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.464914\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.476262\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.466375\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.466649\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.475104\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.516235\n",
            "\n",
            "Test set: Test loss: 0.8922, Accuracy: 3601/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.426293\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.404229\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.467470\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.464125\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.456060\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.468397\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.457513\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.482135\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.481090\n",
            "\n",
            "Test set: Test loss: 0.8816, Accuracy: 3620/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.426175\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.434759\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.444668\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.432040\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.474738\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.452620\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.458604\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.463827\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.479871\n",
            "\n",
            "Test set: Test loss: 0.8710, Accuracy: 3614/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.446453\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.411386\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.403870\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.453981\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.456383\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.438159\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.436372\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.466282\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.442680\n",
            "\n",
            "Test set: Test loss: 0.8721, Accuracy: 3639/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.390952\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.414749\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.444892\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.427264\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.405091\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.419954\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.448866\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.443548\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.454088\n",
            "\n",
            "Test set: Test loss: 0.8638, Accuracy: 3680/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 33: accuracy = 73.6%\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.368708\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.403088\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.412059\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.416900\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.412169\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.422724\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.429657\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.432350\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.452210\n",
            "\n",
            "Test set: Test loss: 0.8756, Accuracy: 3636/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.385255\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.380262\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.396578\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.402486\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.420355\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.457629\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.396662\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.438525\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.403204\n",
            "\n",
            "Test set: Test loss: 0.9255, Accuracy: 3576/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.364078\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.404617\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.364583\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.397104\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.405288\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.411317\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.413228\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.433454\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.411724\n",
            "\n",
            "Test set: Test loss: 0.9104, Accuracy: 3609/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.351447\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.372169\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.364489\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.382053\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.405978\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.420277\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.388179\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.396529\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.378804\n",
            "\n",
            "Test set: Test loss: 0.9043, Accuracy: 3622/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.358179\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.335877\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.334810\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.376212\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.395548\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.396930\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.403594\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.403805\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.411073\n",
            "\n",
            "Test set: Test loss: 0.9373, Accuracy: 3603/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.366817\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.372833\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.352707\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.376804\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.366356\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.404927\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.386255\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.363268\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.407970\n",
            "\n",
            "Test set: Test loss: 0.9058, Accuracy: 3619/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.324549\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.335182\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.370323\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.352836\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.370735\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.374077\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.395911\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.359869\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.405403\n",
            "\n",
            "Test set: Test loss: 0.9485, Accuracy: 3582/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.331964\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.310642\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.350303\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.356684\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.368502\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.378149\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.341179\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.400385\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.360497\n",
            "\n",
            "Test set: Test loss: 0.9913, Accuracy: 3566/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.318880\n",
            "Train Epoch: 42 [10000/50000 (20%)]\tTrain Loss: 0.338568\n",
            "Train Epoch: 42 [15000/50000 (30%)]\tTrain Loss: 0.349247\n",
            "Train Epoch: 42 [20000/50000 (40%)]\tTrain Loss: 0.353203\n",
            "Train Epoch: 42 [25000/50000 (50%)]\tTrain Loss: 0.353869\n",
            "Train Epoch: 42 [30000/50000 (60%)]\tTrain Loss: 0.337567\n",
            "Train Epoch: 42 [35000/50000 (70%)]\tTrain Loss: 0.352715\n",
            "Train Epoch: 42 [40000/50000 (80%)]\tTrain Loss: 0.366451\n",
            "Train Epoch: 42 [45000/50000 (90%)]\tTrain Loss: 0.369449\n",
            "\n",
            "Test set: Test loss: 0.9556, Accuracy: 3617/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 43: lr = 0.1\n",
            "Train Epoch: 43 [5000/50000 (10%)]\tTrain Loss: 0.310795\n",
            "Train Epoch: 43 [10000/50000 (20%)]\tTrain Loss: 0.314408\n",
            "Train Epoch: 43 [15000/50000 (30%)]\tTrain Loss: 0.322756\n",
            "Train Epoch: 43 [20000/50000 (40%)]\tTrain Loss: 0.323763\n",
            "Train Epoch: 43 [25000/50000 (50%)]\tTrain Loss: 0.353319\n",
            "Train Epoch: 43 [30000/50000 (60%)]\tTrain Loss: 0.368916\n",
            "Train Epoch: 43 [35000/50000 (70%)]\tTrain Loss: 0.366223\n",
            "Train Epoch: 43 [40000/50000 (80%)]\tTrain Loss: 0.377551\n",
            "Train Epoch: 43 [45000/50000 (90%)]\tTrain Loss: 0.369102\n",
            "\n",
            "Test set: Test loss: 0.9813, Accuracy: 3600/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 44: lr = 0.1\n",
            "Train Epoch: 44 [5000/50000 (10%)]\tTrain Loss: 0.307996\n",
            "Train Epoch: 44 [10000/50000 (20%)]\tTrain Loss: 0.326863\n",
            "Train Epoch: 44 [15000/50000 (30%)]\tTrain Loss: 0.319393\n",
            "Train Epoch: 44 [20000/50000 (40%)]\tTrain Loss: 0.356740\n",
            "Train Epoch: 44 [25000/50000 (50%)]\tTrain Loss: 0.323356\n",
            "Train Epoch: 44 [30000/50000 (60%)]\tTrain Loss: 0.346054\n",
            "Train Epoch: 44 [35000/50000 (70%)]\tTrain Loss: 0.343087\n",
            "Train Epoch: 44 [40000/50000 (80%)]\tTrain Loss: 0.343103\n",
            "Train Epoch: 44 [45000/50000 (90%)]\tTrain Loss: 0.364085\n",
            "\n",
            "Test set: Test loss: 0.9535, Accuracy: 3588/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 45: lr = 0.1\n",
            "Train Epoch: 45 [5000/50000 (10%)]\tTrain Loss: 0.304128\n",
            "Train Epoch: 45 [10000/50000 (20%)]\tTrain Loss: 0.291936\n",
            "Train Epoch: 45 [15000/50000 (30%)]\tTrain Loss: 0.310054\n",
            "Train Epoch: 45 [20000/50000 (40%)]\tTrain Loss: 0.325027\n",
            "Train Epoch: 45 [25000/50000 (50%)]\tTrain Loss: 0.340686\n",
            "Train Epoch: 45 [30000/50000 (60%)]\tTrain Loss: 0.333069\n",
            "Train Epoch: 45 [35000/50000 (70%)]\tTrain Loss: 0.341379\n",
            "Train Epoch: 45 [40000/50000 (80%)]\tTrain Loss: 0.338146\n",
            "Train Epoch: 45 [45000/50000 (90%)]\tTrain Loss: 0.343576\n",
            "\n",
            "Test set: Test loss: 1.0320, Accuracy: 3540/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 46: lr = 0.1\n",
            "Train Epoch: 46 [5000/50000 (10%)]\tTrain Loss: 0.303212\n",
            "Train Epoch: 46 [10000/50000 (20%)]\tTrain Loss: 0.305411\n",
            "Train Epoch: 46 [15000/50000 (30%)]\tTrain Loss: 0.310824\n",
            "Train Epoch: 46 [20000/50000 (40%)]\tTrain Loss: 0.306968\n",
            "Train Epoch: 46 [25000/50000 (50%)]\tTrain Loss: 0.334474\n",
            "Train Epoch: 46 [30000/50000 (60%)]\tTrain Loss: 0.341249\n",
            "Train Epoch: 46 [35000/50000 (70%)]\tTrain Loss: 0.331812\n",
            "Train Epoch: 46 [40000/50000 (80%)]\tTrain Loss: 0.333799\n",
            "Train Epoch: 46 [45000/50000 (90%)]\tTrain Loss: 0.360339\n",
            "\n",
            "Test set: Test loss: 1.0068, Accuracy: 3595/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 47: lr = 0.1\n",
            "Train Epoch: 47 [5000/50000 (10%)]\tTrain Loss: 0.278167\n",
            "Train Epoch: 47 [10000/50000 (20%)]\tTrain Loss: 0.283203\n",
            "Train Epoch: 47 [15000/50000 (30%)]\tTrain Loss: 0.292467\n",
            "Train Epoch: 47 [20000/50000 (40%)]\tTrain Loss: 0.312131\n",
            "Train Epoch: 47 [25000/50000 (50%)]\tTrain Loss: 0.336918\n",
            "Train Epoch: 47 [30000/50000 (60%)]\tTrain Loss: 0.331389\n",
            "Train Epoch: 47 [35000/50000 (70%)]\tTrain Loss: 0.349787\n",
            "Train Epoch: 47 [40000/50000 (80%)]\tTrain Loss: 0.326561\n",
            "Train Epoch: 47 [45000/50000 (90%)]\tTrain Loss: 0.338979\n",
            "\n",
            "Test set: Test loss: 1.0203, Accuracy: 3578/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 48: lr = 0.1\n",
            "Train Epoch: 48 [5000/50000 (10%)]\tTrain Loss: 0.280484\n",
            "Train Epoch: 48 [10000/50000 (20%)]\tTrain Loss: 0.310950\n",
            "Train Epoch: 48 [15000/50000 (30%)]\tTrain Loss: 0.289434\n",
            "Train Epoch: 48 [20000/50000 (40%)]\tTrain Loss: 0.290266\n",
            "Train Epoch: 48 [25000/50000 (50%)]\tTrain Loss: 0.337439\n",
            "Train Epoch: 48 [30000/50000 (60%)]\tTrain Loss: 0.316561\n",
            "Train Epoch: 48 [35000/50000 (70%)]\tTrain Loss: 0.313846\n",
            "Train Epoch: 48 [40000/50000 (80%)]\tTrain Loss: 0.322330\n",
            "Train Epoch: 48 [45000/50000 (90%)]\tTrain Loss: 0.337323\n",
            "\n",
            "Test set: Test loss: 0.9996, Accuracy: 3618/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 49: lr = 0.1\n",
            "Train Epoch: 49 [5000/50000 (10%)]\tTrain Loss: 0.278932\n",
            "Train Epoch: 49 [10000/50000 (20%)]\tTrain Loss: 0.288169\n",
            "Train Epoch: 49 [15000/50000 (30%)]\tTrain Loss: 0.299346\n",
            "Train Epoch: 49 [20000/50000 (40%)]\tTrain Loss: 0.300670\n",
            "Train Epoch: 49 [25000/50000 (50%)]\tTrain Loss: 0.308056\n",
            "Train Epoch: 49 [30000/50000 (60%)]\tTrain Loss: 0.325363\n",
            "Train Epoch: 49 [35000/50000 (70%)]\tTrain Loss: 0.311341\n",
            "Train Epoch: 49 [40000/50000 (80%)]\tTrain Loss: 0.324624\n",
            "Train Epoch: 49 [45000/50000 (90%)]\tTrain Loss: 0.308693\n",
            "\n",
            "Test set: Test loss: 1.0008, Accuracy: 3617/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 50: lr = 0.1\n",
            "Train Epoch: 50 [5000/50000 (10%)]\tTrain Loss: 0.269694\n",
            "Train Epoch: 50 [10000/50000 (20%)]\tTrain Loss: 0.276843\n",
            "Train Epoch: 50 [15000/50000 (30%)]\tTrain Loss: 0.272058\n",
            "Train Epoch: 50 [20000/50000 (40%)]\tTrain Loss: 0.266506\n",
            "Train Epoch: 50 [25000/50000 (50%)]\tTrain Loss: 0.291599\n",
            "Train Epoch: 50 [30000/50000 (60%)]\tTrain Loss: 0.326866\n",
            "Train Epoch: 50 [35000/50000 (70%)]\tTrain Loss: 0.305560\n",
            "Train Epoch: 50 [40000/50000 (80%)]\tTrain Loss: 0.315886\n",
            "Train Epoch: 50 [45000/50000 (90%)]\tTrain Loss: 0.304124\n",
            "\n",
            "Test set: Test loss: 1.0078, Accuracy: 3617/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 51: lr = 0.1\n",
            "Train Epoch: 51 [5000/50000 (10%)]\tTrain Loss: 0.256447\n",
            "Train Epoch: 51 [10000/50000 (20%)]\tTrain Loss: 0.276298\n",
            "Train Epoch: 51 [15000/50000 (30%)]\tTrain Loss: 0.297571\n",
            "Train Epoch: 51 [20000/50000 (40%)]\tTrain Loss: 0.305667\n",
            "Train Epoch: 51 [25000/50000 (50%)]\tTrain Loss: 0.299730\n",
            "Train Epoch: 51 [30000/50000 (60%)]\tTrain Loss: 0.296900\n",
            "Train Epoch: 51 [35000/50000 (70%)]\tTrain Loss: 0.281850\n",
            "Train Epoch: 51 [40000/50000 (80%)]\tTrain Loss: 0.331175\n",
            "Train Epoch: 51 [45000/50000 (90%)]\tTrain Loss: 0.343416\n",
            "\n",
            "Test set: Test loss: 1.0665, Accuracy: 3533/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 52: lr = 0.1\n",
            "Train Epoch: 52 [5000/50000 (10%)]\tTrain Loss: 0.269682\n",
            "Train Epoch: 52 [10000/50000 (20%)]\tTrain Loss: 0.255053\n",
            "Train Epoch: 52 [15000/50000 (30%)]\tTrain Loss: 0.261192\n",
            "Train Epoch: 52 [20000/50000 (40%)]\tTrain Loss: 0.273548\n",
            "Train Epoch: 52 [25000/50000 (50%)]\tTrain Loss: 0.279475\n",
            "Train Epoch: 52 [30000/50000 (60%)]\tTrain Loss: 0.273669\n",
            "Train Epoch: 52 [35000/50000 (70%)]\tTrain Loss: 0.311013\n",
            "Train Epoch: 52 [40000/50000 (80%)]\tTrain Loss: 0.292467\n",
            "Train Epoch: 52 [45000/50000 (90%)]\tTrain Loss: 0.321748\n",
            "\n",
            "Test set: Test loss: 1.0159, Accuracy: 3583/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 53: lr = 0.1\n",
            "Train Epoch: 53 [5000/50000 (10%)]\tTrain Loss: 0.254302\n",
            "Train Epoch: 53 [10000/50000 (20%)]\tTrain Loss: 0.278506\n",
            "Train Epoch: 53 [15000/50000 (30%)]\tTrain Loss: 0.291641\n",
            "Train Epoch: 53 [20000/50000 (40%)]\tTrain Loss: 0.282792\n",
            "Train Epoch: 53 [25000/50000 (50%)]\tTrain Loss: 0.283276\n",
            "Train Epoch: 53 [30000/50000 (60%)]\tTrain Loss: 0.288947\n",
            "Train Epoch: 53 [35000/50000 (70%)]\tTrain Loss: 0.299345\n",
            "Train Epoch: 53 [40000/50000 (80%)]\tTrain Loss: 0.301295\n",
            "Train Epoch: 53 [45000/50000 (90%)]\tTrain Loss: 0.274603\n",
            "\n",
            "Test set: Test loss: 1.0496, Accuracy: 3567/5000 (71%)\n",
            "\n",
            "\n",
            "Train Epoch 54: lr = 0.1\n",
            "Train Epoch: 54 [5000/50000 (10%)]\tTrain Loss: 0.273169\n",
            "Train Epoch: 54 [10000/50000 (20%)]\tTrain Loss: 0.258298\n",
            "Train Epoch: 54 [15000/50000 (30%)]\tTrain Loss: 0.269219\n",
            "Train Epoch: 54 [20000/50000 (40%)]\tTrain Loss: 0.277162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9001a69423c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs\\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accura...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Calculating gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7e39adcc-c9d9-4112-ef20-997e293787e5",
        "id": "ORlSuIcuP60U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd3xUVd7/32cmvXdIg4SeQKihKCBV\nxQKKog8q9rK67rq7rvvTZx97W3VdFwtrWxUrrF12BXFVBClSpQcIJYFASO99Zs7vjzMTJskkmZRJ\nJuS8X695kbn33HvPDMn93POtQkqJRqPRaHovhu6egEaj0Wi6Fy0EGo1G08vRQqDRaDS9HC0EGo1G\n08vRQqDRaDS9HC0EGo1G08vRQqDRdBNCiA1CiDEdPEc/IUS5EMLYmWOdONdSIcST1p9HCiE2dvSc\nmu5DC4Gm0xBCTBFCbBRClAghCq03uvHdPa+uRgjxoxDitlbGzAXKpJS/2G1LFkKssH5/ZUKINUKI\nc1s6j5TyuJQyQEppbm1ebRnbFqSUu4Fi62fS9EC0EGg6BSFEEPAf4GUgDIgFHgNqunNebsydwPu2\nN0KIgcAGYA+QCMQAXwDfCiHOcXQCIYRHF8zTWT4EftXdk9C0EymlfulXh19AKlDcwn4j8DyQDxwF\n7gYk4GHdnwHMthv/KPCB3ftJwEagGNgFTLfbFwy8BWQDJ4EnAaN13y6g3O4lbce2cs4fgSdQN+cy\n4FsgorX5AE8BZqDaer1XHHwXXkAVEGe37X1gpYOxrwLrrD8nWOd/K3AcWGe3zfY9Jlq3lwHfAUts\n36ODsa19xk+A00CJ9ZzD7fYtBZ60ex9r/Uze3f27qF9tf+kVgaazOASYhRDvCiEuEkKENtp/O3Ap\nMAYlGgucPbEQIhb4GnWDDwPuAz4TQkRahywFTMAg6/kvAG4DkFKOksocEgDcCxwEdjhxToBrgZuB\nKNTN+77W5iOl/D/gJ+A31uv+xsFHGgxYpJRZdtvOR914G/MxMFkI4Wu3bRqQBFzoYPxHwBYgHCWm\n1zsYY4/Dz2hllXWuUcAO1FO/Q6SUJ4E6YGgr19O4IVoINJ2ClLIUmIJ64nwTyLPau/tYh1wNLJZS\nnpBSFgJ/acPpF6GelldKKS1Syv8C24CLree/GPi9lLJCSpkL/B1YaH8CIcQU1I17nnWuzZ7T7rB3\npJSHpJRVqBvy6Nbm4+TnCUE9gdsTgVrRNCYb9XcaZrftUetnrWr0GfsB44GHpZS1Usr1wIpW5tLc\nZ0RK+baUskxKWYMSlVFCiOAWzlVm/WyaHoYWAk2nIaVMk1LeJKWMA0ag7NyLrbtjgBN2wzPbcOr+\nwFVCiGLbCyU60dZ9nkC23b7XUU+xAAgh4lE3uRullIecOKeN03Y/VwIBbTi2JYqAwEbb8ps5Phqw\nWI+xccLBOFDfcaGUstKJsTYcfkYhhFEI8YwQ4ogQohRlugMlWM0RiDKVaXoY7uRs0pxFSCkPCCGW\ncsaBmA3E2w3p1+iQCsDP7n1fu59PAO9LKW9vfB0hRDTKIR0hpTQ52O8LfIlajaxy5pxO0NqxrZX0\nPaymJmKtJhVQ9vyrgHcajb0a2CSlrBRCtHb+bCBMCOFnJwbxzYxtjWuBy4DZKBEIRomRcDTYai7z\nQpneND0MvSLQdApCiGFCiD8KIeKs7+OBa4CfrUM+Bu4RQsRZ/QcPNDrFTmChEMJTCNHYh/ABMFcI\ncaH1SdVHCDFdCBEnpcxGOTn/JoQIEkIYhBADhRDTrMe+DRyQUj7X6HrNntOJj9vasTnAgOYOllLW\nom780+w2PwacK4R4SggRJoQIFEL8FrgBuN+JOSGlzESZqB4VQnhZo43aG9IZiBLYApRAP93K+GnA\nD1YzkqaHoYVA01mUAROBzUKICpQA7AX+aN3/JrAaFWGzA/i80fEPAQNRT52PoZyeAEgpT6CeTv8M\n5KGeyP/Emd/fG1BPo/utx3/KGTPLQmC+NZHK9prqxDmbxYljXwQWCCGKhBAvNXOa17Fz5Eop01Hm\npVGoJ/Bs4ErgQinlhtbmZMd1wDmoG/iTwL9oXwjveyjz3UnU9/pzy8O5DnitHdfRuAFCSt2YRtP1\nCCESgGOApyOTTm9ACLEBFV30S6uD23+Nf6FWRI+48BojgdellA7zHTTujxYCTbeghcA1WDO5C1Hf\n7QUo/8g5rhQbTc9HO4s1mrOLviizWziQBdylRUDTGnpFoNFoNL0c7SzWaDSaXk6PMw1FRETIhISE\n7p6GRqPR9Ci2b9+eL6WMdLSvxwlBQkIC27Zt6+5paDQaTY9CCNFsNr82DWk0Gk0vRwuBRqPR9HJc\nJgRCiLeFELlCiL0tjJkuhNgphNgnhFjrqrloNBqNpnlc6SNYCryCSlVvghAiBPgHMEdKeVwIEeVo\nnEaj6V7q6urIysqiurq6u6eicQIfHx/i4uLw9PR0+hiXCYGUcp01e7Q5rgU+l1Iet47PddVcNBpN\n+8nKyiIwMJCEhATsKqBq3BApJQUFBWRlZZGYmOj0cd3pIxgChFobfW8XQtzQ3EAhxB1CiG1CiG15\neXldOEWNRlNdXU14eLgWgR6AEILw8PA2r966Uwg8gHHAJaiWew8JIYY4GiilfENKmSqlTI2MdBgG\nq9FoXIgWgZ5De/6vulMIsoDV1pZ7+ajm2KNcdbEDp0t59psDlFTVueoSGo1G0yPpTiH4CpgihPAQ\nQvihatmnuepixwsqefXHI2QWVLjqEhqNxgUUFBQwevRoRo8eTd++fYmNja1/X1tb69Q5br75Zg4e\nbLl52pIlS/jwww87Y8pMmTKFnTt3dsq5ugKXOYuFEMuA6UCEECILeATVWxYp5WtSyjQhxDfAblRP\n1n9KKZsNNe0osaG+AJwsqmJknO6vrdH0FMLDw+tvqo8++igBAQHcd999DcZIKZFSYjA4frZ9553G\nHUCbcvfdd3d8sj0Ul60IpJTXSCmjpZSeUso4KeVbVgF4zW7MX6WUyVLKEVLKxS2dr6PEhah2uFlF\nVa68jEaj6SIOHz5McnIy1113HcOHDyc7O5s77riD1NRUhg8fzuOPP14/1vaEbjKZCAkJ4YEHHmDU\nqFGcc8455OaqgMUHH3yQxYsX149/4IEHmDBhAkOHDmXjxo0AVFRUcOWVV5KcnMyCBQtITU1t9cn/\ngw8+ICUlhREjRvDnP/8ZAJPJxPXXX1+//aWXVCO7v//97yQnJzNy5EgWLVrU6d9Zc/S4WkPtJcjX\ng0BvD04WayHQaNrLY//ex/5TpZ16zuSYIB6ZO7xdxx44cID33nuP1NRUAJ555hnCwsIwmUzMmDGD\nBQsWkJyc3OCYkpISpk2bxjPPPMO9997L22+/zQMPNG6hrVYZW7ZsYcWKFTz++ON88803vPzyy/Tt\n25fPPvuMXbt2MXbs2Bbnl5WVxYMPPsi2bdsIDg5m9uzZ/Oc//yEyMpL8/Hz27NkDQHFxMQDPPfcc\nmZmZeHl51W/rCnpNiQkhBLGhvnpFoNGcRQwcOLBeBACWLVvG2LFjGTt2LGlpaezfv7/JMb6+vlx0\n0UUAjBs3joyMDIfnvuKKK5qMWb9+PQsXLgRg1KhRDB/esoBt3ryZmTNnEhERgaenJ9deey3r1q1j\n0KBBHDx4kHvuuYfVq1cTHBwMwPDhw1m0aBEffvhhmxLCOkqvWREAxIb4klVU2d3T0Gh6LO19cncV\n/v7+9T+np6fz4osvsmXLFkJCQli0aJHDeHovL6/6n41GIyaT406p3t7erY5pL+Hh4ezevZtVq1ax\nZMkSPvvsM9544w1Wr17N2rVrWbFiBU8//TS7d+/GaDR26rUd0WtWBABxob7aNKTRnKWUlpYSGBhI\nUFAQ2dnZrF69utOvMXnyZD7++GMA9uzZ43DFYc/EiRNZs2YNBQUFmEwmli9fzrRp08jLy0NKyVVX\nXcXjjz/Ojh07MJvNZGVlMXPmTJ577jny8/OprOyaB9fetSII9aWs2kRJVR3Bvl237NJoNK5n7Nix\nJCcnM2zYMPr378/kyZM7/Rq//e1vueGGG0hOTq5/2cw6joiLi+OJJ55g+vTpSCmZO3cul1xyCTt2\n7ODWW29FSokQgmeffRaTycS1115LWVkZFouF++67j8DAwE7/DI7ocT2LU1NTZXsb03y9O5u7P9rB\nynumkhwT1Mkz02jOTtLS0khKSuruabgFJpMJk8mEj48P6enpXHDBBaSnp+Ph4V7P1I7+z4QQ26WU\nqY7Gu9fsXUycLZeguEoLgUajaTPl5eXMmjULk8mElJLXX3/d7USgPfT8T9AGziSVaYexRqNpOyEh\nIWzfvr27p9Hp9Cpncbi/Fz6eBh1CqtFoNHb0KiEQQhAboiOHNBqNxp5eJQQAsaF+Wgg0Go3Gjt4n\nBCE6u1ij0Wjs6XVCEBfqS2FFLZW1nZspqNFoXMOMGTOaJIctXryYu+66q8XjAgICADh16hQLFixw\nOGb69Om0Fo6+ePHiBoldF198cafUAXr00Ud5/vnnO3yezqBXCgHAKW0e0mh6BNdccw3Lly9vsG35\n8uVcc801Th0fExPDp59+2u7rNxaClStXEhJydpWy73VCEBuihOCENg9pND2CBQsW8PXXX9c3ocnI\nyODUqVNMnTq1Pq5/7NixpKSk8NVXXzU5PiMjgxEjRgBQVVXFwoULSUpKYv78+VRVnbkP3HXXXfUl\nrB955BEAXnrpJU6dOsWMGTOYMWMGAAkJCeTn5wPwwgsvMGLECEaMGFFfwjojI4OkpCRuv/12hg8f\nzgUXXNDgOo7YuXMnkyZNYuTIkcyfP5+ioqL669vKUtuK3a1du7a+Mc+YMWMoKytr93dro1flEQDE\nhaq+BCe1EGg0bWfVA3B6T+ees28KXPRMs7vDwsKYMGECq1at4rLLLmP58uVcffXVCCHw8fHhiy++\nICgoiPz8fCZNmsS8efOa7dv76quv4ufnR1paGrt3725QRvqpp54iLCwMs9nMrFmz2L17N/fccw8v\nvPACa9asISIiosG5tm/fzjvvvMPmzZuRUjJx4kSmTZtGaGgo6enpLFu2jDfffJOrr76azz77rMX+\nAjfccAMvv/wy06ZN4+GHH+axxx5j8eLFPPPMMxw7dgxvb+96c9Tzzz/PkiVLmDx5MuXl5fj4+LTl\n23ZIr1sRRAV642kUOnJIo+lB2JuH7M1CUkr+/Oc/M3LkSGbPns3JkyfJyclp9jzr1q2rvyGPHDmS\nkSNH1u/7+OOPGTt2LGPGjGHfvn2tFpRbv3498+fPx9/fn4CAAK644gp++uknABITExk9ejTQcqlr\nUP0RiouLmTZtGgA33ngj69atq5/jddddxwcffFCfwTx58mTuvfdeXnrpJYqLizsls9mVrSrfBi4F\ncqWUI1oYNx7YBCyUUrbfkOckBoMgOlhHDmk07aKFJ3dXctlll/GHP/yBHTt2UFlZybhx4wD48MMP\nycvLY/v27Xh6epKQkOCw9HRrHDt2jOeff56tW7cSGhrKTTfd1K7z2LCVsAZVxro101BzfP3116xb\nt45///vfPPXUU+zZs4cHHniASy65hJUrVzJ58mRWr17NsGHD2j1XcO2KYCkwp6UBQggj8CzwrQvn\n0YS4UF9dZkKj6UEEBAQwY8YMbrnllgZO4pKSEqKiovD09GTNmjVkZma2eJ7zzjuPjz76CIC9e/ey\ne/duQJWw9vf3Jzg4mJycHFatWlV/TGBgoEM7/NSpU/nyyy+prKykoqKCL774gqlTp7b5swUHBxMa\nGlq/mnj//feZNm0aFouFEydOMGPGDJ599llKSkooLy/nyJEjpKSkcP/99zN+/HgOHDjQ5ms2xmUr\nAinlOiFEQivDfgt8Box31TwcERviy7r0vK68pEaj6SDXXHMN8+fPbxBBdN111zF37lxSUlJITU1t\n9cn4rrvu4uabbyYpKYmkpKT6lcWoUaMYM2YMw4YNIz4+vkEJ6zvuuIM5c+YQExPDmjVr6rePHTuW\nm266iQkTJgBw2223MWbMmBbNQM3x7rvvcuedd1JZWcmAAQN45513MJvNLFq0iJKSEqSU3HPPPYSE\nhPDQQw+xZs0aDAYDw4cPr++21hFcWobaKgT/cWQaEkLEAh8BM4C3reMcmoaEEHcAdwD069dvXGuq\n3xqLvzvE4u/SOfjkHLw9XN/9R6Ppyegy1D2Ptpah7k5n8WLgfimlpbWBUso3pJSpUsrUyMjIDl/Y\nFjmUXdx+G6BGo9GcLXRn+GgqsNwa5hUBXCyEMEkpv3T1hW25BCeLq0iI8G9ltEaj0ZzddJsQSCkT\nbT8LIZaiTEMuFwE4k12sG9lrNM5ha6mocX/aY+53ZfjoMmA6ECGEyAIeATwBpJSvueq6ztA32AeD\n0EllGo0z+Pj4UFBQQHh4uBYDN0dKSUFBQZuTzFwZNeRcIRA19iZXzcMRnkYDfYN8yNJJZRpNq8TF\nxZGVlUVeno606wn4+PgQFxfXpmN6T4mJkzvgx2dgwdvgHUBsqE4q02icwdPTk8TExNYHanosvafE\nhLkO0lfDhhcBFTmkTUMajUbTm4Sg30QYsQA2vgTFx4kN8eV0aTUmc6vRqxqNRnNW03uEAGD2o4CA\n7x4lNtQXs0VyulTnEmg0mt5N7xKCkHiYfA/s/YzkOlVZUJuHNBpNb6d3CQHA5N9BYAzDdj6FwKLL\nUWs0ml5P7xMCL3+Y/Sjeebu5wrBeRw5pNJpeT+8TAoCUqyB2HA94/Yu8/ILuno1Go9F0K71TCAwG\nmPMskRQxLmtpd89Go9FoupXeKQQA8ePZGjiLi8s+hSLnylqbzBae/M9+ThTqGkUajebsofcKAbB5\n4D2YpUD+9xGnxh/KKeef64/x+Y6TLp6ZRqPRdB29WgiC+yTwnvl8SFsBVUWtjj+SVw7A/uwSV09N\no9FouoxeLQRxoX6sNo9HSDMc+aHV8WeEoNTVU9NoNJouo1cLQWyoLzvlIGq8QuDQt62OP5JXAcCJ\nwipKqupcPT2NRqPpEnq3EIT4YsFAZsg5cPi/YDG3OP5Ibjl+XqrH8QG9KtBoNGcJvVoI/L09CPXz\nZIfPRKgsgJPbmx1rsUiO5pdzfnIfQJuHNBrN2UOvFgJQ5qF1lpEgjHBodbPjThZXUV1nYdKAcCIC\nvNl/SguBRqM5O9BCEOJLeqkHxE9U/QqaweYoHhgZQHJMkF4RaDSaswaXCYEQ4m0hRK4QYm8z+68T\nQuwWQuwRQmwUQoxy1VxaIiHCn8yCSkyDzofTe6D0lMNxNkfxwEh/kqODSM8pp9akexloNJqejytX\nBEuBOS3sPwZMk1KmAE8Ab7hwLs0yIiaYWrOFY6FT1IZ0x9FDR/LKCfHzJMzfi+SYIGrNFg7nlnfh\nTDUajcY1uEwIpJTrgMIW9m+UUtqyuH4G2tZtuZMYGRcMwLbKPhAc32wY6ZHccgZGBiCEIDk6CNAO\nY41Gc3bgLj6CW4FVze0UQtwhhNgmhNiWl5fXqRfuF+ZHkI8He06VwuAL4OiPYKppMu5IXgUDI/0B\nSIzwx9fTqB3GGo3mrKDbhUAIMQMlBPc3N0ZK+YaUMlVKmRoZGdnZ1yclLpg9WSUw5EKoq4CM9Q3G\nlFTWkV9ew6CoAACMBsGw6EBdakKj0ZwVdKsQCCFGAv8ELpNSdltjgBGxwRw4XUpN/GTw8G3iJzhs\nFzFkIzk6iP2nSpFSdulcNRqNprPpNiEQQvQDPgeul1Ie6q55AIyMDaHOLDlUYIbE8+DQN2B3gz/i\nSAhigiitNulWlxqNpsfjyvDRZcAmYKgQIksIcasQ4k4hxJ3WIQ8D4cA/hBA7hRDbXDWX1kiJVQ7j\nPSdLYMgFUJQB+en1+4/kleNlNBAX6lu/rd5hrP0EGif5dt9p7ln2S3dPQ6Npgiujhq6RUkZLKT2l\nlHFSyreklK9JKV+z7r9NShkqpRxtfaW6ai6tER/mS7CvJ3tOFsPgC9VGu+SyI7kVJET44WE883UN\n6xuEQejIIY3zrD+cz4pdpzCZdf6Jxr3odmexOyCEICU2WK0IQuIhKrlBuYmjeeUNzEIAvl5GEiP8\n9YpA4zSl1oq1xbpyrcbN0EJgJSUumIOny6iuM6sw0uOboLqEWpOFzMLKJkIAkBwTzD4tBBonKa02\nAVBcWdvNM9FoGqKFwEpKbDB1ZsnB02UqjNRigiNrOF5Ygdki60NH7UmODuJkcRUllfoJT9M6th4W\nxfr3ReNmaCGw0sBhHDcBfEJg7XOcPqRKUzteEegMY43z2ExDRVoING6GFgIrcaG+hPp5qsQyowfM\nfx3Kczjn+yv4o8fHDAg1NjlGl5rQtIXSapsQaNOQxr3QQmBFCMEIm8MYYOgc+M1WdgTN5rceX+L/\n9nTI3NjgmMhAb6ICdW8CjXOUVmkfgcY90UJgx8i4YA7lWB3GAH5hPOl5D89EPA3mGnjnIvjPvVBb\nWX+M7k2gcYZak4Uq6++VNg1p3A0tBHakxAZjskgOnC4DQErJkbwKquKnwV2bYNLdsO1tWPdc/THJ\n0UEczi3TvQk0LVJWfebmr1cEGndDC4EdKXEhAOzJKgYgt6yG8hoTA6MCwDsA5jwNwy+HLf+EKjUm\nOSaIOrMkPbes2+atcX9soaMARRV6RaBxL7QQ2BET7EOYv1e9n+CItfHMIPuIoSn3Qm0ZbHkT0KUm\nNM5RapdEVlylVwQa90ILgR22DOPdWVYhsBWbs88hiB6pylD8/A+oraB/uD9+XkadWKZpEVvEUKif\np84j0LgdWggakRIbTHpuOdV1Zo7kVRDg7UFUoHfDQVP/CFWFsH2p6k3QN1A7jDUtYosY6hfur8NH\nNYrcNPjxWTB3/4OBFoJGpMQFY7ZI9meXcji3nIGR/gghGg7qNxESpsLGl8FUQ3JMEGm6N4GmBWwr\ngv5hfhRV1unfld5OdSksWwg/Pg1f/7FB2fvuQAtBI+ozjLNKOOKg2Fw9U++FsmzY+RHJ0cGU1Zg4\nUah7E2gcY/MR9A/3axBKqumFSAlf3wvFJ2D4fNjxLmx6pfXj9nwKxcddMiUtBI2IDvYhIsCLn48W\nkF1S3dA/YM+AGRAzFjYsZnhfP0BnGGuap7S6DqNBEBuielroXIJezM6PYM8nMP1/4cq3Ifky+PYh\nOPC14/HmOlj5/+CzW2HDiy6ZkhaCRtgyjL8/kAtQ37DewUA47z4oyiCp8Dvdm0DTIqVVJoJ8PAjx\n8wJ0LkGvJe8QrLxPmZan3gsGA1z+GsSMgc9ug+xdDceXnYZ358KW11Ue05xnXDItLQQOGBkbXJ8g\n1qxpCGDIRRCZhNfGvzMowo/9p3Qze41jSqvrCPL1JMTPE9AVSLsNUy28MR3euwxy9nftteuq4dNb\nwMMHrngDDNb6ZV5+cM1y8A2DjxZCabbafvxneH2aEocr31J5TEZPl0xNC4EDRlj9BEaDoH94MysC\nUGo+9Y+Qd4Crg/boXAJNs5RW1RHk40modUWgI4e6iW1vw6lfIGsbvDYFVt0PVUWOx1oscGIL7PoX\n1JR3/Nr/fRhy9sDlr0JQTMN9gX3g2n9BjdWJvGkJLL1EicRt30HKgo5fvwVc2bP4bSFErhBibzP7\nhRDiJSHEYSHEbiHEWFfNpa2MtGYY9w/zw8ujla9o+HwITWBe6TJOlVRRVKH/wDVNKa02EeTrQah1\nRaB9BN1AdQmsfRYSp8Hv98C4m2DLG/DyONi+FCxmMNVA+nfw79/B34bCW+fDF3fA4hRY+9f6igJt\n5sBKZd6ZeJcqaOmIviNgwdtwejes/jMMmg23r4E+w9v7iZ3Gw4XnXgq8ArzXzP6LgMHW10TgVeu/\n3U6fIFVV1FEzmiYYPWDqH4la8VsWGb8jLXsS5w6KcP0kNT2K0qo6ogIDzvgI9AND17N+scr/Of9x\n8AuDS19QYrDqfnXj3/ASlOeqygFeAepGnDQXAvqoUPE1T8LGl2DC7TDp1+DfzN+5xQKlWZB3EPIO\nqFfav6HvSDj/sZbnOORCuOJNqMiHCXcoq0MX4DIhkFKuE0IktDDkMuA9qQKqfxZChAghoqWU2a6a\nk7MIIXjjhlRCfJ20x41eRO2er3jo6PusOjgLBs117QQ1PY7SamUa8vIw4O9l1CuCrqbkpKoGkHI1\nxIw+sz16JNy8EvZ+Bptfh4TJMOxStWrw9DkzLnGqstX/9Df46QX4+VVImKI6GZpqVXVic61aURSf\ngLqKM8f6R0HsOLj4efBolJzqCBebgRzhyhVBa8QCJ+zeZ1m3NRECIcQdwB0A/fr165LJjY4PcX6w\nwYDXVW+S/dx4pu68D2ZOAd9Q101O0+MorTIRbDULhfh56aihlpASdi2DkP7qxtwZrHkapAVmPth0\nnxDq5tvaDTh6FFz9nnrS3/CiMuEYvcHoBV7+YAwDDy8VWh45FCKHqX/9wjrnM7iQ7hQCp5FSvgG8\nAZCamuqeKZl+Yfwz+lEeyP49fHEXLPyoy5Z1GvfGlkAW5KP+3EL8PCmu0isCh0gJP/5F2fKFEeb8\nRZlIGmf322M2QclxCBvgeH/OPtj5IZxzN4T27/gcI4fC5f/o+HnciO68U50E4u3ex1m39Vh8Eybw\ntOk6OLRK2RI1Gs70IgiymhpD/bx01JAjpFRP7mufhdHXwZA5sOr/wX9+33w9nowN8PpUeGmMegCr\nLGw65r+PgE+QivDTOKQ7hWAFcIM1emgSUOIO/oGOkBwTxDumCyhOvAS+f1z9kmp6PbZeBEE+NtOQ\nrkDaBClhzVOq6dOY62HeK/A/H8CUP6iInvfnN7zJl52Gz26HpRdDTRmk3gp7PoYlE2DfF2dq9xz9\nEQ7/F877U48w0XQXrgwfXQZsAoYKIbKEELcKIe4UQtxpHbISOAocBt4Efu2quXQVw2OCAMH3gx+E\n0ASVPFKe293ToqLGxD3LfuFEYWXrgzWdjq3OUJCvMg3pFUEjpIQfnoR1f4WxN8Dcl5RZ1WCA2Y/C\n/DdUPP+bM+D0XhVj/3Iq7P9S3eDv3qIigO74EYJi4ZOb4F+LoPSUit0P7gfjb+/Wj+juuDJq6JpW\n9kvgblddvzuID/UjwNuD3XkWrrz6PfjnLJUenrIABsxU0Qq2bMIu5IcDuazYdYqhfQO5e8agLr9+\nb8dWedS2Igj186Skqg6zRbUSt4cAACAASURBVGI0tGD77g3YROCn55UIXPpiU9/aqP9R9v/l18Jr\nVufxoPPhomchfOCZcX1T4Lbv4eclysT04mgVzXPFmw0jgDRN6BHO4p6CwSBIig5UTWr6nqvSwtc+\no37Rf3gSfEJgwDQVVZB8WZctVdceygNga4YD+6nG5dh6Edh8BCF+XkipVgqh/l7dObXuRUr44QkV\nkjn2Rrh0cfMBFvHj4Y416u8oaS4MvdixA9noAZN/p0JAv75XXWNE14dj9jS0EHQyydFBfLo9C4tF\nYki6FJIuhfI8OLYWjqyBo2tg/1cqFnnRpyoCwYVYLLJeCLZnFumn0G6g8YogpD67uPbsEwIp4fgm\nFWrp1UJ5Finh+8dg/d9bFwEbwXEw/zXn5hE+EG74yvl593J0fGMnkxwTREWtmeP29viASGUeunwJ\n/GEf3LIaTNUqfT1jvUvnk3a6lLyyGqYMiqCs2sShnDKXXk/TlBIHPgLg7AshrSlTpZLfuUgVSzu9\nx/E4KeG7R5UIjLvZORHQuBT97Xcyw2NUwbpmS1ILAf0mqUJSAX1VNMTuT5o/YWm2inxoZwejHw+q\n1cC9FwwBYJs2D3U5pVV1eBgEvp7KP3SmAqmbOYxP7YRl18KWN9Uqti3kpsGbM1XEzsQ7lSi8OQs2\nv9Hwd1dK5cDdsFhF+lzyghYBN0D/D3Qyg6IC8DCI1iuRhvaHW1dD3AT4/DZlKrL9wdRWwO6PlUj8\nPVmVzF33fLvms/ZQHsNjghgTH0KfIG+2ZTZTaVHjMmwlqG0tT+srkFa40Yqg+AR8dDUc/k7Vy//b\nUHj/Cti5TLVVbIndnygRqCpS5piLnoW7Nih/2Ko/wfLrVOinlPDtgyrHZvxtcMnftAi4CdpH0Mn4\neBoZFBXAPmd6E/iGwvWfw5e/VvbS/HS1Ytj/FdSWQ0g/mHofFB5VBa98gmHiHU7PpbS6ju2ZRfzq\nvAEIIUhNCGNbhhaCrsbWlMaG25Wiri5VIlBXBb9aq27Yez9VXbS+vFPVz0+YAhFDVPSO7RXQR93Y\nt70F/c5VlTODotU5/SPgmn/B5ldVQtdrU9U5di9XmcIXPddytrCmS9FC4AKSo4PYcCTfucEe3iq8\nLaQfrH8BvAJVaetR10C/c9QTk9mk/khX/Ql8Q2Dk1U6deuPhfMwWyfShUQCM7x/K17uzOVlcVd8y\nUeN6bCsCG4E+HhiEmzSnMdfBJzdC/iG47lOISlLb+zwMMx+CrK1qdZq5Ub3qHOSinHsPzHq4adMU\ng0GVdeh/Lnxys1UEfqVWDFoE3AotBC4gOSaIz385SX55DREBTlQbNBhg9iMqjjqgj2pGYY/RQz1t\nfbgAvrgTvANh6EWtnvbHg3kEenswpp8qoJeaoMJVt2UUEjs6Fo5vhq3/hIv/qgRG0znUlKkEqP6T\nwdOnvimNDYNBEGKfVFaUoWrl9xnRtXkmUsLKP8GRH2DeyzBwRsP9QkD8BPWyjS/PgYIjapVadEw9\nrAw+v+XrxIyBO39S38nAmVoE3BCnhEAIMRDIklLWCCGmAyNRJaTb2aXh7CY5JgiAtOxSpg6ObLDP\nYpE8+u99jOsfymWjYxseGJbY/Ek9feCaZfDuPPj4Rlj0mSqN2wxSqrDRKYMj8DQqO+ywvoEEeHuw\nLaOIy5KCVI/UkuOqK9LCZdpe21HMJtixFH58BiryIHwwzH2R0moTfYMbJjSF+HpSUVGmzCabXlHl\njL2Dof85yoTSf7KqX2904bPaxpdg+zsw5V71ENIaQkBgX/Vqa1VQ70AYNKt989S4HGf/8j8DzEKI\nQagqoPHARy6bVQ8nOVoJgSOH8bubMnhvUyYfbj7e9hN7ByoBCEtU7exO7mh26KGccrJLqpk25IwQ\neRgNjOkXohLLvnsUSk6oGO5D36jMTk37kFJ1oHr1HPj6j8qWPu9lVZ9+6cX8uvxlojyrGxwyw7iT\nPx+7SUXPjFyoyigMv1z5ib59UJVTeC4RPrxKNVTJ2q6EprPY96WK3hk+X5mANL0aZx83LFJKkxBi\nPvCylPJlIcQvrpxYTybEz4vYEF+VYWzHoZwy/rLqAB4GwZ6sEkxmCx7GNj6F+4XB9V/A2xeq1cG8\nF2HElU2G/XhQ1TiaNrThimR8Qhgbv/8Siv+p2ubN+Yu6Ya15GqJHw5AL2jaf3k7WdvjvQ5C5Qa0A\nFn50Jut1xJWw5mku27iE89N/gf1/h7jx8M0DPFTyJccNcXDT12oFAKqUAqiQ4cwNKsckcwOkf6u2\newVA/ER1DoNRNUEx1555efiqMgvRI5UY2dvszXXKNHPkB/U69YuKWLv8Vb0S1DgtBHVCiGuAGwFb\n+y0n23f1TpKigxrkEtSYzNyz7BeCfDy4c9pAnvw6jcN55QzrG9T2kwfFwM2rlAPu01vUDePCvzSo\np7L2UB5D+wQSHdzQKTwh1pvLPN6gKiAe31kPqRvWpX+HnL0qjPWOHx3XdTfXQdoKZe9tru57b0FK\nFWa58SU4tg78IlT3qXE3Nbz5evlTM+txrvyxL+8EvEfgx9erJiYIVkXeyjMlF7DWJgL2BEU3bJRS\nlqMEwSYOa7+3DhQq2MDora5bWwGmKrXL6A19kpUwVOSredaWqxr/ceNh+v+qloueOmhA47wQ3Azc\nCTwlpTwmhEgE3nfdtHo+yTFB/HAgh6paM75eRv727SEOnC7jrRtTSYzw58mv09h9oqR9QgAq3f7m\nlarc9caXVHTHVe9C+EDKa0xszSjklslNfQ7jjizB05DL8n7/YKGtBICnryr5+/o0WL4IbvvvmfIA\nFrMKI/zxL8qpGZoAv1qnQll7G6YaFUGz6RXVhzYwBmY/Bqm3qHr3DiirNrFXDuCbcz/ielaqpK2Z\nD7J9Uw25zpoHA/vAiCvUyzYPYWzqP7CYlWnp9G7VVvH0btUr1ydYRZoNnAkJU3VggKYJTgmBlHI/\ncA+AECIUCJRSPuvKifV0hscEYZFwMKeMyhoTb/50lOsm9mNWUh8sFkmgjwc7s4q5enx86ydrDqMn\nXPCEMi188St1I5/3IpsMU6gzywb+AQCOb8Zz6+us9LmEz4sGsNB+X2gCLHgLPligGnnPfwPSvlIm\no/xD6snywqeVXfmru+Hq9zs3+qO2EkqyoCJXRaaU56mfTTUqBDEopvOu5SzVpdYG5GmQs1/1ta3I\nhT4pMP91GH6Fak3YArYS1IF+fjDmd/XbQ/0PU1VnprrOjI9nGyOFmut7azBC1DD1cjLEWKMB56OG\nfgTmWcdvB3KFEBuklPe6cG49GpvDeNORAt7blEFiuD//d4mK0TYYBKPiQtid1UlBV0MuhDvX15uK\nEgNSudlrNKkRY8+MqatSN/DgePYO+AO7tuZTYzLj7WF3Exo0W/V0/eEJOP6zciZHDlN9WofNVbZk\naVHOzM2vwaS72HKskIe/2ssLV4+uj5ZqE2U5akWz7e2mMerCaE2wWwE3fNmw5LAzSKnCHLO2qfME\nRKlG4gF9VDKfwaAEqPi4CoUsylCvgsOQewBKs86cy8NXCe65v1GNzZ0UwfqmNL4N/9SCfW1lJuro\nG9z1pck1GnucNQ0FSylLhRC3ocJGHxFC7HblxHo6caG+BPp48MJ/DyIlfP7rc/HzOvN1j4wL5o11\nR9v3ROgIq6lIrl+M949v8YhhGyx+S8WAD7vUeoNLh+u/YGRVPDWbctl7spRx/UMbnmfKvapuTPYu\ntSpIWdAwtv2c30DmJvj2QQ56DOHmFXVU1Jr5YHMmT89PcX6+JVmqAfj2d8FSpxyrgy8A/0h1ow6I\nAt8wOL0LPrgS3p6jsrD7tnANiwVy96n5ZW5QYlZ+2vFYg4eKwqpqlGntFQChiSqMM3IYRCWrJ+yQ\n/u2K8a9vSuPT0KV2pvBcbZPQUo2mq3FWCDyEENHA1cD/uXA+Zw1CCJKjg9h8rJD7LhjCyLiGdtlR\n8SGYLJL92aWM7RfazFma5+NtJ1i99zQLxsUxO7mPyhUwenIk6S5mr0ri5Vk+zPXaoRy8/7WGB465\nHgbOJLW8BlCJZU2EwGBQJqLmPxhcvoTaJVMJ/M8dDAh4gdCIaFbuyebRucPx8mghAsVUCzl71M1/\n50eAhFELlfg097QfMwZu/gbevxyWXgLXfgL9JjY6bw3sWqaqWRZlqG3B8ZB4nrqhx09SZrRyq9mp\nIk/9W1WsHLOhico0FpoAfuGdavIqbdSv2EaorRS1O9Ub0vRanBWCx4HVwAYp5VYhxAAg3XXTOju4\nYmwsfYJ8uHNa05vc6HglDLtOFLdLCD7dnsWWY4V8fyCXqEBvFo6PZ+GEftbeA4LR486FsNkw7f+p\nm2PmJkieB0BEgDeJEf5szSjiV9Pa/rkOlBh5ouJu3uFBPol6l40Tl3DLu3msP5zHzGF91CCzSdnW\nT/1y5pWzT4U5Gr1h3I2qgUhIv9YvGDkEbvkG3rsc3ruM4nnvsFGM5uJhIbDjPbWyKD0JseNg2v3K\nhOPovBGD2/5hO4itKU1wIyEIsa0I3KXekKZX46yz+BPgE7v3R4GmweuNEELMAV4EjMA/pZTPNNrf\nD3gXCLGOeUBKudLp2bs5/zO+H/8z3vGNrk+QD32CvNl1on1+gsO55Vw1Lo4Lh/flw82ZvLzmMK+s\nOUygjycDI/2JD7MrU2F72rUjtX8o36XlqAY6bWhUczi3nEX/3IzRczBl5zxG+Nr/47zEjxjqG03G\nuo/geIGKrc/eecbm7x2k2nROuks94fefrEw/bSGkn+rj8MF8Ar9YRG7dTMyBOzBW5avzXfaK6vzm\nZuULGjelsRHqb2tOo1cEmu7HWWdxHPAyYMsr/wn4nZQyq4VjjMAS4HwgC9gqhFhhjUCy8SDwsZTy\nVSFEMqqhfUKbP0UPRTmMnahS2oiC8hoKK2oZ2jeQ2cl9mJ3chxOFlXy05Tif78jiirFxrZ5jfEIY\nn2zP4mh+OYOiAp26bmZBBdf982dA8NHtkwiPmAX52/BY8zirAU6BzPFC9B2pShbEpkLsWGV66Yyk\npYBIuOlrDr9wMTfJbznuM4l+//Ne28sddCGlVXV4GgU+ng0/v9tVINX0apw1Db2DKilxlfX9Iuu2\nlqpNTQAOW1cPCCGWA5cB9kIgAVuoSTBwysn5nBWMig/h2/05lFTVNTEdtMTh3HIABvc5cwOPD/Pj\n/jnDuH/OMKfOkZqgzFFbM4qcEgKT2cKNb2+h1mRh+R3nMDAyQO2Y+xIEx3OsLpTfrzdy+1WXcenY\nBKc/S5vxCeYm8//hXXMKP4ay0o1FAKyVR33O9CKw4eNpxNvDoE1DGrfA2ce0SCnlO1JKk/W1FIhs\n5ZhY4ITd+yzrNnseBRYJIbJQq4HfOjqREOIOIcQ2IcS2vLw2dk5yY0ZZHch72rgqSLcJQVRAu6+d\nGOFPuL+X0w3tV+/LIaOgkmeuHMnQvnbC4RMEFzxB/4v+QE7gCL7c42T57XaSW1ZNdoVEhg1kf3Yp\nGfkVLr1eRymtMjVxFNsI9fPSpiGNW+CsEBQIIRYJIYzW1yKgoBOufw2wVEoZB1wMvC+EaDInKeUb\nUspUKWVqZGRr+tNzSIlT2bm72phPcDi3HH8vI9EdCDtUjWpCnW5Us3TjMfqF+TE7qY/D/QaDYO6o\naNYeynXpU66tkN/vZinH79d7sl12rc5ArQgcL7xD/DzdoyeBptfjrBDcggodPQ1kAwuAm1o55iSq\nSqmNOOs2e24FPgaQUm4CfIAIJ+fU4wn29WRAhH+bHcaHc8sZFBXQxNzQViYkhnO8sLLVbmp7T5aw\nNaOIG87pj7EFx/K8UbHUmSXf7G0mdr8TSMsuA2DWsD6M6RfCSncXgqq6FlcE2jSkcQecEgIpZaaU\ncp6UMlJKGSWlvJzWo4a2AoOFEIlCCC9gIbCi0ZjjwCwAIUQSSgjOHtuPE4yMC27ziiA9t8xpB29L\nLBgbR7CvJ89+c7DFce9syMDfy9hqOYwRsUEkRvjz1U7XuXr2Z5cSG+JLsJ8nl6REs+9UKZkF7mse\nKq02NYkYshHq76mdxc3w3DcH3F7kzyY6EsrRYnkJKaUJ+A0q/yANFR20TwjxuBBinnXYH4HbhRC7\ngGXATVLaOrj3DkbFh5BTWsPpkurWB6NMDTmlNQzqgH/ARrCfJ7+ZMYh1h/L4Kd2x/uaV1fDvXadY\nMC6u2RuaDSEE80bF8POxAnJKnfs8oJro3LJ0K1/tbLxgbEpadilJ1vIdF6Wo/rjubB5SK4LmTENe\n2jTkACklb60/xgoXPlBoGtIRIWjVLiGlXCmlHCKlHCilfMq67WEp5Qrrz/ullJOllKOklKOllN92\nYD49klG2xDInVwWHO8FRbM/15/QnNsSXv6w8gMXSVIM/2nycWrOFG85NcOp880bHICX8e5fzf8SZ\nBZX8cCCXT7c3G40MQHWdmaN55SRHq9VQbIgvo+Pd2zxkixpyRIivJ8VVdfSyZ59WySurocZkIc+a\nAa9xPR0RAv3b2wkkRwfhYRBO+wkO59hCRztHCHw8jfzpwqHszy7lq10Nn8hrTRY+2JzJ9KGRZ8JF\nW2FgZAAjYoPaJAS2yKXtmUWYzJZmxx08XYZF0qC43SUp0ew9WcrxAgdN1buZGpOZ6jpLiz4Cs0XW\nF6bTKDIL1f9lXpkWgq6iRSEQQpQJIUodvMqAbqgLfPbh42lkWHSg04ll6blleHkYiAv1a32wk8wb\nFcPwmCCeX32I6jpz/faVe7LJK6vhJidXA/bn25VVwjEnQzttkUuVtWb2OmjvacPW6MdmGgK4KKUv\n4J7moTJb5dEWooYASrR5qAE2Uc8rq9GrpS6iRSGQUgZKKYMcvAKllC7sqt27GBkXwq6sYoemmcYc\nzi1nYGRAi9E7bcVgEPz54iROFlfx/qbM+u3vbMxgQKQ/5w1uW8ju3FExCOG8eWhbZiEjraG0m482\nH5Wcll1KgLcH8XYiGBfqxyg3NQ/VVx5tYUUAOru4McetK4KqOjMVteZWRms6A92s1A0YHRdCWbWJ\nDCeiX9JzyzvNP2DP5EERnDckkpd/SKe4spYdx4vYdaKYm85NaFMtIoDoYF/GJ4Tx1c6TrT7RFZTX\ncCSvgjkj+jIgwp/Nx5pPcEvLLmVY38Am87kkpS97Tpa4nXmopJkS1DbO1BvSQmDPicIz/4/aPNQ1\naCFwA0bGO5dYVllrIquoqlMihhzxwJxhlNWY+MePR1i6IYNAbw+udKJukSPmjozmSF5FvXO7ObZn\nKrPQ+IQwJg4IY+uxQswOVkYWiyQtu8xh85uLRqjooZV73WtV0FxTGhtnKpBq05A9mYWVeFjFXgtB\n16CFwA0YHBWIn5eRXSda9hMcya2wjneNECTHBHHFmDiWbshg5Z5srh4fj793+yyAs6wZyN8fyG1x\n3PbMIryMBlJig5mYGE5ZjYm07KZ+gqyiKsprTA38Azbiw/wYFRfsduah5prS2NCmIcccL6xkuFXw\ntRB0DVoI3ACjQTAitvXEssN5Kqu2syKGHPHHC4aAALOU3HhOQrvPExPiS3J0ED+ktSwEWzMKSYkL\nxsfTyMQBYQAOzUP7s5VIJjsQAoCLU6LZneVe5qHmmtLYsDmRdb2hM1TVmskrq2Fcf/W7kFfmfD6K\npv1oIXATRsUFs+9UKbWm5sMn03PK8TAI+of7u2weMSG+PHRpMr+dOZh+4R2LTJqVFMW2zEKKKhw/\n8VbXmdlzsqS+Emp0sC/9wvwcOoz3Z5dhEDQseGfHxSnuZx6yNaVpbkXgYTQQ5OOhy0zYcaJICfmo\n+GCMBqFzCboILQRuwqj4EGpNFg7llDU7Jj23nIQIf9WW0oVcP6k/954/pMPnmZXUB4vE2jWtKbtO\nFFNnloy3Pv0BTEwMY0tGYZMIqv2nShkQGdBsf+f4MD9SYoP5bn9Oh+fdWZRWO+5FYE+ov84utifT\nuqLrH+5PRIAXuaVaCLoCLQRuwhhru8r1h5sv43zERRFDrmJkbDARAd58l+b45rzN6ii275s8ITGM\n4so6DuU2FET70hLNkZoQyr5TpS0mpXUlpVWOexHYE+LnpX0EdthCR/uF+REZ6K1XBF2EFgI3ITbE\nlzH9QvjyF8f1dmpMZjIKKnqUEBgMgpnDIll7KI86BzfnbRmFDIoKINTfq37bpAHhAGyx8xOUVNZx\nsriKpOiWC+2lxAZTVWfmSJ57FKErrW6+F4GNUF2KugEnCisJ9PYg1M+TqEAf7SzuIrQQuBFXjInl\nwOmy+pr79hzLr8AiYWAPEgKAmcP6UFZtatIAx2KRbM8sYnxCaIPtcaG+xAT7sPnomfFpp9X30Zyj\n2EZKrArD3XOy7e0/XYFaEbQcdRWqVwQNOF5YSXyYH0IIIgO8tRB0EVoI3IhLR8bgaRR88UvT4mtn\nis11vPx0VzJ1cAReRkOT6KH03HJKq02k2vkHQFUwnTggnM3HCuqT0WzC2JoQDIgMwM/LyF53EYLq\n5nsR2Aj21SsCezILKugXpoIUIgO9KaiodZhXoulctBC4EaH+XkwfGsVXO081+eVPzynHIGBApOsi\nhlyBv7cHkwaG80OjfALbCiG10YoAlMM4v7y23sSTll1KRIAXkYHeLV7LaBAMjwlysxVBa6YhL8pr\nTC1Gi/UWLBbJiaKq+mi1yEBvzBapV0xdgBYCN+OKMbHkltWwoZHT+HBuOfFhfs1Gzbgzs5OiOJpf\nwdG8M1nG2zIKiQz0rn/6s2ei1U+w+ZgKI91vdRQ705FtRGww+0+VusVTpFM+AmuZCVs5it5MblkN\ntSZLgxUB6KSyrkALgZsxMymKIB8PPt/R0DyUnlvWoxzF9swcFgXQYFWwNUP5Bxzd3BPCVcTIlmOF\n1JktpOeUt2oWsnHGYdxyaYuuoKWmNDbOlJnQT732EUOghaAr0ULgZnh7GLl0VAyr9+VQUaMSkkxm\nC8fyKzqlPWV3EBfqx7C+gfVhpNklVZwsrqrPHm2MEIKJiWFsPlrIkbxyas2WVkNHbdQ7jJ0s6+0q\nquvM1JgsTpiGbIXn9IrA1nK0XggCtBB0FS4VAiHEHCHEQSHEYSHEA82MuVoIsV8IsU8I8ZEr59NT\nuGJMLFV15vom8JmFldSZpcuKzXUFM4dFsTWjiJKquvr+A40jhuyZOCCc06XVrN6rxMNRsTlH2BzG\n3e0nqO9F0Gr4qK43ZONEYSUGobLbwW5FoHMJXI7LhEAIYQSWABcBycA1QojkRmMGA/8LTJZSDgd+\n76r59CTG9Q+lX5gfX1hzCtJzOrc9ZXcwKykKs0Wy9lAe2zIK8fMytmjumZSoVgsfbM7Ey8PAgAjn\nnORGgyA5OqjbI4fq6wy1Ej4abBUKbRpSpqGYEF+8PNRtyd/bA38vo84u7gJcuSKYAByWUh6VUtYC\ny4HLGo25HVgipSwCkFK2XKGslyCE4PIxsWw4ks/pkup6e3dPyyGwZ3R8KGH+XvyQlsO2zCLG9AvB\no4VSGYOiAgj39yKvrIahfQJbHNuYEbGqblN3Ooxba0pjw5ZMp01DSggaBw/o7OKuwZVCEAucsHuf\nZd1mzxBgiBBigxDiZyHEHBfOp0cxf0wsUsJXO0+SnlNGbIgvAe0sCe0OGA2C6UMj+f5ALmnZpc36\nB2wIIZhgXRW0llHcGHdwGNf3ImjFR+DvZcTTKLRpiBaEQFcgdTnd7Sz2AAYD04FrgDeFECGNBwkh\n7hBCbBNCbMvLc1zA7GwjMcKfMf1C+OKXk6Tnlvfo1YCN2Ukqy9giW/YP2LAJgbMRQzZsbS+702Fs\nWxEEtxI1JIQgxM+r1/ctrqgxkV9eS7xDIdArAlfjSiE4CcTbvY+zbrMnC1ghpayTUh4DDqGEoQFS\nyjeklKlSytTIyLb1z+3J2EpOpGWX9mj/gI2pgyPwNAoM4kyRvZaYndSH2BBfpgyOaNN13MFhfMZH\n0PKKAFTkUEEzpbp7C7by0/0blT7XZSa6BlcKwVZgsBAiUQjhBSwEVjQa8yVqNYAQIgJlKjrqwjn1\nKGwlJyyyZzuKbQT6eHLe4EjG9gt1yswVH+bHhgdmtjls1h0cxvW9CFrxEQAkRQexPbPIJT6N1npG\nuwu2hkKOTEOl1Saq63QTe1fiMiGQUpqA3wCrgTTgYynlPiHE40KIedZhq4ECIcR+YA3wJyll064k\nvRRbyQmgR4eO2vPSNWN4++bxLr9OdzuMS6vr8DIa8PZo/U9sVlIfCitq2Xmi5Q517eGWpVuZ98p6\nDpxuWsjQnWicTGbDFkKarx3GLsWlPgIp5Uop5RAp5UAp5VPWbQ9LKVdYf5ZSynullMlSyhQp5XJX\nzqcnctuUREbFBTudUOXu+Ht7OGUu6Sg2h/HRbnIY27KKnSmLMW1wJEaD4Ptm+ja0l/IaE2sP5bE7\nq4S5L69nyZrDbtOroTHHCysJ9PGoD6e1obOLu4budhZrWmHigHC++s2UdjeR762kxHVvSerSapPT\nghfs58n4hFC+b6W/c1vZkVmERcKLC0dzwfC+/HX1Qa54dSPpLXTB6y6OF1bSP9yviXBGBfoAWghc\njRYCzVnJwMgAfD27z2FcWlVHoBP+ARuzk/pwMKeME1YTSWewNaMQo0EwO6kPS64dyyvXjuFEYSWX\nvLye19cecYvCfDYchY6Czi7uKrQQaM5KjAZBckz3OYxLq1tvSmPPrKQ+AE3KdXeELccKGR4TVL+a\nvHRkDN/+YRozhkbyl1UH+GxH074X3YHZIskqrGoSOgoQ5u+FEOjsYhejhUBz1pLSjQ5j5SNwfkWQ\nGOHPgAj/Zvs7t5Uak5mdJ4oZn9AwcS8y0JvXFo0j2NfTJc7p9pBTWk2t2eJwReBpNBDm56VXBC5G\nC4HmrCUlNpjK2u5xGLfFR2BjVlIUm48WUm6tOtsR9p4socZkcZi4J4RgaN9ADp52D1+BLWKof5jj\nelI6qcz1aCHQnLV0p8O4xIleBI2ZldSHWrOF9ekdz57faq3wmprguJTHsL6BHDpd1qE8g5zS6k5p\nqNNc6KiNs1UIckqrCymWhQAAHvdJREFU+XS7e5jntBBozlq6y2FcXWem1oleBI1J7R9KkI8H33VC\n9NDWY4UMiPQnIsBxe88hfQIpqzFxqqT9dXyuffNnHvv3vnYfb+N4QSVGgyA6xMfh/rM1u/jjrSe4\n75NdnCyu6u6paCHQnL10l8O4vrxEG3wEAB5GA9OHRrHmQG6H/BoWi2RbZhETmlkNgFoRABxsZ6JZ\naXUdR/Iq2H+q44lqqvy0D57NVJi1VSDtKVnSzpJdqkQ4rRO+w46ihUBzVtMdDuP68hJtiBqyMSsp\nioIOZhkfyi2jpKquiaPYniFWITjQTj+Bzb9wNL+iw9/t8cLKZv0DoISg1mSpr+h6tpBrE4JsLQQa\njUsZYXUYH8vvOodxe1cEANOHRGE0CH440P7ooa3HCgFaFIIgH09ign041E4hsN28ak2WDuc+nCis\ndBg6aqO17GKzRbpFj+q2ctoqBPu1EGg0rsXWw3h3B0tSv772CK/+eMSpsbabZIS/Y/t8SwT7eZLa\nv2NZxlsyiugT5E18mG+L44b2DWz3isD+KbYjN+HyGhMFFbXNOoqhdSFYvvU4F/x9Xf0Tdk8hx5ob\noVcEGo2LGRjpT1SgN099ndZuc4vJbOGVNYd5bvWBVv0NFTUmFn+Xzph+IYyIbV99qNlJfThwuoys\norY/aUsp2XqskPEJYa3WORraN4gjeeXUtaP+UFp2WX39q8O57ReC5qqO2hPVSnbx2oN5mC2yQ/Po\nakxmC/nlNfh4GsgoqOyUkOGOoIVAc1bjYTSw7I5J+Ht7sPCNTXyz93Sbz7Erq7i+Gf2jK/a16LR8\nfe0R8spqeOjSZKcKzjliVpKqONueLOOsoipOl1bXN/VpiaF9A6gzSzLyK9p0DbNFcvB0GZMGhBER\n4N2hFUF9DkF4CyuCgObrDZktks1WU9ixgrZ9ju5EOb9h8kDVa6O9TvvOQguB5qxnYGQAn//6XIb1\nDeKuD7fz1vpjbTp+7aF8DAL+96JhbMss4sudjfsrKU4VV/HGT0eZNyqGsU403mmOAZEB1izjtgvB\n1ozW/QM2hvZRT/RtNQ9lFlRQVWcmKTqIgZH+HXoSt/kXWvIRBPl64GU0kOugZWVadml9LkNmQefV\naXI1NrPQ9KGq0VZnRF91BC0Eml5BRIA3y26fxAXJfXjiP/t5dMU+p6Nd1h3KY1R8CLdNGcDIuGD+\nsvKAw6X8X1cfxCLh/80Z2uH5zhwWxc9HCtpsMtiaUUigjwdD+rTezGdglD9Gg2hzhrFNOJL6BjEo\nKoDDueXtDu3MKKggyEH5aXuEEM0mlW06otqXRAR4cayNK5vu5LQ1f2NMv1CCfT3Zn929Wd5aCDS9\nBl8vI/+4bhy3TE5k6cYM7vpgO7Wmlu3jxZW17M4q5rzBkRgMgsfmDSe3rIaXf0hvMG7niWK++OUk\nt01JJC60+adbZ7FlGT/w2W6WbTnOL8eLqKxtXRS2HCsktX8oRkPrZilvDyOJEf5tXhGkZZdiEDC4\nTwCDogIorVb9htvDL8eLGWF16LdERDNCsPFIPgMi/RnbL7TNJq7uxLa66RPkQ1J0oFORQ4//ez/f\n7e/cnhU2tBBoehVGg+Dhuck8eEkS3+7PadbMY2P94XwsEs4bopbwY/qFsmBcHG+vP1ZvG5dS8uR/\n9hMR4MWvZwzqlHmOTwhl1rAofjyYx/9+vof5/9jI8EdWM+P5H/nDv3Y67NhVUF7DkbwKxjvhH7Ax\ntG8gh9rYnyAtu4wBkQH4eBoZGKk657XHPFRcWUva6VImDQhvdWyUAyGoM1vYcqyQcwaEkxDhT2Zh\nJRY3Kq3dEjml1XgYBOH+XiRHB3PwdMu5Lrll1by94RiHcl2zctBCoOmV3DolkYRwPz5rpdbLukN5\nBPl4MCruzFPr/XOG4eNh5P+3d+/RcZdlAse/z0xuza2X3EtK0ia9JBRaaFpKablWRFFQLgqLCGdR\nXBcUXd21q7uwoqy4HNn1rHi8oK6cdRcrsFK1CLUUqEIvaQu9pfem1zRJkzZJm3vy7B+/36STyUyS\nSTPNJPN8zuE088tvZt63Heb5/d7nfZ/3id/tRFV5dfsJyg+d4is3zRzUXsyDEef18LMH5rP18ZtY\n+w/X8+P75vGlG2cwMyeNV7dX8Vc/XUddQDAoP+TUF+pvRXGgWTlpHK5v5mwYQ1AVVY09M4Z8W6ju\nG0LCeMPBelQZVCDISkvsE/y2HWvgbHsXi4oyKcxIob2zu2e1brQ70dBGdloiHo9QkpdGa0d3v0Nb\nf9l3EoBrpmdFpD0RDQQicrOI7BaRfSKyrJ/z7hARFZGySLbHGB8R4c55+aw/WB9yQZSq8taeWhZP\nzyTOr/xBVloijy6dzlt7alm57QTfebWCWblpfKJsyrC30+MRpkxK5oOX5PLo0un86L55/Pz++Ryu\nb+be59b3CgYbD9aTEOfpKbY3GDPdFcaDvStoaOng2OkWSvKc5+WNTyI5wcv+IdwRrDtQT2KchzlT\nBm5vVmoidWfbe2216csPLJw2icJMZzhutAwP1TS1kp3uzIYqnewE1f7WE6zdc5JJKQmURmjL2ogF\nAhHxAs8CHwJKgXtEpDTIeWnAo8D6SLXFmGA+fkU+IoTcoGVP9RmqG9uCXoXdv6iQ4uxUHn1hC0fq\nW/inW0oHNS4/HBYVZ/Kz++dz8ORZ7n1uPfVnnfH5jZX1zM2fQGKcd9CvFW4g2O2XKAYnoBZlpQ5p\nCum6A3XMK5g4qPZmpSWiSk9fwQkEs3LTyEhNpDDDKVFROUqmkFY3tpKT7qyPKM5OJc4jIfMEqsrb\ne0+yuDgTT4Q+Y5G8I1gA7FPVA6raDrwA3BbkvG8B3wVGxz2dGTMumjCORUUZvLT5aNCx5bf3OOWg\nffkBf/FeD//y0Uvo7FZumJXN4umZEW+vv6sDgsGx0y1sP97I/KnhTVudMjGZ5ATvoBPGvqvWEr8r\nU9/MoXCEkx+Ac6uLa9w8QVtnF+WH6nuen5ueRGKcZ9TcEZxoaCXXvSNIjPNSnJ0a8o5g14kmTp5p\nC/o5HC6RDAQXAUf8Hh91j/UQkSuAKar6h/5eSEQeEpFyESmvrT3/Wu3G+Nw5L58j9S1scOff+3t7\nby3F2alMnhC8VMPi6Zn88q8X8L275kS6mSHf/7n7y9hfe4bbfvBnurp1UOsH/Hk8wvScwW9Ss+tE\nIxOT43uuZsEJBFUNrWFNdQ0nPwB99y5+7/BpWju6WVSU0dOPgoxkDp6M/rUELe1dNLZ29gwNAZTm\npYdcS7DW3Z9iSQQvNkYsWSwiHuAZ4CsDnauqP1HVMlUty8qKXFQ0seeDl+SSmhjXZ4OQlvYu1h+s\nHzA5d+2MLCamJESyif1aMj2Ln366jMbWTjwC8wrCX8g2Myd10ENDO6uamJWb3mvVdFGWMywTzk5w\n4eQHwMkRwLnVxe/sr0MErpx6LpAUZqRwaBQMDVW7Ce1c/0AwOZ2aprags8He3nOSmTlp5KQH369h\nOEQyEBwD/LNn+e4xnzRgNvCmiFQCC4EVljA2F1JyQhy3XJrHym1VvWbOrD9YR3tnN9fMuLBDPkNx\n7YwsfvWZK3nqjstIC3MzHHBqDp080x70S8ifU1qisdewEPjNHApjeGjdgTquuHhw+QHoW3ju3QN1\nzJ48nvHJ5/o7dZRMIfUFAv8vdt/faeDwUEt7Fxsq6yN6NwCRDQQbgekiMlVEEoC7gRW+X6pqg6pm\nqmqhqhYC64BbVbU8gm0ypo87y/Jpbu/qVYfo7T0nSYjz9LrijGbzCycNedbSuU1q+r8rOFR3ltaO\nbmbl9V61XJCRQpxHBp0wbmjuCCs/AJAU7yUtKY7apjZa2rvYcvgUVxX1fn6BO4X0eMPI7/jVn2o3\nmPkPr4UKBBsq62nv7GZJBPMDEMFAoKqdwCPAa0AFsFxVd4jIEyJya6Te15hwlRVMpCAjudfw0Nt7\na7ly6iTGJQx+Bs5o5StHMVAgqHDLIAROYYz3erg4I3nQdwQbKn35gfDyGb5FZZsOnaKjS/sEAt8U\n0mivOVTtlpfIGX/ujmBSSgK56Ul98gRr99SSEOcJa23IUAzP6pcQVHUlsDLg2GMhzr0ukm0xJhQR\n4Y4r8nlm1R6O1Dv75+6rOcMnI7AuIBplpSWSkZIwYCDYdaIRr0d6hoL8FWcNfubQugN1bn5gQtjt\nrG1q4539J/F6pE9ifGqmk6s4ePIsVxdH75BedWMr4+K9pAUsPiydnN4TbH3W7j3JgsLIX5DYymJj\ngNuvcCa0vbz5WL/TRseqmblp7BogYVxR1ci0zBSS4vt+KRVnp3KornlQexv48gPBXqc/WWlJ1J5p\n490DdczJH99nFXdO2uiYQnrCXUMQWKa8JC+NfbVnaO3oApyAsbu6KeL5AbBAYAwA+ROTWVSUwYub\nj/DWnlpy05OYkdP3ynesmpGTxt7qpn4Trf6b0QQqykqls1sHHJZpaO5gZ1V4+QGfrNREjp9uYevR\nhj7DQuBMIS3MSKEyyoeGahrbgs4AKs0b32uDnbV7nbISSyJUVsKfBQJjXL41Ba/tOME1MzKHvLHM\naDQrN43m9i6OngqeaPWVlghMFPsMdubQUPMD4AwNtXV209WtLCoKfpVcmJk8bKuLu7qVFzcd5Zu/\n29GrtMX5qm5qDRoIfGU7fHmCtXtryUxN7EnmR5IFAmNcN8/OJSXB26vaaKzwlZrYFWKnrF1BVhT7\nK3IDwUAzh9YdqCNhCPkBODeFNMHrCbleojAjhcN1zYPeayIYVeVPO6v58PfX8tXfvM8v/lLZU9Dv\nfKmqs6p4fN9AUJCRQnKCl51VjXR3K3/ee5Il0yNXVsKfBQJjXMkJcdxyWR5ej7A4ipONkeCbORRq\nYZlvWmOoomepiXHkpicNWHzOyQ9MCDs/AOcCweX9PL8wM4X2rm6qhjiFtLyynrt+9C6feb6c9q5u\nnvnEHBK8nmHbB6CxpZO2zu6efZj9eT3CzFxnb4KdVY3UnW2/YOtYLBAY4+frHy7h1w8tZELyyK0W\nHgkpiXFMmTQuZM2hXSeamJgcH/QLzKc4O7XfctTnkx+Ac6uLg+UHfHqKz4VZauJsWycPPV/OnT96\nl8P1zTz58dm8/uVruP2KfBYVZ7CqonrIu7D5q27qu5jMX2leOhVVjbzlTli4ULOfLBAY42dCcgJl\nEZ6zHa1m5qSHnELq24Ogv7xJcXYq+/vZtnJjZXj1hQLNyEnls0umcvf8i0Oe41tLEO5G9k+urGBV\nRTV//8GZvPX313PvlQXEu6XHl5bkcKiueUgVVgP5tqgMNjQEztBbU2snvyk/QkleOtlpkSsr4c8C\ngTEGcBLGB06e7Zm+6NPVreyudmoM9acoK4Wz7V2cCLE5jC8/MHcI+QFwNuv5xi2lIb9EwZlCmhTv\n4VAYU0jf3F3D/6w/zENLpvHw9cV95uzfWJINwKqdNUNqt7+e8hIhvuB9exNU1jVzzQWsaGuBwBgD\nOAXrurqVpc+8xXNrD9DU2gE4Nf5bO7p7ZrWE0pMwrgn+Jbzu4NDzA4N1bgrp4ALB6eZ2vvbSVmbk\npPLlD8wIek7e+HFcetF4/lRx/nkCXyDITg8+xDYrNw3fTdeFmDbqY4HAGAPAdTOz+PF985g8fhzf\n/kMFV33nDb79+528UeFcCYeaMeRzbgpp3+GlhpYOdhwfen4gHE456sEFgsde2UHdmXae+cTcfgPU\n0pIcNh8+NWBhvlfeOxa0/z7VjW1MSI4P+V7JCXFMzUghMc5DWWH4lWSHygKBMQZwSm188JJclv/N\nVax45GpuLMnmF+9U8uTKipClJfxlpSaSlhQXNGG8fOOR88oPhKMwM4Uj9S0DTiH9w9YqVrx/nC/e\nOJ3ZF/VfDntpaTaq8Mau0MNDFVWNPPrCe3x/9b6Q51Q3toYcFvK5Y14+9y0siOidU6CI1hoyxoxO\nl+VP4Pt3X87Xbp7F8+8eIiHOM+AXk4i4CeNzV+Nd3cp3Vlbw3J8PsmR65pD2SwjX1AxnCunx0y1M\nmZQc9Jyaxlb+6bfbmDNlAn97XdGAr1mal87k8UmsrqgOWeX1B284AWBTkE2OfKobW3sVmwvm4euL\nB2zPcLNAYIwJafKEcSz70KxBn1+clcqb7tTHxtYOvvi/W3hzdy33X1XAP3+klDhv5AchCtwppIfq\nmoMGAlVl2cvbaG7v4nt3zRlUm0SEG0tyeHHTUVo7uvoExb3VTazcXsXk8Ukcb2jl2OkWLgqys111\nY1vPmo1oYkNDxphhU5SdSm1TG9uONnD7D9/hz3tP8u2Pzeabt82+IEEA/KqQhkgYLy8/whu7avja\nzbMGHO7yt7Q0h5aOLt7dX9fndz9Ys49x8V6euuMywFmYFqirW6k9E7zO0EizQGCMGTbFWc4X6x0/\neofapjaef3ABn1pYcEHbkJOeSFJ88CqkVQ0tfOv3FVw1LYMHFhWG9boLp00iJcHLqoDZQwdqz/C7\n949z38ICFhVlkJLgZVOQkhR1Z9ro6tYBh4ZGggUCY8yw8dUsmjJxHK88fHXI4nCRJCIh9y9+/JUd\ndHZ38907Lgu7hk9inJdrZ2axuqK6V5XWZ9fsJyHOw2eWTCPO6+HyiyeysbJvIKhudHcm62d19kix\nQGCMGTZTJiXz0uev4rcPX02hO0QzEgozUvpMIf3j9hO8vrOaLy2dwcUZwZPIA1lakkN1YxvbjzcA\nzvadv33vGH+1oKCnFlJZ4UR2n2ik0V2H4XMiyF7F0cICgTFmWM0rmERaUvzAJ0ZQ4BTSxtYOHl+x\nnZK8dB5cPHXIr3v9zGw8Qk8Ruh+u2Y/XI3zu2mk955QVTKJbYcvh072e61tM1t/K6JES0UAgIjeL\nyG4R2Sciy4L8/u9EZKeIbBWR1SJyYQcTjTFjUmFGcs8UUoCn/7ibmqY2nrr90p4aQkMxMcWpRbWq\nooajp5p5afNR7pk/pddV/tyLJ+D1SJ9ppDWNrXgEMlKir6BhxAKBiHiBZ4EPAaXAPSJSGnDaFqBM\nVS8DXgT+LVLtMcbEDt+wVGXdWTYdOsV/rz/EA4sKh7QPQqAPlORQUdXIY6/sQAQ+d23vdQipiXGU\n5KX1yROcaGwlKy3xgs2eCkckW7QA2KeqB1S1HXgBuM3/BFVdo6q+erHrgPwItscYEyN85aj3Vp/h\n6y9vIy89ia/cNHNYXntpaQ7grDK+q2wKk4OsFygrmMR7R0732sO5OsQWldEgkoHgIuCI3+Oj7rFQ\nHgReDfYLEXlIRMpFpLy2tnYYm2iMGYty0hMZF+/lP9/Yy+7qJp64bXafze6HampmCkVZKcR5hM9f\nG3xVclnhRFo6unq2nQQnR3ChykqHKyruUUTkU0AZ8HSw36vqT1S1TFXLsrJiawtBY0z4RISCjGRO\nNXdwy6V5PVfxw+XrHy7hyY/PDlnCoqzA2dPCf4vL6sZWcsdH39RRiGwgOAb4F+XId4/1IiJLgW8A\nt6pq/6X9jDFmkKbnpJGWFMfjHw1MTZ6/G0ty+GQ/G+Tkjk8if+K4nhXGbZ1dnGruGLDg3EiJZCDY\nCEwXkakikgDcDazwP0FELgd+jBMEzn/XB2OMcT3+0VJ+98hiskdoXH5+4STKD51CVanxLSaLwqmj\nEMFAoKqdwCPAa0AFsFxVd4jIEyJyq3va00Aq8BsReU9EVoR4OWOMCUtmauKILmqbVzCR2qY2Dtc3\nn9uZLEqTxRGtPqqqK4GVAcce8/t5aSTf3xhjRopvY5nyylM91UpzQuxMNtKiIllsjDFjzYxsJ0dR\nfqi+p7xEbizeERhjTKzyeIR5BRMprzxFelI8CXEexo8b2dIbodgdgTHGRMj8wknsrTnDrhNN5KQn\nIhJexdMLxQKBMcZEiG9rznf2n4zaYSGwQGCMMREzJ38C8V6ho0tHbBrrYFggMMaYCBmX4OWSyeOB\n6E0UgwUCY4yJqPnuNNJonToKFgiMMSai5rl1h6J1MRlYIDDGmIi6bmYWn10ylWtnRG/BTFtHYIwx\nEZQU7+Ubtwx/4bvhZHcExhgT4ywQGGNMjLNAYIwxMc4CgTHGxDgLBMYYE+MsEBhjTIyzQGCMMTHO\nAoExxsQ4UdWRbkNYRKQWODTEp2cCJ4exOdEqFvoZC32E2OhnLPQRRr6fBaoadHnzqAsE50NEylW1\nbKTbEWmx0M9Y6CPERj9joY8Q3f20oSFjjIlxFgiMMSbGxVog+MlIN+ACiYV+xkIfITb6GQt9hCju\nZ0zlCIwxxvQVa3cExhhjAlggMMaYGBczgUBEbhaR3SKyT0SWjXR7houI/FxEakRku9+xSSKySkT2\nun9OHMk2ni8RmSIia0Rkp4jsEJFH3eNjpp8ikiQiG0TkfbeP33SPTxWR9e7n9tcikjDSbT1fIuIV\nkS0i8nv38VjsY6WIbBOR90Sk3D0WtZ/XmAgEIuIFngU+BJQC94hIdG8ZNHj/BdwccGwZsFpVpwOr\n3cejWSfwFVUtBRYCD7v/fmOpn23ADao6B5gL3CwiC4HvAv+uqsXAKeDBEWzjcHkUqPB7PBb7CHC9\nqs71WzsQtZ/XmAgEwAJgn6oeUNV24AXgthFu07BQ1beB+oDDtwG/dH/+JfCxC9qoYaaqVaq62f25\nCedL5CLGUD/VccZ9GO/+p8ANwIvu8VHdRwARyQduAZ5zHwtjrI/9iNrPa6wEgouAI36Pj7rHxqoc\nVa1yfz4B5IxkY4aTiBQClwPrGWP9dIdM3gNqgFXAfuC0qna6p4yFz+1/AP8AdLuPMxh7fQQniL8u\nIptE5CH3WNR+Xm3z+jFOVVVExsQcYRFJBV4CvqSqjc7FpGMs9FNVu4C5IjIB+D9g1gg3aViJyEeA\nGlXdJCLXjXR7Imyxqh4TkWxglYjs8v9ltH1eY+WO4Bgwxe9xvntsrKoWkTwA98+aEW7PeROReJwg\n8CtVfdk9POb6CaCqp4E1wFXABBHxXbCN9s/t1cCtIlKJMzx7A/B9xlYfAVDVY+6fNThBfQFR/HmN\nlUCwEZjuzk5IAO4GVoxwmyJpBXC/+/P9wCsj2Jbz5o4j/wyoUNVn/H41ZvopIlnunQAiMg74AE4u\nZA1wp3vaqO6jqv6jquaraiHO/4NvqOq9jKE+AohIioik+X4GbgK2E8Wf15hZWSwiH8YZn/QCP1fV\nJ0e4ScNCRP4XuA6nxG018DjwW2A5cDFOye5PqGpgQnnUEJHFwFpgG+fGlr+OkycYE/0UkctwEohe\nnAu05ar6hIhMw7l6ngRsAT6lqm0j19Lh4Q4NfVVVPzLW+uj25//ch3HA/6jqkyKSQZR+XmMmEBhj\njAkuVoaGjDHGhGCBwBhjYpwFAmOMiXEWCIwxJsZZIDDGmBhngcBEFRFREfme3+Ovisi/DNNr/5eI\n3Dnwmef9PneJSIWIrAk4PllEXnR/nutOaR6u95wgIn8b7L2MGYgFAhNt2oDbRSRzpBviz2/l62A8\nCHxWVa/3P6iqx1XVF4jmAmEFggHaMAHoCQQB72VMvywQmGjTibO365cDfxF4RS8iZ9w/rxORt0Tk\nFRE5ICJPici9bn3/bSJS5PcyS0WkXET2uLVvfMXenhaRjSKyVUQ+5/e6a0VkBbAzSHvucV9/u4h8\n1z32GLAY+JmIPB1wfqF7bgLwBPBJt179J93VqD9327xFRG5zn/OAiKwQkTeA1SKSKiKrRWSz+96+\nKrpPAUXu6z3tey/3NZJE5Bfu+VtE5Hq/135ZRP4oTo38fwv7X8uMCVZ0zkSjZ4GtYX4xzQFKcEpy\nHwCeU9UF4mxi8wXgS+55hTh1X4qANSJSDHwaaFDV+SKSCPxFRF53z78CmK2qB/3fTEQm49TRn4dT\nQ/91EfmYuxr4BpxVs+XBGqqq7W7AKFPVR9zX+1eckgt/7Zaa2CAif/Jrw2WqWu/eFXzcLbqXCaxz\nA9Uyt51z3dcr9HvLh5231UtFZJbb1hnu7+biVHNtA3aLyH+qqn+lXhMD7I7ARB1VbQSeB74YxtM2\nuvsWtOGUb/Z9kW/D+fL3Wa6q3aq6FydgzMKpBfNpcUpAr8cpjTzdPX9DYBBwzQfeVNVat4Tyr4Br\nwmhvoJuAZW4b3gSScEoRAKzyK0UgwL+KyFbgTzglmwcqZ7wY+G8AVd2FU97AFwhWq2qDqrbi3PUU\nnEcfzChldwQmWv0HsBn4hd+xTtyLFxHxAP5bGvrXpun2e9xN7895YE0Vxfly/YKqvub/C7ceztmh\nNT9sAtyhqrsD2nBlQBvuBbKAearaIU4lz6TzeF//v7cu7DshJtkdgYlK7hXwcnpvW1iJMxQDcCvO\nLl7huktEPG7eYBqwG3gN+Lw4pa4RkRlu1cj+bACuFZFMcbZCvQd4K4x2NAFpfo9fA74g4myyICKX\nh3jeeJya/h3uWL/vCj7w9fytxQkguENCF+P02xjAAoGJbt/Dqarq81OcL9/3cWr1D+Vq/TDOl/ir\nwN+4QyLP4QyLbHYTrD9mgCtjd6epZTgllN8HNqlqOGWF1wClvmQx8C2cwLZVRHa4j4P5FVAmIttw\nchu73PbU4eQ2tgcmqYEfAh73Ob8GHhjN1T3N8LPqo8YYE+PsjsAYY2KcBQJjjIlxFgiMMSbGWSAw\nxpgYZ4HAGGNinAUCY4yJcRYIjDEmxv0/kl14y1zmLAIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrKD/B/DJA/h/LSTYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 23.20761729661128 seconds\n",
            "Best accuracy: 73.6  Best training loss: 0.096737802028656  Best validation loss: 0.8069479709863662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a9b42b60-653c-4b8d-a751-453d7e3e53a1",
        "id": "5GK3sU83P60Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
            "[1.5736654996871948, 1.2229846715927124, 1.0099021196365356, 1.2836661338806152, 1.0553721189498901, 0.8274673223495483, 0.8884718418121338, 0.6834949254989624, 0.5655368566513062, 0.8289435505867004, 0.8113844394683838, 0.962935209274292, 0.779599666595459, 0.7010205984115601, 0.8003870248794556, 0.8447260856628418, 0.7219758033752441, 0.5783211588859558, 0.7622759342193604, 0.5874494314193726, 0.8189527988433838, 0.6898026466369629, 0.4748799800872803, 0.4375569820404053, 0.32292935252189636, 0.4775329530239105, 0.4046761989593506, 0.48076844215393066, 1.1655406951904297, 0.5826317667961121, 0.43950164318084717, 0.4316883087158203, 0.3884623348712921, 0.5691026449203491, 0.5401845574378967, 0.2944376468658447, 0.2753751277923584, 0.5109671354293823, 0.24080008268356323, 0.27642711997032166, 0.44843170046806335, 0.7689778804779053, 0.3133493661880493, 0.4810526371002197, 0.18146547675132751, 0.24927732348442078, 0.31193625926971436, 0.3348938822746277, 0.19393660128116608, 0.281372606754303, 0.096737802028656, 0.5129836797714233, 0.2817838490009308, 0.43850353360176086]\n",
            "[1.3505789524316782, 1.2756081318855288, 1.1293651217222216, 1.1017858368158338, 1.0122067934274677, 0.9558628690242766, 0.9442830532789231, 0.9067271530628199, 0.9225094479322432, 0.8765052825212475, 0.8443277543783189, 0.830682610869408, 0.8958051493763929, 0.841389916539192, 0.863542918264866, 0.8069479709863662, 0.816176891326904, 0.8249038729071617, 0.8457656276226044, 0.8496766424179081, 0.8097293403744698, 0.8305334505438807, 0.8407803028821945, 0.8368529573082925, 0.8425731217861177, 0.8497952985763552, 0.8691542366147045, 0.8928529787063598, 0.8654727375507355, 0.8922225627303125, 0.8816053685545923, 0.8709767842292782, 0.8720705962181091, 0.8638370376825333, 0.8756174597144124, 0.9254961982369424, 0.9104362151026727, 0.9043465369939805, 0.9372741371393203, 0.905842404663563, 0.9484809464216231, 0.9912667256593707, 0.9555818766355514, 0.9812858098745344, 0.9535087245702747, 1.0319731768965716, 1.0068317282199857, 1.0203085121512412, 0.9996400517225265, 1.000793667137623, 1.0078365066647532, 1.0665358537435532, 1.0158764567971232, 1.0496315807104106]\n",
            "[53.5, 54.66, 59.9, 61.52, 65.22, 66.94, 67.72, 68.08, 67.84, 69.62, 70.32, 70.86, 68.94, 70.68, 70.86, 71.72, 72.44, 72.22, 71.3, 71.48, 73.02, 72.0, 71.7, 72.5, 72.2, 72.1, 71.84, 71.22, 72.18, 72.02, 72.4, 72.28, 72.78, 73.6, 72.72, 71.52, 72.18, 72.44, 72.06, 72.38, 71.64, 71.32, 72.34, 72.0, 71.76, 70.8, 71.9, 71.56, 72.36, 72.34, 72.34, 70.66, 71.66, 71.34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SChtbkLTb8eN"
      },
      "source": [
        "## squeeze skip residuals (batch normed) (2 extra layers depth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B8MHSPGhb8eP",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            #nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "        self.features4 = nn.Sequential(\n",
        "            Fire(384, 64, 256, 256),\n",
        "            #nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        \n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "        self.features5 = nn.Sequential(\n",
        "            Fire(512, 80, 320, 320),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block5 = nn.Sequential(\n",
        "            Fire(640, 80, 320, 320),\n",
        "        )\n",
        "\n",
        "        self.features6 = nn.Sequential(\n",
        "            Fire(640, 96, 384, 384),\n",
        "            #nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "       \n",
        "        self.block6 = nn.Sequential(\n",
        "            Fire(768, 96, 384, 384),\n",
        "        )\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(768, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "\n",
        "        x = self.features4(x)\n",
        "\n",
        "        residual4 = x\n",
        "        x = self.block4(x)\n",
        "        x += residual4\n",
        "\n",
        "        x = self.features5(x)\n",
        "\n",
        "        residual5 = x\n",
        "        x = self.block5(x)\n",
        "        x += residual5\n",
        "\n",
        "        x = self.features6(x)\n",
        "\n",
        "        residual6 = x\n",
        "        x = self.block6(x)\n",
        "        x += residual6\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mOPoc-Xyb8eV",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wq6ecoDzb8eb",
        "colab": {}
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0a8a8ee3-4cc3-4243-9294-6f7a097291b4",
        "id": "w7VbGqKmb8eg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.233442\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.980543\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.880717\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.795509\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.720093\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.698942\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.651946\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.593989\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.589501\n",
            "\n",
            "Test set: Test loss: 1.5121, Accuracy: 2353/5000 (47%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 47.06%\n",
            "Better loss at Epoch 0: loss = 1.5121425127983095%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.526160\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.440299\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.467156\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.424484\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.410444\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.382611\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.368049\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.327329\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.308235\n",
            "\n",
            "Test set: Test loss: 1.2825, Accuracy: 2774/5000 (55%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 55.48%\n",
            "Better loss at Epoch 1: loss = 1.2824513864517206%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.261136\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.215220\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.200007\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.247119\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.193689\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.217860\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.204509\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.194136\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.154150\n",
            "\n",
            "Test set: Test loss: 1.0977, Accuracy: 3110/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 62.2%\n",
            "Better loss at Epoch 2: loss = 1.0977454257011414%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.079484\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.094554\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.079300\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.087025\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.082613\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.055845\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.045841\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.032597\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.036150\n",
            "\n",
            "Test set: Test loss: 1.0038, Accuracy: 3233/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 64.66%\n",
            "Better loss at Epoch 3: loss = 1.003833923339844%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.970975\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.936210\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.963519\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.961690\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.975190\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.941360\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.964551\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.973434\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.924672\n",
            "\n",
            "Test set: Test loss: 0.9144, Accuracy: 3388/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 67.76%\n",
            "Better loss at Epoch 4: loss = 0.9143832659721375%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.851170\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.829195\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.885578\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.885829\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.877740\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.893503\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.868373\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.879691\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.854188\n",
            "\n",
            "Test set: Test loss: 0.9170, Accuracy: 3401/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 68.02%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.796181\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.768513\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.783704\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.788725\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.800416\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.774703\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.810096\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.767117\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.831089\n",
            "\n",
            "Test set: Test loss: 0.8439, Accuracy: 3519/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 70.38%\n",
            "Better loss at Epoch 6: loss = 0.8439497417211532%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.727096\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.686122\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.708198\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.749685\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.719526\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.716793\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.729337\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.732867\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.739960\n",
            "\n",
            "Test set: Test loss: 0.8162, Accuracy: 3588/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 71.76%\n",
            "Better loss at Epoch 7: loss = 0.8161922967433932%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.634708\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.649361\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.643511\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.670283\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.650579\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.665738\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.682762\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.653611\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.693294\n",
            "\n",
            "Test set: Test loss: 0.8001, Accuracy: 3604/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 72.08%\n",
            "Better loss at Epoch 8: loss = 0.8000517854094503%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.586219\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.582788\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.633765\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.593658\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.624751\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.631682\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.582375\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.650167\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.634323\n",
            "\n",
            "Test set: Test loss: 0.8133, Accuracy: 3660/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 73.2%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.523309\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.546936\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.563581\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.556511\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.565855\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.561440\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.591011\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.552595\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.569302\n",
            "\n",
            "Test set: Test loss: 0.8172, Accuracy: 3637/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.478988\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.466810\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.518102\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.497741\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.528482\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.519883\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.558089\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.541146\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.557912\n",
            "\n",
            "Test set: Test loss: 0.7936, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 74.06%\n",
            "Better loss at Epoch 11: loss = 0.7935634583234785%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.429986\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.434108\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.468512\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.457319\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.482231\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.488095\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.509478\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.503105\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.514699\n",
            "\n",
            "Test set: Test loss: 0.7874, Accuracy: 3720/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 12: accuracy = 74.4%\n",
            "Better loss at Epoch 12: loss = 0.7873917382955549%\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.390302\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.393896\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.419823\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.427553\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.476955\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.460474\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.461945\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.431832\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.487070\n",
            "\n",
            "Test set: Test loss: 0.7653, Accuracy: 3744/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 13: accuracy = 74.88%\n",
            "Better loss at Epoch 13: loss = 0.7652966991066933%\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.343768\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.348077\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.401557\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.415356\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.430182\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.393727\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.408728\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.412223\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.448282\n",
            "\n",
            "Test set: Test loss: 0.8369, Accuracy: 3655/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.327221\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.327582\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.346725\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.374036\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.375327\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.368730\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.397364\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.389758\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.390098\n",
            "\n",
            "Test set: Test loss: 0.8420, Accuracy: 3679/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.287804\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.317443\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.308331\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.323613\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.337962\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.352732\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.351274\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.355972\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.374609\n",
            "\n",
            "Test set: Test loss: 0.8259, Accuracy: 3734/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.264984\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.284165\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.304515\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.318657\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.322244\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.330255\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.328102\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.346036\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.353314\n",
            "\n",
            "Test set: Test loss: 0.8784, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.275099\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.243685\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.282959\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.287133\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.302170\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.294807\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.321824\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.315873\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.318532\n",
            "\n",
            "Test set: Test loss: 0.8992, Accuracy: 3678/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.246922\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.231226\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.252621\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.270142\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.290448\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.257687\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.283999\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.294802\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.307278\n",
            "\n",
            "Test set: Test loss: 0.8707, Accuracy: 3700/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.208915\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.234342\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.240241\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.233522\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.238485\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.230940\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.248624\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.279343\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.286369\n",
            "\n",
            "Test set: Test loss: 0.9633, Accuracy: 3644/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.215017\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.200391\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.207135\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.229311\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.219086\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.223744\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.236077\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.237493\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.244806\n",
            "\n",
            "Test set: Test loss: 0.9204, Accuracy: 3744/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.173618\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.174122\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.192459\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.216007\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.201386\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.208640\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.238226\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.222578\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.208341\n",
            "\n",
            "Test set: Test loss: 0.9276, Accuracy: 3715/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.164866\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.173167\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.177291\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.183513\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.179756\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.191338\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.198080\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.217785\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.207494\n",
            "\n",
            "Test set: Test loss: 0.9760, Accuracy: 3699/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.142637\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.153568\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.166465\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.195958\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.174666\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.184426\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.185517\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.181074\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.202317\n",
            "\n",
            "Test set: Test loss: 0.9592, Accuracy: 3744/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.159742\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.113240\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.162282\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.175872\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.165292\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.175499\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.165818\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.175334\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.176281\n",
            "\n",
            "Test set: Test loss: 0.9536, Accuracy: 3738/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.125130\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.113434\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.146475\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.146642\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.150848\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.158583\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.163974\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.183893\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.179516\n",
            "\n",
            "Test set: Test loss: 0.9751, Accuracy: 3770/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 26: accuracy = 75.4%\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.122661\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.101592\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.139431\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.136926\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.141353\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.148197\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.153650\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.156170\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.151982\n",
            "\n",
            "Test set: Test loss: 0.9697, Accuracy: 3763/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.121165\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.118496\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.111506\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.132093\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.117053\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.117933\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.159242\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.133180\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.153182\n",
            "\n",
            "Test set: Test loss: 1.0412, Accuracy: 3719/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.102007\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.097961\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.109390\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.117226\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.142399\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.123049\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.116189\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.128400\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.137805\n",
            "\n",
            "Test set: Test loss: 1.0750, Accuracy: 3676/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.124328\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.108085\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.098073\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.129875\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.114156\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.108115\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.107033\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.109953\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.122732\n",
            "\n",
            "Test set: Test loss: 1.0329, Accuracy: 3701/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.086905\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.095705\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.089965\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.096088\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.098858\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.127755\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.126180\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.126900\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.121520\n",
            "\n",
            "Test set: Test loss: 1.0148, Accuracy: 3726/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.097476\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.098608\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.096522\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.094121\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.113043\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.109078\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.118532\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.113778\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.121624\n",
            "\n",
            "Test set: Test loss: 1.0612, Accuracy: 3733/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.088524\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.080974\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.087009\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.084706\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.102407\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.093143\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.101457\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.110879\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.122413\n",
            "\n",
            "Test set: Test loss: 1.0628, Accuracy: 3737/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.067298\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.082857\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.079249\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.087349\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.082001\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.082108\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.082599\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.080722\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.096029\n",
            "\n",
            "Test set: Test loss: 1.0754, Accuracy: 3772/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 34: accuracy = 75.44%\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Calculating gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "64454a06-cf59-4ab7-b97d-fc4a3d74a2bb",
        "id": "P1nK0a4gb8ey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4tUcWbIgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 48.546672692571406 seconds\n",
            "Best accuracy: 75.44  Best training loss: 0.05418708920478821  Best validation loss: 0.7652966991066933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "da1a52e6-d759-46c2-a3b1-208bc337683d",
        "id": "8RRvVJe-b8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
            "[1.671850562095642, 1.4294451475143433, 1.2597286701202393, 1.1926536560058594, 0.8873090147972107, 0.826803982257843, 0.7074623107910156, 0.6133054494857788, 0.633284330368042, 0.5885295867919922, 0.5012684464454651, 0.39865317940711975, 0.5535438656806946, 0.4638020396232605, 0.485892653465271, 0.43394768238067627, 0.3143607974052429, 0.6553335785865784, 0.38519614934921265, 0.22057229280471802, 0.21904069185256958, 0.3447380065917969, 0.07020840048789978, 0.21228760480880737, 0.13398712873458862, 0.2944369614124298, 0.3080297112464905, 0.1039414182305336, 0.12129847705364227, 0.05418708920478821, 0.07100073993206024, 0.14000892639160156, 0.2532752454280853, 0.07283256202936172, 0.11819754540920258]\n",
            "[1.5121425127983095, 1.2824513864517206, 1.0977454257011414, 1.003833923339844, 0.9143832659721375, 0.916989808678627, 0.8439497417211532, 0.8161922967433932, 0.8000517854094503, 0.8132895508408544, 0.8171623694896698, 0.7935634583234785, 0.7873917382955549, 0.7652966991066933, 0.8368609210848806, 0.8419940316677094, 0.8259078162908555, 0.8784282550215723, 0.89924132078886, 0.8706793075799939, 0.9632957187294959, 0.9203883928060532, 0.9276180094480513, 0.9760384330153464, 0.9592139083147051, 0.9536105239391331, 0.9750947001576418, 0.9697353872656823, 1.0412206524610519, 1.0750351437926289, 1.0328764069080352, 1.0147591415047645, 1.0612324786186216, 1.0628420361876487, 1.075413476526737]\n",
            "[47.06, 55.48, 62.2, 64.66, 67.76, 68.02, 70.38, 71.76, 72.08, 73.2, 72.74, 74.06, 74.4, 74.88, 73.1, 73.58, 74.68, 73.82, 73.56, 74.0, 72.88, 74.88, 74.3, 73.98, 74.88, 74.76, 75.4, 75.26, 74.38, 73.52, 74.02, 74.52, 74.66, 74.74, 75.44]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HsUHYd6Cs9l4"
      },
      "source": [
        "## squeeze skip residuals (batch normed) (2 extra layers for residuals)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H2iSrsTqs9l6",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.block11 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.block22 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "        # self.block33 = nn.Sequential(\n",
        "        #     Fire(384, 48, 192, 192),\n",
        "        # )\n",
        "        self.features4 = nn.Sequential(\n",
        "            Fire(384, 64, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        \n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "        residual11 = x\n",
        "        x = self.block11(x)\n",
        "        x += residual11\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "        residual22 = x\n",
        "        x = self.block22(x)\n",
        "        x += residual22\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "        # residual33 = x\n",
        "        # x = self.block33(x)\n",
        "        # x += residual33\n",
        "\n",
        "        x = self.features4(x)\n",
        "\n",
        "        residual4 = x\n",
        "        x = self.block4(x)\n",
        "        x += residual4\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_6t6sqos9l-",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qbL-M692s9mH",
        "outputId": "90eaa777-9c67-4398-e1cb-58bb3ea0fdfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "582383a2-4b57-42d1-b5e6-339ca76cf54e",
        "id": "tv23eQtDs9mK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.189392\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 1.918506\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.831333\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.753702\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.701960\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.639620\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.572478\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.546740\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.503192\n",
            "\n",
            "Test set: Test loss: 1.4009, Accuracy: 2560/5000 (51%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 51.2%\n",
            "Better loss at Epoch 0: loss = 1.400896774530411%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.405455\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.387146\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.407003\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.336567\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.336836\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.314781\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.305289\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.248644\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.278999\n",
            "\n",
            "Test set: Test loss: 1.2604, Accuracy: 2795/5000 (56%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 55.9%\n",
            "Better loss at Epoch 1: loss = 1.2604336732625963%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.210243\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.164843\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.177062\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.173110\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.179558\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.142425\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.163474\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.109345\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.108561\n",
            "\n",
            "Test set: Test loss: 1.0765, Accuracy: 3111/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 62.22%\n",
            "Better loss at Epoch 2: loss = 1.0765493458509445%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.064272\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.038083\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.046079\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.046268\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.043906\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.078363\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 0.985089\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.011910\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.006350\n",
            "\n",
            "Test set: Test loss: 1.0102, Accuracy: 3249/5000 (65%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 64.98%\n",
            "Better loss at Epoch 3: loss = 1.0101558506488797%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.931400\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.925216\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.986841\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.915377\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.942444\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.926532\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.930923\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.916214\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.920906\n",
            "\n",
            "Test set: Test loss: 0.9544, Accuracy: 3327/5000 (67%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 66.54%\n",
            "Better loss at Epoch 4: loss = 0.9544164842367173%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.868572\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.851095\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.842521\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.857581\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.847401\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.866499\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.857850\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.856977\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.881260\n",
            "\n",
            "Test set: Test loss: 0.8836, Accuracy: 3425/5000 (68%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 68.5%\n",
            "Better loss at Epoch 5: loss = 0.8835893097519871%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.794967\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.791042\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.797164\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.798501\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.798620\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.776185\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.814918\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.794067\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.817454\n",
            "\n",
            "Test set: Test loss: 0.8216, Accuracy: 3517/5000 (70%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 70.34%\n",
            "Better loss at Epoch 6: loss = 0.8215996313095091%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.729213\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.699671\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.731742\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.743253\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.769828\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.747581\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.714700\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.744083\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.731792\n",
            "\n",
            "Test set: Test loss: 0.8554, Accuracy: 3490/5000 (70%)\n",
            "\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.689481\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.667187\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.692823\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.692265\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.674785\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.679162\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.709588\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.701068\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.704656\n",
            "\n",
            "Test set: Test loss: 0.8189, Accuracy: 3585/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 71.7%\n",
            "Better loss at Epoch 8: loss = 0.8189084324240686%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.625006\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.625405\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.607808\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.677522\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.636340\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.640174\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.654696\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.628683\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.672432\n",
            "\n",
            "Test set: Test loss: 0.7762, Accuracy: 3657/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 73.14%\n",
            "Better loss at Epoch 9: loss = 0.7761981755495069%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.571073\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.590741\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.584625\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.609196\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.595504\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.603407\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.603879\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.608566\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.648390\n",
            "\n",
            "Test set: Test loss: 0.7732, Accuracy: 3671/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 73.42%\n",
            "Better loss at Epoch 10: loss = 0.7732098037004468%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.541686\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.536970\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.546914\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.569771\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.581610\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.548151\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.587405\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.586904\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.596743\n",
            "\n",
            "Test set: Test loss: 0.7639, Accuracy: 3677/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 73.54%\n",
            "Better loss at Epoch 11: loss = 0.7638533076643943%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.517022\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.524588\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.508013\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.531623\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.499581\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.544527\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.548167\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.546349\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.540549\n",
            "\n",
            "Test set: Test loss: 0.8019, Accuracy: 3651/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.469461\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.454727\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.477905\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.479839\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.491906\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.495823\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.564407\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.540537\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.508962\n",
            "\n",
            "Test set: Test loss: 0.8062, Accuracy: 3633/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.444286\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.435580\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.446053\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.473579\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.458286\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.489490\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.466606\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.475845\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.525409\n",
            "\n",
            "Test set: Test loss: 0.7741, Accuracy: 3693/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 14: accuracy = 73.86%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.402084\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.365268\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.402851\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.433403\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.462525\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.477539\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.452066\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.465099\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.469477\n",
            "\n",
            "Test set: Test loss: 0.8269, Accuracy: 3675/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.398515\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.362922\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.393423\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.398234\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.400306\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.403713\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.445150\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.459022\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.424293\n",
            "\n",
            "Test set: Test loss: 0.7982, Accuracy: 3679/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.353691\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.357914\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.379864\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.405580\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.407054\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.392724\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.410380\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.398992\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.429729\n",
            "\n",
            "Test set: Test loss: 0.8271, Accuracy: 3698/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 73.96%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.367695\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.357050\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.355410\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.362042\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.358455\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.392828\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.385593\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.394456\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.370300\n",
            "\n",
            "Test set: Test loss: 0.8501, Accuracy: 3697/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.317422\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.329886\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.319005\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.322078\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.351634\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.366389\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.375720\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.392608\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.350381\n",
            "\n",
            "Test set: Test loss: 0.8392, Accuracy: 3692/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.295161\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.296179\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.301279\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.326688\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.333761\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.332327\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.335246\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.376271\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.343525\n",
            "\n",
            "Test set: Test loss: 0.8444, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 20: accuracy = 74.06%\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.289686\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.264561\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.292696\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.308823\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.307634\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.327585\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.322442\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.309881\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.354370\n",
            "\n",
            "Test set: Test loss: 0.8959, Accuracy: 3653/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.274141\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.278100\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.274658\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.285669\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.307150\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.302308\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.307063\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.312435\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.310554\n",
            "\n",
            "Test set: Test loss: 0.8423, Accuracy: 3768/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 22: accuracy = 75.36%\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.231868\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.238877\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.251350\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.285083\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.277534\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.278720\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.306107\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.299897\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.291596\n",
            "\n",
            "Test set: Test loss: 0.8912, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.242686\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.225341\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.234263\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.271907\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.268025\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.273155\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.270627\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.279308\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.277275\n",
            "\n",
            "Test set: Test loss: 0.9301, Accuracy: 3676/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.236692\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.233038\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.246623\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.256289\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.242344\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.257729\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.260123\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.257460\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.275544\n",
            "\n",
            "Test set: Test loss: 0.9105, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.230532\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.224396\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.228242\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.219487\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.225482\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.254153\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.260614\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.244556\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.243414\n",
            "\n",
            "Test set: Test loss: 0.9326, Accuracy: 3699/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.193505\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.194978\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.209027\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.216250\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.223303\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.236956\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.241933\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.240112\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.262639\n",
            "\n",
            "Test set: Test loss: 0.9349, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.185015\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.175858\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.198115\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.198522\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.213595\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.248388\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.228262\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.233868\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.213596\n",
            "\n",
            "Test set: Test loss: 0.9321, Accuracy: 3687/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.180718\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.183577\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.206352\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.192888\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.185004\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.230019\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.219838\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.215450\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.230864\n",
            "\n",
            "Test set: Test loss: 0.9609, Accuracy: 3717/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.188880\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.153218\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.177480\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.184605\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.197458\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.194234\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.199724\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.205602\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.219362\n",
            "\n",
            "Test set: Test loss: 0.9416, Accuracy: 3741/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.167875\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.154349\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.153565\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.186588\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.171101\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.193406\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.195176\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.196376\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.190116\n",
            "\n",
            "Test set: Test loss: 0.9995, Accuracy: 3727/5000 (75%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.136519\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.151555\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.165249\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.157740\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.163676\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.202552\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.210600\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.186240\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.194233\n",
            "\n",
            "Test set: Test loss: 1.0116, Accuracy: 3699/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.132422\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.136779\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.143403\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.149288\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.171053\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.171193\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.174689\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.189095\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.177193\n",
            "\n",
            "Test set: Test loss: 1.0398, Accuracy: 3667/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.157185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Calculating gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "251ee708-8bf4-4908-c170-119784ac4016",
        "id": "eTjxfzv-s9mN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Original)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRjnV87tlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd7hU1fX/8fcCQQSRoogIImJBxCgq\navCHisaYiL3EmlhisERNLCT6aOw9Suw/VOxIVIJoMLFCjFzUqEi5EARRRAGRIiBNKfeu7x/7TO54\nvWVumTkzcz6v55lnZs6csuaga/Zde599zN0REZHkaBJ3ACIikltK/CIiCaPELyKSMEr8IiIJo8Qv\nIpIwSvwiIgmjxC+SQ2b2tpnt0cB9dDWzVWbWtDHXzWBfT5jZTdHr3czsnYbuU+KhxC8NYmb9zOwd\nM/vGzJZGiW3vuOPKNTP7t5n9ppZ1jgRWuvuktGW7mNno6PytNLM3zWy/mvbj7l+4+6buXlZbXHVZ\nty7cvRRYHn0nKTBK/FJvZrYZ8A/gPqA90Bm4HlgbZ1x57DxgWOqNmW0PvA1MBbYDtgZeAF43s75V\n7cDMNspBnJkaDpwbdxBSD+6uhx71egB9gOU1fN4UuBNYAswGLgAc2Cj6fA5wSNr61wFPp73/MfAO\nsByYAvRP+6wN8CiwAJgP3AQ0jT6bAqxKe3hq21r2+W/gRkIyXgm8DmxRWzzAzUAZ8F10vPurOBfN\ngW+BLmnLhgEvV7HuEGBc9LpbFP/ZwBfAuLRlqfO4XbR8JTAGeCB1HqtYt7bv+DfgK+CbaJ+90j57\nArgp7X3n6DttHPd/i3rU7aEWvzTEx0CZmT1pZoeZWbtKnw8EjgD2IPxInJDpjs2sM/BPQkJvDwwC\nnjezDtEqTwAbgB2i/R8K/AbA3Xf3UN7YFLgUmAlMzGCfAKcCZwFbEpL1oNricfergBLgwui4F1bx\nlXYEyt19XtqynxISbWUjgP9nZpukLTsQ6An8rIr1/wq8D2xO+PH8VRXrpKvyO0ZeiWLdEphIaNVX\nyd3nA+uBHrUcT/KMEr/Um7uvAPoRWpRDgcVRvbpjtMqJwN3uPtfdlwK31mH3vyS0hl9293J3fwOY\nAAyI9j8AuNjdV7v7IuAu4OT0HZhZP0KiPiqKtdp9pm32uLt/7O7fEhJw79riyfD7tCW0sNNtQfiL\npbIFhP8326ctuy76rt9W+o5dgb2Ba9x9nbuPB0bXEkt13xF3f8zdV7r7WsKPyO5m1qaGfa2MvpsU\nECV+aRB3/8jdz3T3LsCuhDr13dHHWwNz01b/vA673hb4hZktTz0IPzKdos+aAQvSPnuI0EoFwMy2\nISS1M9z94wz2mfJV2us1wKZ12LYmy4DWlZYtqWb7TkB5tE3K3CrWg3COl7r7mgzWTanyO5pZUzO7\nzcw+NbMVhFIchB+o6rQmlL6kgORTR5EUOHefYWZPUNHhtwDYJm2VrpU2WQ20THu/VdrrucAwdx9Y\n+Thm1onQgbyFu2+o4vNNgBcJf228ksk+M1DbtrVNc/tJCM06RyUSCPX4XwCPV1r3ROBdd19jZrXt\nfwHQ3sxapiX/bapZtzanAkcDhxCSfhvCj49VtXJU/mpOKKVJAVGLX+rNzHY2s8vMrEv0fhvgFOA/\n0SojgN+ZWZeo/n9FpV1MBk42s2ZmVrkP4GngSDP7WdQSbWFm/c2si7svIHRKDjazzcysiZltb2YH\nRts+Bsxw9z9XOl61+8zg69a27UKge3Ubu/s6QqI/MG3x9cB+ZnazmbU3s9ZmdhFwOnB5BjHh7p8T\nSk7XmVnzaDRQfYdYtib8oH5N+EG+pZb1DwT+FZWFpIAo8UtDrAT2Bd4zs9WEhD8NuCz6fCjwGmEE\nzERgVKXtrwa2J7Qqryd0UgLg7nMJrc8rgcWEFvcfqPhv9nRCa3N6tP1IKsomJwPHRhcupR77Z7DP\namWw7T3ACWa2zMzurWY3D5HW8eruswjlot0JLewFwPHAz9z97dpiSnMa0JeQsG8CnqN+Q2qfIpTj\n5hPO639qXp3TgAfrcRyJmbnrRiySG2bWDfgMaFZViSYJzOxtwuifSbWuXP9jPEf4i+faLB5jN+Ah\nd6/yegPJb0r8kjNK/NkRXSm9lHBuDyX0b/TN5o+LFDZ17ooUvq0IZbTNgXnA+Ur6UhO1+EVEEkad\nuyIiCVMQpZ4tttjCu3XrFncYIiIF5cMPP1zi7h0qLy+IxN+tWzcmTJgQdxgiIgXFzKq8Wl6lHhGR\nhFHiFxFJGCV+EZGEUeIXEUkYJX4RkYRR4hcRSRglfhGRhFHiFxGYOhX+9a+4o5AcydoFXGbWgzAv\neEp34BrC/TkHEuY0B7jS3V/OVhwiUov16+Goo2DePBg/HvbdN+6IJMuy1uJ395nu3tvdewN7Ee7t\n+UL08V2pz5T0RWL29NMwZw60bAknnwzLdQvdYperUs9PgE+j28SJSL7YsAFuuQX23BNeeQXmzoVz\nzwXN2lvUcpX4TwaeSXt/oZmVmtlj0b1Yf8DMzjGzCWY2YfHixVWtIiIN9eyz8MkncPXVsN9+cOON\nMGIEPPpo3JHlp3XroKws7igaLOvz8ZtZc+BLoJe7LzSzjsASwIEbgU7u/uua9tGnTx/XJG0ijays\nDHbdFZo1g8mToUkTKC+HQw+Fd96BCRNgl13ijjI/zJgB998PTz4JZrD//tC/Pxx4YPhraaP8nO/S\nzD509z6Vl+ci2sOAie6+ECD1HAU1FPhHDmIQkcpGjgwJbcSIkPQhPA8bBrvvDiedBO+/D5tsEm+c\ncSkrg5dfhvvugzfegObNwzlp1Qr+/e/wGUDr1tCvX/gR6N8//BA0axZn5LXKRannFNLKPGbWKe2z\nY4FpOYhBRNKVl8NNN0HPnnD88d//rFMneOopmDYNLrssnvjitGwZDB4MO+0URjtNnx7O1dy54bwM\nGQIffQQLFsBzz8Evfwmffw5XXAE//jG0bw8nnhj+ispX7p61B9AK+Bpok7ZsGDAVKAVGE0o9Ne5n\nr732chFpRM8/7w7uTz9d/TqDBoV1nn8+d3HFaepU93POcW/ZMnzv/fd3HzHCfd26zLb/6quw/vnn\nu7dpE/Zx9NHuEydmN+4aABO8ipxaEPfcVY1fpBG5w157wapVoTVbXX163bpQwpg1K7Ret902t3Hm\nyscfh87tESOgRQs47TS48ELo3bv++1y+HO69F+66K7w+8ki49tpw3nOouhq/rtwVSZp//hMmTYIr\nr6y5U7J5c3jmmVDrPvXUMPSzmHz5JZx3XujA/sc/4KqrwkVsjzzSsKQP0LYtXHNNuD7ihhvChXF9\n+oQfgDxoxKrFL9KYysrgu+9CQm3WrKLTtLEsXgwTJ4bEPXEifPppqMOfempm27uHK3OXLIGZMzPr\nhHzmmbD/q64Kte6G2LAhDCG9/35YsybUw9u3h803r3id/ujcGbp1Cz9CjWXZMrj99tAi37AhXLfw\npz9Bx46Nd4zKVqwIncR/+QssXQoDBoS/APbZJ3vHpPoWvxK/FLf160OpomfPMAwvGzZsCKM8RoyA\nUaPg668rPjMLyTX1Q5B6btkSttwyJJvUc+XXLVqEOXQmTqx4zJtXse/u3WHjjUNH4223wR//WPt3\nfO01+PnP4eGHYeDAzL/j2WfD44/DmDFw8MF1Oj0ArF0bhkLefjvMng29esGOO4YkmHp8/XVYr7Im\nTUKZaccdYYcdwiP1ervtwjnIxJo1Ifnedht88034MbvhhnAec2XlyvCjN3hw+L4nnRTOSZbKaEr8\nkjzr1sFxx4XSxjbbwDHHhPf9+jV83HVZGYwbF0Z1PP98aEFvumkYBbL77uHz9evDj0JVz6tWwaJF\n4bFwYWjJl5dXfSwz6NEjDBNMPXr3hnbtQqI888zQir7gArjnHmjatOr9uIfvPnduuGirLq3o1atD\nqeKrr8I5POSQ8ANQWyt59erwI3PnnaG0ss8+4S+HI4744V9D7vDtt9//IUjFOmtWxfM331Rs06RJ\nGIVU3V8MqeULFoQrlL/8Eg4/HG6+Ofw7xWXlynBO7rgj/LtfdlkYFdS6daMeprrEn9VRPY310Kge\nqbMNG9xPOimMrLjkkjC6okWL8H6LLdzPPtv9n/90/+67uu3zrbfcf/tb944dw75atgzHGTXKfc2a\nhsW7cGEYWTJmjPvw4e4PPeQ+frz7ypU1b1tWVjEC59hjq49j7NiwzgMP1C/GGTPcjz/evV27sB9w\n320390svdX/5ZfdVqyrWXbbM/cYb3TffPKx30EHhe5WX1+/YKeXl7kuWuL/7rvuwYe7XXut+1lnh\n33f//d179XLv1Mm9efOKGFOP/fZzHzeuYcdvbF984X7aaSG+rbZyf/TR8N9CI0GjeiTn3EOrbf78\nise8eWHZgAFw2GHZKb+4h7rt0KHw5z/DH/4Qlq9aBa++Ci+8EDrzVqwILazDDw/xwPdLD6lWZ+r1\nokWhpbbJJmGbk04K27Vs2fjfoT7uvRcuvhj69oXRo0NrN13//qHF/OmnoYxUX2Vloew0Zkx4jB8f\n/rpq1ixM+9CjR/gLZMWK0LK/8soQUy65h9JO6t+uvDz8lZStcl9DvfceXHIJvPtuiPOuu8K/VwOp\nxS/ZU1bmPmmS+113uZ96amh5de/uvvHGP2x1mVWMk951V/cnn3Rfu7bxYikvr2j9Xnll9et9911o\npQ4c6N6hww9jbNfOffvt3ffe2/3nPw/f68IL3Z99tvYWeJxGjgznvUcP988+q1j+1lvhu919d+Mf\nc/Vq99dfd//jH9333NN9o43cTzzRffLkxj9WMSsvd3/mGfeuXSv+evvkkwbtErX4pdGUlcGUKaFD\n8623Qq07NZVv165hFEaXLmFERuqRer/VVmG9Z58NrfFp08Jnl1wSOhsbWuO8+eYwQuOCC0JHXiYt\nvLKyMJ69RYvQSm7Tpvo6eSEoKYGjjw6dni+/DHvsAT/9aegonj07+3+huOdvy7oQfPttGP1z663h\nL6lRo8JfTvWgFr80zKJF7nfe6X7kkRVXJYL7DjuEevmwYaFeWRfl5aHV3b9/2FfbtqGVvmBB/WK8\n776wn1/9KvwVkmTTp4eW46abut98czgvd9wRd1RSF19+6X7BBaG/pJ5Qi1/qbc2aULudMiUMo+vf\nv2Jmws6dG+cY770XRjiMGhVGm5xxRqjT77FHZq3Hp54K2xxzDPztb3k7W2JOffll6IOYMiX8JTNn\nThh5JImh4ZxSP+5w+ukwfDi89FLo1MymWbPCGOcnnghDFbt1g2OPDUMI+/atugTzwgtwwglw0EGh\n07YhHZfFZsUK+N3vwlTLmV7kJUVDiV/q54EHwrwl118fLkHPlSVL4O9/D38BjBkTap0dO4YW/bHH\nhiTfvHmYLveII8LY9jfeUItWJI0Sv9TdO++Ecs7PfhaGBzb29AOZWrEidFKOGhWeV68OHbCHHRbi\n2mGH0NHcrsqbuYkklhK/1M3ChaEV3aJFmFQqX5Lqt9+Glv0LL4Sk36FDGFmUzXlWRApUnHfgkkKz\nYUO4OGnZsnBBSb4kfQgXTx11VHhs2BD6IPL8bkci+UaJX37oiitCKzp1C758pZE7IvWi+fjl+0aM\nCKNqLrgg3FJORIqOEr9UmD4dfv3rMGzyL3+JOxoRyRIlfglWrAhj5Vu1ChdANeaNL0Qkr6hIKqGD\n9KyzwnznY8c23tW4IpKXlPgl3BBi1KhQ2z/wwLijEZEsU6kn6T75JMxmedxxYYZMESl6SvxJd8kl\noZ6f6RTGIlLwVOpJsn/8IzzuuAO23jruaEQkR9TiT6rvvgu36dt55zB7o4gkhlr8STV4cLj36uuv\na+imSMKoxZ9EX3wRblF4/PHhlnwikihK/El02WXhefDgeOMQkVgo8SfN2LEwciRceSVsu23c0YhI\nDJT4k2T9erjoIujeHQYNijsaEYmJOneT5L774KOPwg1MdF9akcRSi79QrV8PzzwDixdntv6CBXDd\ndTBgQLhHrYgkVtYSv5n1MLPJaY8VZnaxmbU3szfMbFb0nEe3dyogt90Gp54a6vQXXwzz5tW8/uWX\nw9q1cPfdukJXJOGylvjdfaa793b33sBewBrgBeAKYKy77wiMjd5LXXz6aRiOOWAAnHgi3H9/qNsP\nHBjm3qls/PhwN61Bg2DHHXMfr4jklVyVen4CfOrunwNHA09Gy58EjslRDMXBHS68MFx0NXQoPPFE\nSPYDB4bk3qMHnHYaTJsW1i8rC+t36RJG8ohI4uUq8Z8MPBO97ujuC6LXXwEdq9rAzM4xswlmNmFx\npnXsJBg1Cl59FW64oWJ+nW7d4IEH4LPPwhj90aPhRz+CY44JJZ4pU8IdtVq1ijV0EckP5u7ZPYBZ\nc+BLoJe7LzSz5e7eNu3zZe5eY52/T58+PmHChKzGWRBWroSePaFDB/jgg+pvNr50aRjBc889sGwZ\nHHwwjBmj2r5IwpjZh+7ep/LyXLT4DwMmuvvC6P1CM+sUBdUJWJSDGIrDddfBl1/CkCHVJ32A9u3h\n2mvh88/hkUfgqaeU9EXkf3KR+E+hoswDMBo4I3p9BvD3HMRQ+EpLQwt+4ED48Y8z26Z1azj7bN1K\nUUS+J6uJ38xaAT8FRqUtvg34qZnNAg6J3ktNysvhvPNCS/7WW+OORkQKXFav3HX31cDmlZZ9TRjl\nI5l6/HF4990wgqd9+7ijEZECpyt3892SJfDHP8L++8Ppp8cdjYgUASX+fHf55bBiRejQVQetiDQC\nJf589vbb8NhjcOml0KtX3NGISJFQ4s9X69eHDt2uXeGaa+KORkSKiKZlzlf33BOmXXjxRV1xKyKN\nSi3+fDR3brhY64gj4Kij4o5GRIqMEn8++sMfwtj9++5Th66INDol/nxTUgLPPReGcHbrFnc0IlKE\nlPjzSXl5uKlKly4h8YuIZIE6d/PJE0/AxIkwfDi0bBl3NCJSpNTizxcrVoQbpey3H5xyStzRiEgR\nU4s/X9x8MyxcCC+9pA5dEckqtfjzwaefhpugn3EG7L133NGISJFT4s8HgwZBs2Zwyy1xRyIiCaBS\nT9zGjg1X595yS8U9dEVEskgt/jht2BCGb263HVxySdzRiEhCqMUfp6FDw3w8I0dCixZxRyMiCaEW\nf1yWLYOrr4YDD4Tjjos7GhFJECX+uNxwQ0j+d9+t4ZsiklNK/HGYMQPuvx9+8xvo3TvuaEQkYZT4\n43DppWFKhhtvjDsSEUkgde7m2iuvhMedd8KWW8YdjYgkkFr8ubR8OZx/Puy0E1x0UdzRiEhCqcWf\nK+7hHrrz58P48dC8edwRiUhCKfHnyhNPhBus3HIL7Ltv3NGISIKp1JMLH38cSjsHHaQbrIhI7JT4\ns23t2jC/fosWMGwYNG0ad0QiknAq9WTbVVeFu2q9+CJ07hx3NCIiavFn1WuvweDB8NvfwtFHxx2N\niAiQQeI3s4vMrF0ugikqixaFG6vsumsYsy8ikicyafF3BD4wsxFm9nMzTSxTq/JyOPNM+OYbeOYZ\n2GSTuCMSEfmfWhO/u/8J2BF4FDgTmGVmt5jZ9lmOrXDde2+4Onfw4NDiFxHJIxnV+N3dga+ixwag\nHTDSzP5c03Zm1tbMRprZDDP7yMz6mtl1ZjbfzCZHjwEN/hb5ZNIkuPzyUNM///y4oxER+YFaR/WY\n2e+B04ElwCPAH9x9vZk1AWYBNQ1Mvwd41d1PMLPmQEvgZ8Bd7l58he/Vq8PQzS22gEce0XTLIpKX\nMhnO2R44zt0/T1/o7uVmdkR1G5lZG+AAQnkId18HrCvqLoJLLw0Xa40dG5K/iEgeyqTU8wqwNPXG\nzDYzs30B3P2jGrbbDlgMPG5mk8zsETNrFX12oZmVmtlj1Y0YMrNzzGyCmU1YvHhxZt8mTp9/Hm6l\n+Pvfhyt0RUTyVCaJfwiwKu39qmhZbTYC9gSGuPsewGrgimjb7YHewAJgcFUbu/vD7t7H3ft06NAh\ng8PFbOjQUNrRTdNFJM9lkvgt6twFQomHzEpE84B57v5e9H4ksKe7L3T3smg/Q4F96hp03lm3LtT0\nDz8cunaNOxoRkRplkvhnm9nvzKxZ9Pg9MLu2jdz9K2CumfWIFv0EmG5mndJWOxaYVueo882LL8LC\nhRrFIyIFIZOW+3nAvcCfAAfGAudkuP+LgOHRiJ7ZwFnAvWbWO9rXHODcOsacfx58ELp1g0MPjTsS\nEZFa1Zr43X0RcHJ9du7uk4E+lRb/qj77ylszZsCbb8Ktt2rmTREpCJmM428BnA30Alqklrv7r7MY\nV+F46CFo1gx+rdMhIoUhkxr/MGArwoVXbwFdgJXZDKpgrFkT7qx1/PG6cbqIFIxMEv8O7n41sNrd\nnwQOB3TvQIARI8IN1M87L+5IREQylkniXx89LzezXYE2gJq3AEOGwC67wAEHxB2JiEjGMhnV83B0\nde2fgNHApsDVWY2qEEycCO+/H2biLOZpKESk6NSY+KOJ2Fa4+zJgHNA9J1EVggcfhJYt4VfFNUhJ\nRIpfjaWe6OrammbfTKZvvoHhw8NMnG3bxh2NiEidZFLjH2Nmg8xsGzNrn3pkPbJ89vTTYUSPOnVF\npABlUuM/KXq+IG2Zk9Syj3vo1O3TJzxERApMJlfubpeLQArG22/Df/8bJmUTESlAmVy5e3pVy939\nqcYPpwAMGQJt2sDJ9ZrFQkQkdpmUevZOe92CMMvmRCB5iX/xYhg5Es49F1q1qn19EZE8lEmp56L0\n92bWFng2axHls8cfD3Pvq1NXRApYJqN6KltNuK1ispSXhwnZDjggXK0rIlKgMqnxv0QYxQPhh2IX\nYEQ2g8pLb7wBs2fDzTfHHYmISINkUuO/M+31BuBzd5+XpXjy14MPQocOcNxxcUciItIgmST+L4AF\n7v4dgJltYmbd3H1OViPLJ4sWwUsvwaBB0Lx53NGIiDRIJjX+vwHlae/LomXJ8eGHUFYGAwbEHYmI\nSINlkvg3cvd1qTfR62Q1e6dODc8/+lG8cYiINIJMEv9iMzsq9cbMjgaWZC+kPFRaCl26QLt2cUci\nItJgmdT4zwOGm9n90ft5QJVX8xat0lLYbbe4oxARaRSZXMD1KfBjM9s0er8q61Hlk3XrYMYMOPzw\nuCMREWkUtZZ6zOwWM2vr7qvcfZWZtTOzm3IRXF6YORPWr1d9X0SKRiY1/sPcfXnqTXQ3ruQMbykt\nDc8q9YhIkcgk8Tc1s41Tb8xsE2DjGtYvLlOnQrNm0KNH3JGIiDSKTDp3hwNjzexxwIAzgSezGVRe\nKS2Fnj1D8hcRKQKZdO7ebmZTgEMIc/a8Bmyb7cDyRmkpHHRQ3FGIiDSaTGfnXEhI+r8ADgY+ylpE\n+WTpUpg/X/V9ESkq1bb4zWwn4JTosQR4DjB3T07zV1fsikgRqqnUMwMoAY5w908AzOySnESVLzSi\nR0SKUE2lnuOABcCbZjbUzH5C6NxNjqlTYfPNoVOnuCMREWk01SZ+d3/R3U8GdgbeBC4GtjSzIWZ2\naK4CjFVpaSjzWLJ+70SkuNXauevuq939r+5+JNAFmARcnsnOzaytmY00sxlm9pGZ9TWz9mb2hpnN\nip7zc+az8nKYNk1lHhEpOnW65667L3P3h939Jxlucg/wqrvvDOxOGA10BTDW3XcExkbv889nn8Hq\n1Ur8IlJ06nOz9YyYWRvgAOBRCPP4R1M/HE3FBWBPAsdkK4YGSXXsakSPiBSZrCV+YDtgMfC4mU0y\ns0fMrBXQ0d0XROt8BXSsamMzO8fMJpjZhMWLF2cxzGqUlobafq9euT+2iEgWZTPxbwTsCQxx9z2A\n1VQq67i7Ey4M+4GopNTH3ft06NAhi2FWY+pU2GEHaNUq98cWEcmibCb+ecA8d38vej+S8EOw0Mw6\nAUTPi7IYQ/2lRvSIiBSZrCV+d/8KmGtmqWktfwJMB0YDZ0TLzgD+nq0Y6m31avjkE3XsikhRymR2\nzoa4iHDbxubAbOAswo/NCDM7G/gcODHLMdTd9OngrsQvIkUpq4nf3ScDfar4KNPhoPHQiB4RKWLZ\nrPEXrtJSaNkSunePOxIRkUanxF+VqVNDa7+JTo+IFB9ltsrcNaJHRIqaEn9lCxbA11+rY1dEipYS\nf2Wpm68o8YtIkVLir0wjekSkyCnxV1ZaCp07Q/v2cUciIpIVSvyVTZ2qMo+IFDUl/nTr14erdlXm\nEZEipsSfbubMkPzV4heRIqbEn04jekQkAZT405WWwkYbQY8eta8rIlKglPjTlZZCz57QvHnckYiI\nZI0SfzqN6BGRBFDiT1m2DObO1YgeESl6Svwp6tgVkYRQ4k9R4heRhFDiTykthXbtYOut445ERCSr\nlPhTUh27ZnFHIiKSVUr8AOXlGtEjIomhxA8wZw6sWqURPSKSCEr8oI5dEUkUJX4IHbtm0KtX3JGI\niGSdEj+ExN+9O2y6adyRiIhknRI/qGNXRBJFiX/NGpg1S4lfRBJDiX/69DCcUyN6RCQhlPgnTQrP\navGLSEIo8ZeUwJZbwg47xB2JiEhOKPGXlMD++2uqBhFJjGQn/rlzw1W7++8fdyQiIjmT7MRfUhKe\nDzgg3jhERHIoq4nfzOaY2VQzm2xmE6Jl15nZ/GjZZDMbkM0YalRSApttpo5dEUmUjXJwjIPcfUml\nZXe5+505OHbNxo2D/faDpk3jjkREJGeSW+pZsiSM4VeZR0QSJtuJ34HXzexDMzsnbfmFZlZqZo+Z\nWbuqNjSzc8xsgplNWLx4ceNHNn58eFbHrogkTLYTfz933xM4DLjAzA4AhgDbA72BBcDgqjZ094fd\nvY+79+nQoUPjR1ZSAhtvDHvv3fj7FhHJY1lN/O4+P3peBLwA7OPuC929zN3LgaHAPtmMoVolJbDv\nviH5i4gkSNYSv5m1MrPWqdfAocA0M+uUttqxwLRsxVCtVatg4kSVeUQkkbI5qqcj8IKFK2I3Av7q\n7q+a2TAz602o/88Bzs1iDFV7910oK1PHrogkUtYSv7vPBnavYvmvsnXMjI0bB02aQN++cUciIpJz\nyRzOWVICe+4JrVvHHYmISM4lL/GvXQvvvaf6vogkVvIS/4QJ8N13SvwikljJS/ypidn69Ys3DhGR\nmCQz8ffsCdm4KExEpAAkK1lVRDAAAAlMSURBVPGXlYWpGjSMU0QSLFmJf+pUWLFC9X0RSbRkJf5x\n48KzEr+IJFiyEn9JCWy7LXTtGnckIiKxSU7id6+4sbqISIIlJ/HPmgULF6pjV0QSLzmJPzV+Xy1+\nEUm45CT+cePC2P0ePeKOREQkVslJ/Kn6fpgmWkQksZKR+OfNg88+U5lHRISkJP5UfV8duyIiCUr8\nrVvD7j+4L4yISOIkJ/Hvtx80bRp3JCIisSv+xP/11zBtmso8IiKR4k/8b78dntWxKyICJCHxjxsH\nG28Me+8ddyQiInmh+BN/SQnssw+0aBF3JCIieaG4E/+qVTBxoso8IiJpijvx/+c/sGGDOnZFRNIU\nd+IvKYEmTaBv37gjERHJG8Wd+Lt2hTPPhM02izsSEZG8UdyJ/+yz4dFH445CRCSvFHfiFxGRH1Di\nFxFJGCV+EZGEUeIXEUkYJX4RkYTZKJs7N7M5wEqgDNjg7n3MrD3wHNANmAOc6O7LshmHiIhUyEWL\n/yB37+3ufaL3VwBj3X1HYGz0XkREciSOUs/RwJPR6yeBY2KIQUQksbJa6gEceN3MHHjI3R8GOrr7\ngujzr4COVW1oZucA50RvV5nZzHrGsAWwpJ7bxklx516hxq64c6uQ4t62qoXm7lk7opl1dvf5ZrYl\n8AZwETDa3dumrbPM3dtlMYYJaWWmgqG4c69QY1fcuVWocafLaqnH3edHz4uAF4B9gIVm1gkgel6U\nzRhEROT7spb4zayVmbVOvQYOBaYBo4EzotXOAP6erRhEROSHslnj7wi8YGap4/zV3V81sw+AEWZ2\nNvA5cGIWYwB4OMv7zxbFnXuFGrvizq1Cjft/slrjFxGR/KMrd0VEEkaJX0QkYYo68ZvZz81sppl9\nYmYFc4Wwmc0xs6lmNtnMJsQdT3XM7DEzW2Rm09KWtTezN8xsVvSctaG69VVN3NeZ2fzonE82swFx\nxlgVM9vGzN40s+lm9l8z+320PK/PeQ1x5/U5N7MWZva+mU2J4r4+Wr6dmb0X5ZXnzKx53LHWVdHW\n+M2sKfAx8FNgHvABcIq7T481sAxEcxz1cfe8vkjEzA4AVgFPufuu0bI/A0vd/bbox7adu18eZ5yV\nVRP3dcAqd78zzthqEg1/7uTuE6MRcx8Srnw/kzw+5zXEfSJ5fM4tjExp5e6rzKwZMB74PXApMMrd\nnzWzB4Ep7j4kzljrqphb/PsAn7j7bHdfBzxLmC5CGom7jwOWVlqc91NyVBN33nP3Be4+MXq9EvgI\n6Eyen/Ma4s5rHqyK3jaLHg4cDIyMlufd+c5EMSf+zsDctPfzKID/2CKpqS4+jKauKCQZTcmRpy40\ns9KoFJRX5ZLKzKwbsAfwHgV0zivFDXl+zs2sqZlNJlxo+gbwKbDc3TdEqxRSXvmfYk78hayfu+8J\nHAZcEJUmCo6HOmKh1BKHANsDvYEFwOB4w6memW0KPA9c7O4r0j/L53NeRdx5f87dvczdewNdCFWE\nnWMOqVEUc+KfD2yT9r5LtCzvVTPVRaEoyCk53H1h9D95OTCUPD3nUa35eWC4u4+KFuf9Oa8q7kI5\n5wDuvhx4E+gLtDWz1MWvBZNX0hVz4v8A2DHqgW8OnEyYLiKv1TDVRaEoyCk5Uokzcix5eM6jzsZH\ngY/c/S9pH+X1Oa8u7nw/52bWwczaRq83IQwU+YjwA3BCtFrene9MFO2oHoBoeNjdQFPgMXe/OeaQ\namVm3QmtfKiY6iIv4zazZ4D+hGlqFwLXAi8CI4CuRFNyuHtedaRWE3d/QsnBCXeGOzetbp4XzKwf\nUAJMBcqjxVcS6uV5e85riPsU8vicm9luhM7bpoRG8gh3vyH6f/RZoD0wCfilu6+NL9K6K+rELyIi\nP1TMpR4REamCEr+ISMIo8YuIJIwSv4hIwijxi4gkjBK/xMrM3MwGp70fFE2Y1hj7fsLMTqh9zQYf\n5xdm9pGZvVlp+dZmNjJ63bsxZ580s7Zm9tuqjiVSGyV+idta4Dgz2yLuQNKlXZmZibOBge5+UPpC\nd//S3VM/PL2BOiX+WmJoC/wv8Vc6lkiNlPglbhsI9zC9pPIHlVvsZrYqeu5vZm+Z2d/NbLaZ3WZm\np0Vzp081s+3TdnOImU0ws4/N7Iho+6ZmdoeZfRBNEHZu2n5LzGw08IPpu83slGj/08zs9mjZNUA/\n4FEzu6PS+t2idZsDNwAnRfPOnxRdof1YFPMkMzs62uZMMxttZv8CxprZpmY21swmRsdOzTB7G7B9\ntL87UseK9tHCzB6P1p9kZgel7XuUmb1qYe7+P9f5X0uKQjZvti6SqQeA0jomot2BnoTplWcDj7j7\nPhZu8nERcHG0XjfCHDDbA2+a2Q7A6cA37r63mW0MvG1mr0fr7wns6u6fpR/MzLYGbgf2ApYRZk89\nJrqS82BgkLtXedMcd18X/UD0cfcLo/3dAvzL3X8dTQvwvpmNSYthN3dfGrX6j3X3FdFfRf+Jfpiu\niOLsHe2vW9ohLwiH9R+Z2c5RrDtFn/UmzI65FphpZve5e/ostpIAavFL7KKZGp8CfleHzT6I5nlf\nS5gqN5W4pxKSfcoIdy9391mEH4idCfMfnW5hut33gM2BHaP136+c9CN7A/9298XRlLzDgYbMmnoo\ncEUUw7+BFoQpFwDeSJtywYBbzKwUGEOYAri2aZf7AU8DuPsMwjQOqcQ/1t2/cffvCH/VbNuA7yAF\nSi1+yRd3AxOBx9OWbSBqnJhZEyD9Fnfpc6OUp70v5/v/XVeek8QJyfQid38t/QMz6w+srl/4dWbA\n8e4+s1IM+1aK4TSgA7CXu6+3cHe2Fg04bvp5K0M5IJHU4pe8ELVwRxA6SlPmEEorAEcR7oBUV78w\nsyZR3b87MBN4DTjfwlTBmNlOFmZCrcn7wIFmtoWF23qeArxVhzhWAq3T3r8GXGRmFsWwRzXbtQEW\nRUn/ICpa6JX3l66E8INBVOLpSvjeIoASv+SXwYQZM1OGEpLtFMI86PVpjX9BSNqvAOdFJY5HCGWO\niVGH6EPU0vKNZo28gjAl7xTgQ3evy3S8bwK7pDp3gRsJP2SlZvbf6H1VhgN9zGwqoW9iRhTP14S+\niWmVO5WB/w80ibZ5Djiz0GaPlOzS7JwiIgmjFr+ISMIo8YuIJIwSv4hIwijxi4gkjBK/iEjCKPGL\niCSMEr+ISML8H6ugs4782eBAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 40.30431194623527 seconds\n",
            "Best accuracy: 75.36  Best training loss: 0.11685790121555328  Best validation loss: 0.7638533076643943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "88386c43-4e8c-44f3-e8b6-07e98f760bd1",
        "id": "gDoUiL35s9mQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
            "[1.4892572164535522, 1.304968237876892, 0.889128565788269, 0.8483538031578064, 0.9262908101081848, 0.8590133190155029, 0.8569309115409851, 0.9260328412055969, 0.6168851852416992, 0.7309474349021912, 0.48363128304481506, 0.5448474884033203, 0.4057130813598633, 0.5479651689529419, 0.5544900298118591, 0.6224130392074585, 0.40180450677871704, 0.3106207847595215, 0.4355888068675995, 0.5793075561523438, 0.3990998864173889, 0.3439003825187683, 0.2174069583415985, 0.1814344972372055, 0.26400136947631836, 0.11685790121555328, 0.5061777234077454, 0.5928786993026733, 0.35849058628082275, 0.394870400428772, 0.33372604846954346, 0.14104095101356506, 0.19826139509677887, 0.1764681190252304]\n",
            "[1.400896774530411, 1.2604336732625963, 1.0765493458509445, 1.0101558506488797, 0.9544164842367173, 0.8835893097519871, 0.8215996313095091, 0.8554058110713961, 0.8189084324240686, 0.7761981755495069, 0.7732098037004468, 0.7638533076643943, 0.8018766564130784, 0.8062319868803024, 0.7741041332483294, 0.8269118210673331, 0.7982379561662671, 0.8270634579658509, 0.8500555172562598, 0.8391899418830869, 0.8444426342844963, 0.895923542380333, 0.8422882559895517, 0.891230211853981, 0.9300823867321016, 0.9105420508980753, 0.932580091357231, 0.9349433210492137, 0.932054820358753, 0.9609150034189222, 0.9415992608666421, 0.9994860421121122, 1.0116262674331669, 1.039794435203075]\n",
            "[51.2, 55.9, 62.22, 64.98, 66.54, 68.5, 70.34, 69.8, 71.7, 73.14, 73.42, 73.54, 73.02, 72.66, 73.86, 73.5, 73.58, 73.96, 73.94, 73.84, 74.06, 73.06, 75.36, 73.82, 73.52, 74.06, 73.98, 73.82, 73.74, 74.34, 74.82, 74.54, 73.98, 73.34]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ObHa5LVyXSj"
      },
      "source": [
        "## squeeze skip residuals (batch normed) (4 extra layers for residuals)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YlNyQGvmyXSl",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    'squeezenet1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    'squeezenet1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, squeeze_planes,\n",
        "                 expand1x1_planes, expand3x3_planes):\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.bns1 = nn.BatchNorm2d(squeeze_planes)\n",
        "\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.bne1 = nn.BatchNorm2d(expand1x1_planes)\n",
        "\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.bne3 = nn.BatchNorm2d(expand3x3_planes)\n",
        "\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.squeeze_activation(self.bns1(self.squeeze(x)))\n",
        "\n",
        "\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.bne1(self.expand1x1(x))),\n",
        "            self.expand3x3_activation(self.bne3(self.expand3x3(x)))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, version='1_0', num_classes=1000):\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # self.features = nn.Sequential(\n",
        "        #         nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "        #         nn.ReLU(inplace=True),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(96, 16, 64, 64),\n",
        "        #         Fire(128, 16, 64, 64),\n",
        "        #         Fire(128, 32, 128, 128),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(256, 32, 128, 128),\n",
        "        #         Fire(256, 48, 192, 192),\n",
        "        #         Fire(384, 48, 192, 192),\n",
        "        #         Fire(384, 64, 256, 256),\n",
        "        #         nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        #         Fire(512, 64, 256, 256),\n",
        "        # )\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "            Fire(96, 16, 64, 64),\n",
        "        )\n",
        "        self.block1 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.block11 = nn.Sequential(\n",
        "            Fire(128, 16, 64, 64),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            Fire(128, 32, 128, 128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.block22 = nn.Sequential(\n",
        "            Fire(256, 32, 128, 128),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            Fire(256, 48, 192, 192),\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "        self.block33 = nn.Sequential(\n",
        "            Fire(384, 48, 192, 192),\n",
        "        )\n",
        "        self.features4 = nn.Sequential(\n",
        "            Fire(384, 64, 256, 256),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "        \n",
        "        )\n",
        "        self.block4 = nn.Sequential(\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "        self.block44 = nn.Sequential(\n",
        "            Fire(512, 64, 256, 256),\n",
        "        )\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.BatchNorm2d(self.num_classes),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "\n",
        "        residual1 = x\n",
        "        x = self.block1(x)\n",
        "        x += residual1\n",
        "        residual11 = x\n",
        "        x = self.block11(x)\n",
        "        x += residual11\n",
        "\n",
        "        x = self.features2(x)\n",
        "\n",
        "        residual2 = x\n",
        "        x = self.block2(x)\n",
        "        x += residual2\n",
        "        residual22 = x\n",
        "        x = self.block22(x)\n",
        "        x += residual22\n",
        "\n",
        "        x = self.features3(x)\n",
        "\n",
        "        residual3 = x\n",
        "        x = self.block3(x)\n",
        "        x += residual3\n",
        "        residual33 = x\n",
        "        x = self.block33(x)\n",
        "        x += residual33\n",
        "\n",
        "        x = self.features4(x)\n",
        "\n",
        "        residual4 = x\n",
        "        x = self.block4(x)\n",
        "        x += residual4\n",
        "        residual44 = x\n",
        "        x = self.block44(x)\n",
        "        x += residual44\n",
        "        \n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version, pretrained, progress, **kwargs):\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ent_PL-oyXSp",
        "colab": {}
      },
      "source": [
        "\n",
        "#change the line below for different models\n",
        "\n",
        "# import torchvision.models as models\n",
        "# model = models.squeezenet1_0(pretrained=True)\n",
        "\n",
        "model = squeezenet1_0(num_classes=10)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "model = model.to(device=device, dtype=torch.float)\n",
        "\n",
        "# Cross Entropy Loss \n",
        "error = CrossEntropyLoss().to(device=device, dtype=torch.float)\n",
        "\n",
        "#Optimizer\n",
        "learning_rate = 0.1\n",
        "optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=learning_rate)\n",
        "\n",
        "#Optimizer adam\n",
        "# learning_rate = 0.04\n",
        "# optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, dampening=0, weight_decay=0.0002, nesterov=False)\n",
        "# optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#adaptive learning rate\n",
        "# how many epoc before decreasing learning rate\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "90eaa777-9c67-4398-e1cb-58bb3ea0fdfa",
        "id": "2hvOUEowyXSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load trained dictionary\n",
        "PATH = '/content/squeezenet_loss.pth'\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "54e96a07-9a0a-4866-c3a5-44c4e2c8ea8c",
        "id": "V-1DiqNDyXSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#training/testing\n",
        "loss_list = []\n",
        "loss_list_test = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "# execution time checking\n",
        "execution_time = []\n",
        "\n",
        "PATH = '/content/squeezenet_acc.pth'\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "PATH2 = '/content/squeezenet_loss.pth'\n",
        "best_model_loss = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#PATH = '/content/modified_mnist_effb2_pre_original_latest.pth'\n",
        "#latest_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = float(\"inf\")\n",
        "best_acc = 0.0\n",
        "#set number of epochs \n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    #print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n",
        "    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\n",
        "\n",
        "    start = timeit.default_timer()\n",
        "    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\n",
        "    loss_test, accuracy = test(test_loader, model, error, batch_size)\n",
        "    stop = timeit.default_timer()\n",
        "    etime = stop - start\n",
        "    execution_time.append(etime)\n",
        "\n",
        "    loss_list_test.append(loss_test)\n",
        "    accuracy_list.append(accuracy)\n",
        "    iteration_list.append(epoch)\n",
        "    #exp_lr_scheduler.step()\n",
        "\n",
        "    #saving model with best acc \n",
        "    if accuracy > best_acc:\n",
        "      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\n",
        "      best_acc = accuracy\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_wts, PATH)\n",
        "    #saving model with best loss\n",
        "    if loss_test < best_loss:\n",
        "      print(f\"Better loss at Epoch {epoch}: loss = {loss_test}%\")\n",
        "      best_loss = loss_test\n",
        "      best_model_loss = copy.deepcopy(model.state_dict())\n",
        "      torch.save(best_model_loss, PATH2)\n",
        "#load model\n",
        "# model = Net()\n",
        "# model.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch 0: lr = 0.1\n",
            "Train Epoch: 0 [5000/50000 (10%)]\tTrain Loss: 2.278577\n",
            "Train Epoch: 0 [10000/50000 (20%)]\tTrain Loss: 2.001624\n",
            "Train Epoch: 0 [15000/50000 (30%)]\tTrain Loss: 1.868917\n",
            "Train Epoch: 0 [20000/50000 (40%)]\tTrain Loss: 1.755066\n",
            "Train Epoch: 0 [25000/50000 (50%)]\tTrain Loss: 1.739366\n",
            "Train Epoch: 0 [30000/50000 (60%)]\tTrain Loss: 1.672918\n",
            "Train Epoch: 0 [35000/50000 (70%)]\tTrain Loss: 1.630520\n",
            "Train Epoch: 0 [40000/50000 (80%)]\tTrain Loss: 1.571283\n",
            "Train Epoch: 0 [45000/50000 (90%)]\tTrain Loss: 1.541608\n",
            "\n",
            "Test set: Test loss: 1.4487, Accuracy: 2477/5000 (50%)\n",
            "\n",
            "Better accuracy at Epoch 0: accuracy = 49.54%\n",
            "Better loss at Epoch 0: loss = 1.4487174665927887%\n",
            "\n",
            "Train Epoch 1: lr = 0.1\n",
            "Train Epoch: 1 [5000/50000 (10%)]\tTrain Loss: 1.445172\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tTrain Loss: 1.418047\n",
            "Train Epoch: 1 [15000/50000 (30%)]\tTrain Loss: 1.432177\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tTrain Loss: 1.348816\n",
            "Train Epoch: 1 [25000/50000 (50%)]\tTrain Loss: 1.363841\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tTrain Loss: 1.352160\n",
            "Train Epoch: 1 [35000/50000 (70%)]\tTrain Loss: 1.328795\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tTrain Loss: 1.323387\n",
            "Train Epoch: 1 [45000/50000 (90%)]\tTrain Loss: 1.299537\n",
            "\n",
            "Test set: Test loss: 1.3192, Accuracy: 2713/5000 (54%)\n",
            "\n",
            "Better accuracy at Epoch 1: accuracy = 54.26%\n",
            "Better loss at Epoch 1: loss = 1.319182298183441%\n",
            "\n",
            "Train Epoch 2: lr = 0.1\n",
            "Train Epoch: 2 [5000/50000 (10%)]\tTrain Loss: 1.244424\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tTrain Loss: 1.211586\n",
            "Train Epoch: 2 [15000/50000 (30%)]\tTrain Loss: 1.183835\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tTrain Loss: 1.207375\n",
            "Train Epoch: 2 [25000/50000 (50%)]\tTrain Loss: 1.180338\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tTrain Loss: 1.178288\n",
            "Train Epoch: 2 [35000/50000 (70%)]\tTrain Loss: 1.172518\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tTrain Loss: 1.156841\n",
            "Train Epoch: 2 [45000/50000 (90%)]\tTrain Loss: 1.127047\n",
            "\n",
            "Test set: Test loss: 1.1127, Accuracy: 3055/5000 (61%)\n",
            "\n",
            "Better accuracy at Epoch 2: accuracy = 61.1%\n",
            "Better loss at Epoch 2: loss = 1.1126837414503097%\n",
            "\n",
            "Train Epoch 3: lr = 0.1\n",
            "Train Epoch: 3 [5000/50000 (10%)]\tTrain Loss: 1.058322\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tTrain Loss: 1.061512\n",
            "Train Epoch: 3 [15000/50000 (30%)]\tTrain Loss: 1.061597\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tTrain Loss: 1.062386\n",
            "Train Epoch: 3 [25000/50000 (50%)]\tTrain Loss: 1.059800\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tTrain Loss: 1.055520\n",
            "Train Epoch: 3 [35000/50000 (70%)]\tTrain Loss: 1.064332\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tTrain Loss: 1.030783\n",
            "Train Epoch: 3 [45000/50000 (90%)]\tTrain Loss: 1.044064\n",
            "\n",
            "Test set: Test loss: 1.1061, Accuracy: 3098/5000 (62%)\n",
            "\n",
            "Better accuracy at Epoch 3: accuracy = 61.96%\n",
            "Better loss at Epoch 3: loss = 1.1061175251007074%\n",
            "\n",
            "Train Epoch 4: lr = 0.1\n",
            "Train Epoch: 4 [5000/50000 (10%)]\tTrain Loss: 0.978578\n",
            "Train Epoch: 4 [10000/50000 (20%)]\tTrain Loss: 0.956776\n",
            "Train Epoch: 4 [15000/50000 (30%)]\tTrain Loss: 0.958392\n",
            "Train Epoch: 4 [20000/50000 (40%)]\tTrain Loss: 0.941935\n",
            "Train Epoch: 4 [25000/50000 (50%)]\tTrain Loss: 0.953908\n",
            "Train Epoch: 4 [30000/50000 (60%)]\tTrain Loss: 0.953473\n",
            "Train Epoch: 4 [35000/50000 (70%)]\tTrain Loss: 0.959978\n",
            "Train Epoch: 4 [40000/50000 (80%)]\tTrain Loss: 0.954004\n",
            "Train Epoch: 4 [45000/50000 (90%)]\tTrain Loss: 0.928795\n",
            "\n",
            "Test set: Test loss: 1.0109, Accuracy: 3287/5000 (66%)\n",
            "\n",
            "Better accuracy at Epoch 4: accuracy = 65.74%\n",
            "Better loss at Epoch 4: loss = 1.010921832919121%\n",
            "\n",
            "Train Epoch 5: lr = 0.1\n",
            "Train Epoch: 5 [5000/50000 (10%)]\tTrain Loss: 0.855853\n",
            "Train Epoch: 5 [10000/50000 (20%)]\tTrain Loss: 0.876006\n",
            "Train Epoch: 5 [15000/50000 (30%)]\tTrain Loss: 0.870379\n",
            "Train Epoch: 5 [20000/50000 (40%)]\tTrain Loss: 0.849420\n",
            "Train Epoch: 5 [25000/50000 (50%)]\tTrain Loss: 0.891807\n",
            "Train Epoch: 5 [30000/50000 (60%)]\tTrain Loss: 0.919194\n",
            "Train Epoch: 5 [35000/50000 (70%)]\tTrain Loss: 0.864335\n",
            "Train Epoch: 5 [40000/50000 (80%)]\tTrain Loss: 0.899845\n",
            "Train Epoch: 5 [45000/50000 (90%)]\tTrain Loss: 0.897509\n",
            "\n",
            "Test set: Test loss: 0.9006, Accuracy: 3454/5000 (69%)\n",
            "\n",
            "Better accuracy at Epoch 5: accuracy = 69.08%\n",
            "Better loss at Epoch 5: loss = 0.9006039363145828%\n",
            "\n",
            "Train Epoch 6: lr = 0.1\n",
            "Train Epoch: 6 [5000/50000 (10%)]\tTrain Loss: 0.797182\n",
            "Train Epoch: 6 [10000/50000 (20%)]\tTrain Loss: 0.802346\n",
            "Train Epoch: 6 [15000/50000 (30%)]\tTrain Loss: 0.813439\n",
            "Train Epoch: 6 [20000/50000 (40%)]\tTrain Loss: 0.797904\n",
            "Train Epoch: 6 [25000/50000 (50%)]\tTrain Loss: 0.807491\n",
            "Train Epoch: 6 [30000/50000 (60%)]\tTrain Loss: 0.795189\n",
            "Train Epoch: 6 [35000/50000 (70%)]\tTrain Loss: 0.815275\n",
            "Train Epoch: 6 [40000/50000 (80%)]\tTrain Loss: 0.815156\n",
            "Train Epoch: 6 [45000/50000 (90%)]\tTrain Loss: 0.854703\n",
            "\n",
            "Test set: Test loss: 0.8648, Accuracy: 3526/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 6: accuracy = 70.52%\n",
            "Better loss at Epoch 6: loss = 0.8648271256685255%\n",
            "\n",
            "Train Epoch 7: lr = 0.1\n",
            "Train Epoch: 7 [5000/50000 (10%)]\tTrain Loss: 0.749101\n",
            "Train Epoch: 7 [10000/50000 (20%)]\tTrain Loss: 0.724909\n",
            "Train Epoch: 7 [15000/50000 (30%)]\tTrain Loss: 0.761048\n",
            "Train Epoch: 7 [20000/50000 (40%)]\tTrain Loss: 0.754475\n",
            "Train Epoch: 7 [25000/50000 (50%)]\tTrain Loss: 0.740937\n",
            "Train Epoch: 7 [30000/50000 (60%)]\tTrain Loss: 0.770025\n",
            "Train Epoch: 7 [35000/50000 (70%)]\tTrain Loss: 0.761108\n",
            "Train Epoch: 7 [40000/50000 (80%)]\tTrain Loss: 0.765576\n",
            "Train Epoch: 7 [45000/50000 (90%)]\tTrain Loss: 0.752509\n",
            "\n",
            "Test set: Test loss: 0.8573, Accuracy: 3557/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 7: accuracy = 71.14%\n",
            "Better loss at Epoch 7: loss = 0.8572568905353544%\n",
            "\n",
            "Train Epoch 8: lr = 0.1\n",
            "Train Epoch: 8 [5000/50000 (10%)]\tTrain Loss: 0.674392\n",
            "Train Epoch: 8 [10000/50000 (20%)]\tTrain Loss: 0.671681\n",
            "Train Epoch: 8 [15000/50000 (30%)]\tTrain Loss: 0.695263\n",
            "Train Epoch: 8 [20000/50000 (40%)]\tTrain Loss: 0.704344\n",
            "Train Epoch: 8 [25000/50000 (50%)]\tTrain Loss: 0.718038\n",
            "Train Epoch: 8 [30000/50000 (60%)]\tTrain Loss: 0.699339\n",
            "Train Epoch: 8 [35000/50000 (70%)]\tTrain Loss: 0.707348\n",
            "Train Epoch: 8 [40000/50000 (80%)]\tTrain Loss: 0.728916\n",
            "Train Epoch: 8 [45000/50000 (90%)]\tTrain Loss: 0.698542\n",
            "\n",
            "Test set: Test loss: 0.8415, Accuracy: 3558/5000 (71%)\n",
            "\n",
            "Better accuracy at Epoch 8: accuracy = 71.16%\n",
            "Better loss at Epoch 8: loss = 0.841548652648926%\n",
            "\n",
            "Train Epoch 9: lr = 0.1\n",
            "Train Epoch: 9 [5000/50000 (10%)]\tTrain Loss: 0.607509\n",
            "Train Epoch: 9 [10000/50000 (20%)]\tTrain Loss: 0.633117\n",
            "Train Epoch: 9 [15000/50000 (30%)]\tTrain Loss: 0.659853\n",
            "Train Epoch: 9 [20000/50000 (40%)]\tTrain Loss: 0.660430\n",
            "Train Epoch: 9 [25000/50000 (50%)]\tTrain Loss: 0.653999\n",
            "Train Epoch: 9 [30000/50000 (60%)]\tTrain Loss: 0.670696\n",
            "Train Epoch: 9 [35000/50000 (70%)]\tTrain Loss: 0.653693\n",
            "Train Epoch: 9 [40000/50000 (80%)]\tTrain Loss: 0.661758\n",
            "Train Epoch: 9 [45000/50000 (90%)]\tTrain Loss: 0.677670\n",
            "\n",
            "Test set: Test loss: 0.8413, Accuracy: 3592/5000 (72%)\n",
            "\n",
            "Better accuracy at Epoch 9: accuracy = 71.84%\n",
            "Better loss at Epoch 9: loss = 0.8413213419914245%\n",
            "\n",
            "Train Epoch 10: lr = 0.1\n",
            "Train Epoch: 10 [5000/50000 (10%)]\tTrain Loss: 0.563500\n",
            "Train Epoch: 10 [10000/50000 (20%)]\tTrain Loss: 0.591543\n",
            "Train Epoch: 10 [15000/50000 (30%)]\tTrain Loss: 0.599289\n",
            "Train Epoch: 10 [20000/50000 (40%)]\tTrain Loss: 0.608910\n",
            "Train Epoch: 10 [25000/50000 (50%)]\tTrain Loss: 0.613071\n",
            "Train Epoch: 10 [30000/50000 (60%)]\tTrain Loss: 0.609743\n",
            "Train Epoch: 10 [35000/50000 (70%)]\tTrain Loss: 0.632365\n",
            "Train Epoch: 10 [40000/50000 (80%)]\tTrain Loss: 0.617778\n",
            "Train Epoch: 10 [45000/50000 (90%)]\tTrain Loss: 0.626682\n",
            "\n",
            "Test set: Test loss: 0.8064, Accuracy: 3639/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 10: accuracy = 72.78%\n",
            "Better loss at Epoch 10: loss = 0.8063691279292109%\n",
            "\n",
            "Train Epoch 11: lr = 0.1\n",
            "Train Epoch: 11 [5000/50000 (10%)]\tTrain Loss: 0.511577\n",
            "Train Epoch: 11 [10000/50000 (20%)]\tTrain Loss: 0.566198\n",
            "Train Epoch: 11 [15000/50000 (30%)]\tTrain Loss: 0.555729\n",
            "Train Epoch: 11 [20000/50000 (40%)]\tTrain Loss: 0.555226\n",
            "Train Epoch: 11 [25000/50000 (50%)]\tTrain Loss: 0.544935\n",
            "Train Epoch: 11 [30000/50000 (60%)]\tTrain Loss: 0.585616\n",
            "Train Epoch: 11 [35000/50000 (70%)]\tTrain Loss: 0.594254\n",
            "Train Epoch: 11 [40000/50000 (80%)]\tTrain Loss: 0.591646\n",
            "Train Epoch: 11 [45000/50000 (90%)]\tTrain Loss: 0.573020\n",
            "\n",
            "Test set: Test loss: 0.8160, Accuracy: 3645/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 11: accuracy = 72.9%\n",
            "\n",
            "Train Epoch 12: lr = 0.1\n",
            "Train Epoch: 12 [5000/50000 (10%)]\tTrain Loss: 0.513217\n",
            "Train Epoch: 12 [10000/50000 (20%)]\tTrain Loss: 0.506325\n",
            "Train Epoch: 12 [15000/50000 (30%)]\tTrain Loss: 0.516336\n",
            "Train Epoch: 12 [20000/50000 (40%)]\tTrain Loss: 0.523234\n",
            "Train Epoch: 12 [25000/50000 (50%)]\tTrain Loss: 0.560909\n",
            "Train Epoch: 12 [30000/50000 (60%)]\tTrain Loss: 0.546105\n",
            "Train Epoch: 12 [35000/50000 (70%)]\tTrain Loss: 0.569760\n",
            "Train Epoch: 12 [40000/50000 (80%)]\tTrain Loss: 0.556840\n",
            "Train Epoch: 12 [45000/50000 (90%)]\tTrain Loss: 0.555377\n",
            "\n",
            "Test set: Test loss: 0.8591, Accuracy: 3602/5000 (72%)\n",
            "\n",
            "\n",
            "Train Epoch 13: lr = 0.1\n",
            "Train Epoch: 13 [5000/50000 (10%)]\tTrain Loss: 0.470096\n",
            "Train Epoch: 13 [10000/50000 (20%)]\tTrain Loss: 0.478408\n",
            "Train Epoch: 13 [15000/50000 (30%)]\tTrain Loss: 0.483197\n",
            "Train Epoch: 13 [20000/50000 (40%)]\tTrain Loss: 0.490456\n",
            "Train Epoch: 13 [25000/50000 (50%)]\tTrain Loss: 0.491883\n",
            "Train Epoch: 13 [30000/50000 (60%)]\tTrain Loss: 0.506872\n",
            "Train Epoch: 13 [35000/50000 (70%)]\tTrain Loss: 0.519860\n",
            "Train Epoch: 13 [40000/50000 (80%)]\tTrain Loss: 0.504048\n",
            "Train Epoch: 13 [45000/50000 (90%)]\tTrain Loss: 0.523332\n",
            "\n",
            "Test set: Test loss: 0.8492, Accuracy: 3631/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 14: lr = 0.1\n",
            "Train Epoch: 14 [5000/50000 (10%)]\tTrain Loss: 0.425259\n",
            "Train Epoch: 14 [10000/50000 (20%)]\tTrain Loss: 0.451467\n",
            "Train Epoch: 14 [15000/50000 (30%)]\tTrain Loss: 0.451849\n",
            "Train Epoch: 14 [20000/50000 (40%)]\tTrain Loss: 0.481956\n",
            "Train Epoch: 14 [25000/50000 (50%)]\tTrain Loss: 0.480671\n",
            "Train Epoch: 14 [30000/50000 (60%)]\tTrain Loss: 0.477225\n",
            "Train Epoch: 14 [35000/50000 (70%)]\tTrain Loss: 0.498325\n",
            "Train Epoch: 14 [40000/50000 (80%)]\tTrain Loss: 0.502790\n",
            "Train Epoch: 14 [45000/50000 (90%)]\tTrain Loss: 0.487058\n",
            "\n",
            "Test set: Test loss: 0.8472, Accuracy: 3649/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 14: accuracy = 72.98%\n",
            "\n",
            "Train Epoch 15: lr = 0.1\n",
            "Train Epoch: 15 [5000/50000 (10%)]\tTrain Loss: 0.406343\n",
            "Train Epoch: 15 [10000/50000 (20%)]\tTrain Loss: 0.414635\n",
            "Train Epoch: 15 [15000/50000 (30%)]\tTrain Loss: 0.445100\n",
            "Train Epoch: 15 [20000/50000 (40%)]\tTrain Loss: 0.431193\n",
            "Train Epoch: 15 [25000/50000 (50%)]\tTrain Loss: 0.453768\n",
            "Train Epoch: 15 [30000/50000 (60%)]\tTrain Loss: 0.445884\n",
            "Train Epoch: 15 [35000/50000 (70%)]\tTrain Loss: 0.438643\n",
            "Train Epoch: 15 [40000/50000 (80%)]\tTrain Loss: 0.497054\n",
            "Train Epoch: 15 [45000/50000 (90%)]\tTrain Loss: 0.445251\n",
            "\n",
            "Test set: Test loss: 0.8689, Accuracy: 3641/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 16: lr = 0.1\n",
            "Train Epoch: 16 [5000/50000 (10%)]\tTrain Loss: 0.377141\n",
            "Train Epoch: 16 [10000/50000 (20%)]\tTrain Loss: 0.388873\n",
            "Train Epoch: 16 [15000/50000 (30%)]\tTrain Loss: 0.394547\n",
            "Train Epoch: 16 [20000/50000 (40%)]\tTrain Loss: 0.419532\n",
            "Train Epoch: 16 [25000/50000 (50%)]\tTrain Loss: 0.450537\n",
            "Train Epoch: 16 [30000/50000 (60%)]\tTrain Loss: 0.446176\n",
            "Train Epoch: 16 [35000/50000 (70%)]\tTrain Loss: 0.421989\n",
            "Train Epoch: 16 [40000/50000 (80%)]\tTrain Loss: 0.441601\n",
            "Train Epoch: 16 [45000/50000 (90%)]\tTrain Loss: 0.432404\n",
            "\n",
            "Test set: Test loss: 0.8584, Accuracy: 3666/5000 (73%)\n",
            "\n",
            "Better accuracy at Epoch 16: accuracy = 73.32%\n",
            "\n",
            "Train Epoch 17: lr = 0.1\n",
            "Train Epoch: 17 [5000/50000 (10%)]\tTrain Loss: 0.357270\n",
            "Train Epoch: 17 [10000/50000 (20%)]\tTrain Loss: 0.346319\n",
            "Train Epoch: 17 [15000/50000 (30%)]\tTrain Loss: 0.375604\n",
            "Train Epoch: 17 [20000/50000 (40%)]\tTrain Loss: 0.392739\n",
            "Train Epoch: 17 [25000/50000 (50%)]\tTrain Loss: 0.379948\n",
            "Train Epoch: 17 [30000/50000 (60%)]\tTrain Loss: 0.378435\n",
            "Train Epoch: 17 [35000/50000 (70%)]\tTrain Loss: 0.408617\n",
            "Train Epoch: 17 [40000/50000 (80%)]\tTrain Loss: 0.421244\n",
            "Train Epoch: 17 [45000/50000 (90%)]\tTrain Loss: 0.423440\n",
            "\n",
            "Test set: Test loss: 0.8499, Accuracy: 3689/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 17: accuracy = 73.78%\n",
            "\n",
            "Train Epoch 18: lr = 0.1\n",
            "Train Epoch: 18 [5000/50000 (10%)]\tTrain Loss: 0.328891\n",
            "Train Epoch: 18 [10000/50000 (20%)]\tTrain Loss: 0.319119\n",
            "Train Epoch: 18 [15000/50000 (30%)]\tTrain Loss: 0.383688\n",
            "Train Epoch: 18 [20000/50000 (40%)]\tTrain Loss: 0.366064\n",
            "Train Epoch: 18 [25000/50000 (50%)]\tTrain Loss: 0.363179\n",
            "Train Epoch: 18 [30000/50000 (60%)]\tTrain Loss: 0.368236\n",
            "Train Epoch: 18 [35000/50000 (70%)]\tTrain Loss: 0.381120\n",
            "Train Epoch: 18 [40000/50000 (80%)]\tTrain Loss: 0.388363\n",
            "Train Epoch: 18 [45000/50000 (90%)]\tTrain Loss: 0.419069\n",
            "\n",
            "Test set: Test loss: 0.8357, Accuracy: 3672/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 19: lr = 0.1\n",
            "Train Epoch: 19 [5000/50000 (10%)]\tTrain Loss: 0.301231\n",
            "Train Epoch: 19 [10000/50000 (20%)]\tTrain Loss: 0.323093\n",
            "Train Epoch: 19 [15000/50000 (30%)]\tTrain Loss: 0.323743\n",
            "Train Epoch: 19 [20000/50000 (40%)]\tTrain Loss: 0.350023\n",
            "Train Epoch: 19 [25000/50000 (50%)]\tTrain Loss: 0.365657\n",
            "Train Epoch: 19 [30000/50000 (60%)]\tTrain Loss: 0.356039\n",
            "Train Epoch: 19 [35000/50000 (70%)]\tTrain Loss: 0.365790\n",
            "Train Epoch: 19 [40000/50000 (80%)]\tTrain Loss: 0.389188\n",
            "Train Epoch: 19 [45000/50000 (90%)]\tTrain Loss: 0.377747\n",
            "\n",
            "Test set: Test loss: 0.8715, Accuracy: 3662/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 20: lr = 0.1\n",
            "Train Epoch: 20 [5000/50000 (10%)]\tTrain Loss: 0.281639\n",
            "Train Epoch: 20 [10000/50000 (20%)]\tTrain Loss: 0.322106\n",
            "Train Epoch: 20 [15000/50000 (30%)]\tTrain Loss: 0.295553\n",
            "Train Epoch: 20 [20000/50000 (40%)]\tTrain Loss: 0.329678\n",
            "Train Epoch: 20 [25000/50000 (50%)]\tTrain Loss: 0.348744\n",
            "Train Epoch: 20 [30000/50000 (60%)]\tTrain Loss: 0.353719\n",
            "Train Epoch: 20 [35000/50000 (70%)]\tTrain Loss: 0.354448\n",
            "Train Epoch: 20 [40000/50000 (80%)]\tTrain Loss: 0.358763\n",
            "Train Epoch: 20 [45000/50000 (90%)]\tTrain Loss: 0.352936\n",
            "\n",
            "Test set: Test loss: 0.8962, Accuracy: 3669/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 21: lr = 0.1\n",
            "Train Epoch: 21 [5000/50000 (10%)]\tTrain Loss: 0.299646\n",
            "Train Epoch: 21 [10000/50000 (20%)]\tTrain Loss: 0.299913\n",
            "Train Epoch: 21 [15000/50000 (30%)]\tTrain Loss: 0.314261\n",
            "Train Epoch: 21 [20000/50000 (40%)]\tTrain Loss: 0.316923\n",
            "Train Epoch: 21 [25000/50000 (50%)]\tTrain Loss: 0.285295\n",
            "Train Epoch: 21 [30000/50000 (60%)]\tTrain Loss: 0.311335\n",
            "Train Epoch: 21 [35000/50000 (70%)]\tTrain Loss: 0.298703\n",
            "Train Epoch: 21 [40000/50000 (80%)]\tTrain Loss: 0.335112\n",
            "Train Epoch: 21 [45000/50000 (90%)]\tTrain Loss: 0.318678\n",
            "\n",
            "Test set: Test loss: 0.9010, Accuracy: 3650/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 22: lr = 0.1\n",
            "Train Epoch: 22 [5000/50000 (10%)]\tTrain Loss: 0.254352\n",
            "Train Epoch: 22 [10000/50000 (20%)]\tTrain Loss: 0.278238\n",
            "Train Epoch: 22 [15000/50000 (30%)]\tTrain Loss: 0.263113\n",
            "Train Epoch: 22 [20000/50000 (40%)]\tTrain Loss: 0.282217\n",
            "Train Epoch: 22 [25000/50000 (50%)]\tTrain Loss: 0.294255\n",
            "Train Epoch: 22 [30000/50000 (60%)]\tTrain Loss: 0.299851\n",
            "Train Epoch: 22 [35000/50000 (70%)]\tTrain Loss: 0.330183\n",
            "Train Epoch: 22 [40000/50000 (80%)]\tTrain Loss: 0.336192\n",
            "Train Epoch: 22 [45000/50000 (90%)]\tTrain Loss: 0.304637\n",
            "\n",
            "Test set: Test loss: 0.9422, Accuracy: 3648/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 23: lr = 0.1\n",
            "Train Epoch: 23 [5000/50000 (10%)]\tTrain Loss: 0.242591\n",
            "Train Epoch: 23 [10000/50000 (20%)]\tTrain Loss: 0.251980\n",
            "Train Epoch: 23 [15000/50000 (30%)]\tTrain Loss: 0.254920\n",
            "Train Epoch: 23 [20000/50000 (40%)]\tTrain Loss: 0.277582\n",
            "Train Epoch: 23 [25000/50000 (50%)]\tTrain Loss: 0.244554\n",
            "Train Epoch: 23 [30000/50000 (60%)]\tTrain Loss: 0.268706\n",
            "Train Epoch: 23 [35000/50000 (70%)]\tTrain Loss: 0.319099\n",
            "Train Epoch: 23 [40000/50000 (80%)]\tTrain Loss: 0.320389\n",
            "Train Epoch: 23 [45000/50000 (90%)]\tTrain Loss: 0.328204\n",
            "\n",
            "Test set: Test loss: 0.9300, Accuracy: 3652/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 24: lr = 0.1\n",
            "Train Epoch: 24 [5000/50000 (10%)]\tTrain Loss: 0.214802\n",
            "Train Epoch: 24 [10000/50000 (20%)]\tTrain Loss: 0.205851\n",
            "Train Epoch: 24 [15000/50000 (30%)]\tTrain Loss: 0.237820\n",
            "Train Epoch: 24 [20000/50000 (40%)]\tTrain Loss: 0.257104\n",
            "Train Epoch: 24 [25000/50000 (50%)]\tTrain Loss: 0.290759\n",
            "Train Epoch: 24 [30000/50000 (60%)]\tTrain Loss: 0.275862\n",
            "Train Epoch: 24 [35000/50000 (70%)]\tTrain Loss: 0.267447\n",
            "Train Epoch: 24 [40000/50000 (80%)]\tTrain Loss: 0.286446\n",
            "Train Epoch: 24 [45000/50000 (90%)]\tTrain Loss: 0.294140\n",
            "\n",
            "Test set: Test loss: 0.9491, Accuracy: 3638/5000 (73%)\n",
            "\n",
            "\n",
            "Train Epoch 25: lr = 0.1\n",
            "Train Epoch: 25 [5000/50000 (10%)]\tTrain Loss: 0.207223\n",
            "Train Epoch: 25 [10000/50000 (20%)]\tTrain Loss: 0.237256\n",
            "Train Epoch: 25 [15000/50000 (30%)]\tTrain Loss: 0.212008\n",
            "Train Epoch: 25 [20000/50000 (40%)]\tTrain Loss: 0.237017\n",
            "Train Epoch: 25 [25000/50000 (50%)]\tTrain Loss: 0.287386\n",
            "Train Epoch: 25 [30000/50000 (60%)]\tTrain Loss: 0.266535\n",
            "Train Epoch: 25 [35000/50000 (70%)]\tTrain Loss: 0.259613\n",
            "Train Epoch: 25 [40000/50000 (80%)]\tTrain Loss: 0.274978\n",
            "Train Epoch: 25 [45000/50000 (90%)]\tTrain Loss: 0.265159\n",
            "\n",
            "Test set: Test loss: 0.9669, Accuracy: 3696/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 25: accuracy = 73.92%\n",
            "\n",
            "Train Epoch 26: lr = 0.1\n",
            "Train Epoch: 26 [5000/50000 (10%)]\tTrain Loss: 0.200657\n",
            "Train Epoch: 26 [10000/50000 (20%)]\tTrain Loss: 0.213984\n",
            "Train Epoch: 26 [15000/50000 (30%)]\tTrain Loss: 0.232172\n",
            "Train Epoch: 26 [20000/50000 (40%)]\tTrain Loss: 0.226938\n",
            "Train Epoch: 26 [25000/50000 (50%)]\tTrain Loss: 0.227274\n",
            "Train Epoch: 26 [30000/50000 (60%)]\tTrain Loss: 0.265836\n",
            "Train Epoch: 26 [35000/50000 (70%)]\tTrain Loss: 0.275271\n",
            "Train Epoch: 26 [40000/50000 (80%)]\tTrain Loss: 0.248387\n",
            "Train Epoch: 26 [45000/50000 (90%)]\tTrain Loss: 0.247608\n",
            "\n",
            "Test set: Test loss: 0.9462, Accuracy: 3704/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 26: accuracy = 74.08%\n",
            "\n",
            "Train Epoch 27: lr = 0.1\n",
            "Train Epoch: 27 [5000/50000 (10%)]\tTrain Loss: 0.211615\n",
            "Train Epoch: 27 [10000/50000 (20%)]\tTrain Loss: 0.198843\n",
            "Train Epoch: 27 [15000/50000 (30%)]\tTrain Loss: 0.213237\n",
            "Train Epoch: 27 [20000/50000 (40%)]\tTrain Loss: 0.213637\n",
            "Train Epoch: 27 [25000/50000 (50%)]\tTrain Loss: 0.210426\n",
            "Train Epoch: 27 [30000/50000 (60%)]\tTrain Loss: 0.230991\n",
            "Train Epoch: 27 [35000/50000 (70%)]\tTrain Loss: 0.240502\n",
            "Train Epoch: 27 [40000/50000 (80%)]\tTrain Loss: 0.237204\n",
            "Train Epoch: 27 [45000/50000 (90%)]\tTrain Loss: 0.254226\n",
            "\n",
            "Test set: Test loss: 0.9531, Accuracy: 3692/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 28: lr = 0.1\n",
            "Train Epoch: 28 [5000/50000 (10%)]\tTrain Loss: 0.174975\n",
            "Train Epoch: 28 [10000/50000 (20%)]\tTrain Loss: 0.181893\n",
            "Train Epoch: 28 [15000/50000 (30%)]\tTrain Loss: 0.185778\n",
            "Train Epoch: 28 [20000/50000 (40%)]\tTrain Loss: 0.203424\n",
            "Train Epoch: 28 [25000/50000 (50%)]\tTrain Loss: 0.207784\n",
            "Train Epoch: 28 [30000/50000 (60%)]\tTrain Loss: 0.224403\n",
            "Train Epoch: 28 [35000/50000 (70%)]\tTrain Loss: 0.244377\n",
            "Train Epoch: 28 [40000/50000 (80%)]\tTrain Loss: 0.204103\n",
            "Train Epoch: 28 [45000/50000 (90%)]\tTrain Loss: 0.239379\n",
            "\n",
            "Test set: Test loss: 0.9481, Accuracy: 3690/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 29: lr = 0.1\n",
            "Train Epoch: 29 [5000/50000 (10%)]\tTrain Loss: 0.170448\n",
            "Train Epoch: 29 [10000/50000 (20%)]\tTrain Loss: 0.180477\n",
            "Train Epoch: 29 [15000/50000 (30%)]\tTrain Loss: 0.182144\n",
            "Train Epoch: 29 [20000/50000 (40%)]\tTrain Loss: 0.194879\n",
            "Train Epoch: 29 [25000/50000 (50%)]\tTrain Loss: 0.205381\n",
            "Train Epoch: 29 [30000/50000 (60%)]\tTrain Loss: 0.201247\n",
            "Train Epoch: 29 [35000/50000 (70%)]\tTrain Loss: 0.212750\n",
            "Train Epoch: 29 [40000/50000 (80%)]\tTrain Loss: 0.210643\n",
            "Train Epoch: 29 [45000/50000 (90%)]\tTrain Loss: 0.217507\n",
            "\n",
            "Test set: Test loss: 1.0008, Accuracy: 3688/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 30: lr = 0.1\n",
            "Train Epoch: 30 [5000/50000 (10%)]\tTrain Loss: 0.169274\n",
            "Train Epoch: 30 [10000/50000 (20%)]\tTrain Loss: 0.154603\n",
            "Train Epoch: 30 [15000/50000 (30%)]\tTrain Loss: 0.188933\n",
            "Train Epoch: 30 [20000/50000 (40%)]\tTrain Loss: 0.186964\n",
            "Train Epoch: 30 [25000/50000 (50%)]\tTrain Loss: 0.184685\n",
            "Train Epoch: 30 [30000/50000 (60%)]\tTrain Loss: 0.183783\n",
            "Train Epoch: 30 [35000/50000 (70%)]\tTrain Loss: 0.191877\n",
            "Train Epoch: 30 [40000/50000 (80%)]\tTrain Loss: 0.202892\n",
            "Train Epoch: 30 [45000/50000 (90%)]\tTrain Loss: 0.211084\n",
            "\n",
            "Test set: Test loss: 1.0027, Accuracy: 3725/5000 (74%)\n",
            "\n",
            "Better accuracy at Epoch 30: accuracy = 74.5%\n",
            "\n",
            "Train Epoch 31: lr = 0.1\n",
            "Train Epoch: 31 [5000/50000 (10%)]\tTrain Loss: 0.159788\n",
            "Train Epoch: 31 [10000/50000 (20%)]\tTrain Loss: 0.150823\n",
            "Train Epoch: 31 [15000/50000 (30%)]\tTrain Loss: 0.162125\n",
            "Train Epoch: 31 [20000/50000 (40%)]\tTrain Loss: 0.160432\n",
            "Train Epoch: 31 [25000/50000 (50%)]\tTrain Loss: 0.155836\n",
            "Train Epoch: 31 [30000/50000 (60%)]\tTrain Loss: 0.185017\n",
            "Train Epoch: 31 [35000/50000 (70%)]\tTrain Loss: 0.197090\n",
            "Train Epoch: 31 [40000/50000 (80%)]\tTrain Loss: 0.209189\n",
            "Train Epoch: 31 [45000/50000 (90%)]\tTrain Loss: 0.192897\n",
            "\n",
            "Test set: Test loss: 1.0145, Accuracy: 3723/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 32: lr = 0.1\n",
            "Train Epoch: 32 [5000/50000 (10%)]\tTrain Loss: 0.137563\n",
            "Train Epoch: 32 [10000/50000 (20%)]\tTrain Loss: 0.150285\n",
            "Train Epoch: 32 [15000/50000 (30%)]\tTrain Loss: 0.161849\n",
            "Train Epoch: 32 [20000/50000 (40%)]\tTrain Loss: 0.169246\n",
            "Train Epoch: 32 [25000/50000 (50%)]\tTrain Loss: 0.171341\n",
            "Train Epoch: 32 [30000/50000 (60%)]\tTrain Loss: 0.187159\n",
            "Train Epoch: 32 [35000/50000 (70%)]\tTrain Loss: 0.194674\n",
            "Train Epoch: 32 [40000/50000 (80%)]\tTrain Loss: 0.167514\n",
            "Train Epoch: 32 [45000/50000 (90%)]\tTrain Loss: 0.197827\n",
            "\n",
            "Test set: Test loss: 1.0239, Accuracy: 3714/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 33: lr = 0.1\n",
            "Train Epoch: 33 [5000/50000 (10%)]\tTrain Loss: 0.134734\n",
            "Train Epoch: 33 [10000/50000 (20%)]\tTrain Loss: 0.138409\n",
            "Train Epoch: 33 [15000/50000 (30%)]\tTrain Loss: 0.132153\n",
            "Train Epoch: 33 [20000/50000 (40%)]\tTrain Loss: 0.152321\n",
            "Train Epoch: 33 [25000/50000 (50%)]\tTrain Loss: 0.170693\n",
            "Train Epoch: 33 [30000/50000 (60%)]\tTrain Loss: 0.181755\n",
            "Train Epoch: 33 [35000/50000 (70%)]\tTrain Loss: 0.171129\n",
            "Train Epoch: 33 [40000/50000 (80%)]\tTrain Loss: 0.168199\n",
            "Train Epoch: 33 [45000/50000 (90%)]\tTrain Loss: 0.177296\n",
            "\n",
            "Test set: Test loss: 1.0401, Accuracy: 3736/5000 (75%)\n",
            "\n",
            "Better accuracy at Epoch 33: accuracy = 74.72%\n",
            "\n",
            "Train Epoch 34: lr = 0.1\n",
            "Train Epoch: 34 [5000/50000 (10%)]\tTrain Loss: 0.143681\n",
            "Train Epoch: 34 [10000/50000 (20%)]\tTrain Loss: 0.123849\n",
            "Train Epoch: 34 [15000/50000 (30%)]\tTrain Loss: 0.117445\n",
            "Train Epoch: 34 [20000/50000 (40%)]\tTrain Loss: 0.152229\n",
            "Train Epoch: 34 [25000/50000 (50%)]\tTrain Loss: 0.131348\n",
            "Train Epoch: 34 [30000/50000 (60%)]\tTrain Loss: 0.169263\n",
            "Train Epoch: 34 [35000/50000 (70%)]\tTrain Loss: 0.165867\n",
            "Train Epoch: 34 [40000/50000 (80%)]\tTrain Loss: 0.165446\n",
            "Train Epoch: 34 [45000/50000 (90%)]\tTrain Loss: 0.156378\n",
            "\n",
            "Test set: Test loss: 1.0363, Accuracy: 3711/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 35: lr = 0.1\n",
            "Train Epoch: 35 [5000/50000 (10%)]\tTrain Loss: 0.125246\n",
            "Train Epoch: 35 [10000/50000 (20%)]\tTrain Loss: 0.135407\n",
            "Train Epoch: 35 [15000/50000 (30%)]\tTrain Loss: 0.144539\n",
            "Train Epoch: 35 [20000/50000 (40%)]\tTrain Loss: 0.138661\n",
            "Train Epoch: 35 [25000/50000 (50%)]\tTrain Loss: 0.160513\n",
            "Train Epoch: 35 [30000/50000 (60%)]\tTrain Loss: 0.152620\n",
            "Train Epoch: 35 [35000/50000 (70%)]\tTrain Loss: 0.153099\n",
            "Train Epoch: 35 [40000/50000 (80%)]\tTrain Loss: 0.156809\n",
            "Train Epoch: 35 [45000/50000 (90%)]\tTrain Loss: 0.157004\n",
            "\n",
            "Test set: Test loss: 1.0406, Accuracy: 3724/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 36: lr = 0.1\n",
            "Train Epoch: 36 [5000/50000 (10%)]\tTrain Loss: 0.122620\n",
            "Train Epoch: 36 [10000/50000 (20%)]\tTrain Loss: 0.132234\n",
            "Train Epoch: 36 [15000/50000 (30%)]\tTrain Loss: 0.125227\n",
            "Train Epoch: 36 [20000/50000 (40%)]\tTrain Loss: 0.121809\n",
            "Train Epoch: 36 [25000/50000 (50%)]\tTrain Loss: 0.130352\n",
            "Train Epoch: 36 [30000/50000 (60%)]\tTrain Loss: 0.142285\n",
            "Train Epoch: 36 [35000/50000 (70%)]\tTrain Loss: 0.165704\n",
            "Train Epoch: 36 [40000/50000 (80%)]\tTrain Loss: 0.154284\n",
            "Train Epoch: 36 [45000/50000 (90%)]\tTrain Loss: 0.148222\n",
            "\n",
            "Test set: Test loss: 1.0855, Accuracy: 3703/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 37: lr = 0.1\n",
            "Train Epoch: 37 [5000/50000 (10%)]\tTrain Loss: 0.114685\n",
            "Train Epoch: 37 [10000/50000 (20%)]\tTrain Loss: 0.115210\n",
            "Train Epoch: 37 [15000/50000 (30%)]\tTrain Loss: 0.127225\n",
            "Train Epoch: 37 [20000/50000 (40%)]\tTrain Loss: 0.135864\n",
            "Train Epoch: 37 [25000/50000 (50%)]\tTrain Loss: 0.135122\n",
            "Train Epoch: 37 [30000/50000 (60%)]\tTrain Loss: 0.127306\n",
            "Train Epoch: 37 [35000/50000 (70%)]\tTrain Loss: 0.136936\n",
            "Train Epoch: 37 [40000/50000 (80%)]\tTrain Loss: 0.124245\n",
            "Train Epoch: 37 [45000/50000 (90%)]\tTrain Loss: 0.151759\n",
            "\n",
            "Test set: Test loss: 1.1328, Accuracy: 3691/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 38: lr = 0.1\n",
            "Train Epoch: 38 [5000/50000 (10%)]\tTrain Loss: 0.114746\n",
            "Train Epoch: 38 [10000/50000 (20%)]\tTrain Loss: 0.123364\n",
            "Train Epoch: 38 [15000/50000 (30%)]\tTrain Loss: 0.122854\n",
            "Train Epoch: 38 [20000/50000 (40%)]\tTrain Loss: 0.122912\n",
            "Train Epoch: 38 [25000/50000 (50%)]\tTrain Loss: 0.121634\n",
            "Train Epoch: 38 [30000/50000 (60%)]\tTrain Loss: 0.121266\n",
            "Train Epoch: 38 [35000/50000 (70%)]\tTrain Loss: 0.131018\n",
            "Train Epoch: 38 [40000/50000 (80%)]\tTrain Loss: 0.150621\n",
            "Train Epoch: 38 [45000/50000 (90%)]\tTrain Loss: 0.142927\n",
            "\n",
            "Test set: Test loss: 1.1101, Accuracy: 3704/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 39: lr = 0.1\n",
            "Train Epoch: 39 [5000/50000 (10%)]\tTrain Loss: 0.107782\n",
            "Train Epoch: 39 [10000/50000 (20%)]\tTrain Loss: 0.107625\n",
            "Train Epoch: 39 [15000/50000 (30%)]\tTrain Loss: 0.115074\n",
            "Train Epoch: 39 [20000/50000 (40%)]\tTrain Loss: 0.103155\n",
            "Train Epoch: 39 [25000/50000 (50%)]\tTrain Loss: 0.116401\n",
            "Train Epoch: 39 [30000/50000 (60%)]\tTrain Loss: 0.129545\n",
            "Train Epoch: 39 [35000/50000 (70%)]\tTrain Loss: 0.137338\n",
            "Train Epoch: 39 [40000/50000 (80%)]\tTrain Loss: 0.122843\n",
            "Train Epoch: 39 [45000/50000 (90%)]\tTrain Loss: 0.121869\n",
            "\n",
            "Test set: Test loss: 1.0815, Accuracy: 3715/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 40: lr = 0.1\n",
            "Train Epoch: 40 [5000/50000 (10%)]\tTrain Loss: 0.110780\n",
            "Train Epoch: 40 [10000/50000 (20%)]\tTrain Loss: 0.105309\n",
            "Train Epoch: 40 [15000/50000 (30%)]\tTrain Loss: 0.111585\n",
            "Train Epoch: 40 [20000/50000 (40%)]\tTrain Loss: 0.105118\n",
            "Train Epoch: 40 [25000/50000 (50%)]\tTrain Loss: 0.119037\n",
            "Train Epoch: 40 [30000/50000 (60%)]\tTrain Loss: 0.121300\n",
            "Train Epoch: 40 [35000/50000 (70%)]\tTrain Loss: 0.128607\n",
            "Train Epoch: 40 [40000/50000 (80%)]\tTrain Loss: 0.121774\n",
            "Train Epoch: 40 [45000/50000 (90%)]\tTrain Loss: 0.113976\n",
            "\n",
            "Test set: Test loss: 1.1265, Accuracy: 3723/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 41: lr = 0.1\n",
            "Train Epoch: 41 [5000/50000 (10%)]\tTrain Loss: 0.092417\n",
            "Train Epoch: 41 [10000/50000 (20%)]\tTrain Loss: 0.096462\n",
            "Train Epoch: 41 [15000/50000 (30%)]\tTrain Loss: 0.096647\n",
            "Train Epoch: 41 [20000/50000 (40%)]\tTrain Loss: 0.105379\n",
            "Train Epoch: 41 [25000/50000 (50%)]\tTrain Loss: 0.115041\n",
            "Train Epoch: 41 [30000/50000 (60%)]\tTrain Loss: 0.116845\n",
            "Train Epoch: 41 [35000/50000 (70%)]\tTrain Loss: 0.101711\n",
            "Train Epoch: 41 [40000/50000 (80%)]\tTrain Loss: 0.114233\n",
            "Train Epoch: 41 [45000/50000 (90%)]\tTrain Loss: 0.105273\n",
            "\n",
            "Test set: Test loss: 1.1400, Accuracy: 3683/5000 (74%)\n",
            "\n",
            "\n",
            "Train Epoch 42: lr = 0.1\n",
            "Train Epoch: 42 [5000/50000 (10%)]\tTrain Loss: 0.102077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9dd446c687eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'#training/testing\\nloss_list = []\\nloss_list_test = []\\niteration_list = []\\naccuracy_list = []\\n# execution time checking\\nexecution_time = []\\n\\nPATH = \\'/content/squeezenet_acc.pth\\'\\nbest_model_wts = copy.deepcopy(model.state_dict())\\nPATH2 = \\'/content/squeezenet_loss.pth\\'\\nbest_model_loss = copy.deepcopy(model.state_dict())\\n\\n#PATH = \\'/content/modified_mnist_effb2_pre_original_latest.pth\\'\\n#latest_model_wts = copy.deepcopy(model.state_dict())\\nbest_loss = float(\"inf\")\\nbest_acc = 0.0\\n#set number of epochs \\nnum_epochs = 100\\nfor epoch in range(num_epochs):\\n    #print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\\n    print(\"\\\\nTrain Epoch {}: lr = {}\".format(epoch,learning_rate))\\n\\n    start = timeit.default_timer()\\n    loss_list.append(train(epoch, train_loader, model, error, optimizer, batch_size))\\n    loss_test, accuracy = test(test_loader, model, error, batch_size)\\n    stop = timeit.default_timer()\\n    etime = stop - start\\n    execution_time.append(etime)\\n\\n    loss_list_test.append(loss_test)\\n    accuracy_list.append(accuracy)\\n    iteration_list.append(epoch)\\n    #exp_lr_scheduler.step()\\n\\n    #saving model with best acc \\n    if accuracy > best_acc:\\n      print(f\"Better accuracy at Epoch {epoch}: accuracy = {accuracy}%\")\\n      best_acc = accur...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-26bdf055236f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader, model, error, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print every 100 batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/adabound/adabound.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5f7577cf-27a3-4bd9-fc4d-449d1ca790ed",
        "id": "VVrcdmhRyXSz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        }
      },
      "source": [
        "  # visualization loss \n",
        "plt.plot(iteration_list,loss_list, label='Training loss')\n",
        "plt.plot(iteration_list,loss_list_test, label='Validation loss')\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Squeezenet (Skip&Extra Layers)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# visualization accuracy \n",
        "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
        "plt.xlabel(\"Number of iteration\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Squeezenet (Skip&Extra Layers)\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Mean time per epoch {np.mean(execution_time)} seconds\") # returns time in sec \n",
        "print(f\"Best accuracy: {best_acc}  Best training loss: {min(loss_list)}  Best validation loss: {min(loss_list_test)}\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd3hcxbXAf2fVpVUvLiqWLMlV7nIB\ndyCA6cUUg5NQDSQEEpI8eIQAgUCAR8AQSoDQawymGLCxSXDBgHuRuyzbsiXZliVZvVhaad4fs5JX\nXbK0Wkua3/fdb3fvzJ17di3fM3POmXNEKYXBYDAYei8WVwtgMBgMBtdiFIHBYDD0cowiMBgMhl6O\nUQQGg8HQyzGKwGAwGHo5RhEYDAZDL8coAkOvRUR+EJExbeyrRCShmbYSERnYudIZ2oqIrBOR4a6W\noztjFEEPQkSmiMiPIlIoIsftD7rxrparqxGRFSJySyt9LgaKlVKb7Z+DROQNETkqIsUikioi97Xl\nfkopq1JqfxtlExH5u4jk2Y9PmujzlohU2hVM7bG1jeM3q7BOFRFJF5FzOnPMTuZp4BFXC9GdMYqg\nhyAiAcBXwD+AECAS+AtwwpVyncbcDrzr8PlZwAoMBQKBS4A0J9z3XGAuMAroD7zSTL+n7Aqm9hjV\nGTcXEffOGKcraYPMi4CZItK3K+TpiRhF0HMYBKCU+lApVa2UKldKLVNKpQCIiJuIPC0iuSKyX0R+\nbZ89utvb6836RORhEXnP4fMk+2qjQES2isgMh7ZAEXldRI6ISJaI/FVE3OxtWxvMbFXtta2MuUJE\nHrWvaopFZJmIhLUmj4g8BkwFXrDf74WGP5SIeAJnASsdTo8HPlBK5SulapRSu5VSjWbr9uuniEiG\nwz3rZuH22fw/ReRbu9wrRWSAw+VVQDlwVCl1Qin1bdP/nE0jIteIyAG74kdEZtlXMeEissrerfY3\nv0ZEZohIpojcKyJHgTdFJFhEvhKRHBHJt7+Pao8c9ns3O46IXCUiGxv0v0dEvrC/97L/PR4SkWz7\nb+Zjb2tK5jD7+AWiV7vfi4gFQClVAWwEzmvvdzDYUUqZowccQACQB7wNzAKCG7TfDuwGotErhuWA\nAtzt7enAOQ79Hwbes7+PtI99AXry8DP753B7+2foma0fEAGsA25rQsZ5dhkC2jDmCmAfWsH52D8/\n0UZ5VgC3tPBbDQdKG5z7F7ADuBFIbOIaBSQA5wMZwISGbfb3bwHFwDTAC3gOWO3Qtz9QZO9naUa+\nt4C/tiD/+/Y+ocBh4KKmZLF/ngHYgCft8vjYr7sS8AX8gY+Bz1u4X72/DYfzzY5jv9dxYKhD/83A\nlfb3z6Jn8iH2a78E/taCzH8D/gl42I+pgDiM/TzwjKv/H3bXw+UCmKMT/zG1WeMtINP+H2kR0Mfe\n9h1wu0Pfc2m7IrgXeLfBvZYCvwT6oM1PPg5tc4DlDfpPAY4Bg1ob0/5+BfCAQ9uvgG/acW1LimAy\nekbueM4HuB89s6xCm4VmObQr4H+Bg0BSg2sbKoKPHNqsQDVaAXsA29CmoS+AN7ArA2A1cLHDGBVA\ngcPxtsOYQcAh+1ivNCeL/fMMoBLwbuH3GA3kt9Be72+jreMALwOP2d8PB/LRD3YBSoF4h75nAAea\nkxntA/jC8bs1uPdjwBuu/j/YXQ9jGupBKKV2KaVuUEpFAUno2ed8e3N/9Ey2loPtGHoAcJV9WV4g\nIgXoB3s/e5sHcMSh7RX0ygAAEYkGFqAf1KltGLOWow7vy9AP1bZe2xL56FloHUqb0h5XSo1Dz3QX\nAB+LSIhDt98CC5RS21sZv+53VkqVoGfG/dHmKE+l1HvANUAc8C+7mWcIWhnU8rRSKsjh+KXDmAXo\n2XcS8Pc2fN8cpc0nAIiIr4i8IiIHRaQIWAUE1Zrz2kobxnkbuE5EBPg5+rc7AYSjVxEbHf79vrGf\nb1Jm4P/QynmZaNNmQ0e+P1phGk4Bowh6KEqp3eiZZZL91BH0rLSWmAaXlKL/c9bi6HjLQM/AHR9M\nfkqpJ+xtJ4Awh7YApdRwALvd93NgvlJqSRvHbI3Wrm0tpW6aFk0im2pUShUBj6NNXXEOTVcBl4nI\n3a2MX/c7i4gVbf44DLijlSb2h9wlwEhgPXoVkd/KuLVjjgZuAj5Em0Rao+Hv8XtgMDBRKRWANmOB\nnqm3hxbHUUqtQc/spwLXcdI5n4v2kwx3+PcLVEpZHcauJ7NSqlgp9Xul1ED073aPiJzt0GUo0KbI\nKkNjjCLoIYjIEBH5vYOzLhptollj77IAuEtEokQkGGg4o9oCXCsiHiKSDMx2aHsPuFhEzhPtdPa2\nO/SilFJHgGXA30UkQEQsIhIvItPt174B7FZKPdXgfs2O2Yav29q12UCzcf1KqUrgP0CtjIjIn0Vk\nvIh4iog3cDd6hrnH4dLDwNnA3SJyRwvyXWB3KHsCjwJrlFIZ6Bm/t4g8YleQFrSvZhB6xdMqdtne\nQ5uxbgQiReRXDl1a/O52/NEP4gL7iuehNtzaw/471x7ubRznHeAFoEoptRpAKVUDvAY8KyIR9u8V\nKSLNOntF5CIRSbCvLgrR5rYae5s3MA5ol+Pd4ICrbVPm6JwD7UBdAGShZ/dZaBNNgL3dHe2gywMO\nAL+mvo9gILAWKAG+Rs8033MYfyI6yuY4kGPvE2NvC0TbgzPR/0k3A9fa2xT6IVficExtw5grcLDz\nAzdQ3+na0rVnAKloE9DzzfxeFwJLHD4/AGxHO3KP2+9/pkO7ox8gDm1au6WJtrfQTs1v7d91FRDn\nME4SWnHmo2fGb6AjlnKAWx3GqGzwm+Xa255tIPcou7yJ9s+3o1d/BcDVaHt7ZoPv3t/+/Ursv9Nt\njn8LTfxW6fZ2x+OvbRkHvfKsAf7SYExv9Kprv/033wXcZW9rSubf2eUoRf+d/dmh7SrgU1f/H+zO\nh9h/SEMvQ0Ri0QrBQyllc600rkFEfgDuVPZNZZ005lvoh9gDnTVmd8a+8jkGjFVK7XXSPdYCN6vW\nfTeGZuh2m0sMhs5CKTXZ1TL0Au4A1jtLCQAopSY6a+zeglEEBoPBKYhIOtpxfJmLRTG0gjENGQwG\nQy/HRA0ZDAZDL6fbmYbCwsJUbGysq8UwGAyGbsXGjRtzlVLhTbV1O0UQGxvLhg0bXC2GwWAwdCtE\npNlsAsY0ZDAYDL0cowgMBoOhl2MUgcFgMPRyup2PwGAwdC1VVVVkZmZSUVHRemeDy/H29iYqKgoP\nD482X2MUgcFgaJHMzEz8/f2JjY1F53wznK4opcjLyyMzM5O4uLjWL7BjTEMGg6FFKioqCA0NNUqg\nGyAihIaGtnv1ZhSBwWBoFaMEug+n8m/VaxRBanYxj361k4qqaleLYjAYDKcVvUYRZOaX8frqA6xP\nP+5qUQwGQzvIy8tj9OjRjB49mr59+xIZGVn3ubKysk1j3HjjjezZs6fFPi+++CLvv/9+Z4jMlClT\n2LJlS6eM1RX0GmfxpIGheLpbWLEnh6mJTe6yNhgMpyGhoaF1D9WHH34Yq9XKH/7wh3p9agusWCxN\nz23ffPPNVu/z61//uuPCdlN6zYrA19OdiXEhrEzNcbUoBoOhE0hLS2PYsGFcf/31DB8+nCNHjjBv\n3jySk5MZPnw4jzzySF3f2hm6zWYjKCiI++67j1GjRnHGGWdw7NgxAB544AHmz59f1/++++5jwoQJ\nDB48mB9//BGA0tJSrrzySoYNG8bs2bNJTk5udeb/3nvvMWLECJKSkrj//vsBsNls/PznP687//zz\nuvT0s88+y7Bhwxg5ciRz587t9N+sOXrNigBg+qBw/vr1LjLzy4gK9m39AoPBUI+/fLmDnYeLOnXM\nYf0DeOji4ad07e7du3nnnXdITk4G4IknniAkJASbzcbMmTOZPXs2w4YNq3dNYWEh06dP54knnuCe\ne+7hjTfe4L77Gpbw1quMdevWsWjRIh555BG++eYb/vGPf9C3b18WLlzI1q1bGTt2bIvyZWZm8sAD\nD7BhwwYCAwM555xz+OqrrwgPDyc3N5dt27YBUFBQAMBTTz3FwYMH8fT0rDvXFThtRSAib4jIMRFp\ntnycveD4FhHZISIrnSVLLTMGa5OQWRUYDD2D+Pj4OiUA8OGHHzJ27FjGjh3Lrl272LlzZ6NrfHx8\nmDVrFgDjxo0jPT29ybGvuOKKRn1Wr17NtddeC8CoUaMYPrxlBbZ27VrOOusswsLC8PDw4LrrrmPV\nqlUkJCSwZ88e7rrrLpYuXUpgYCAAw4cPZ+7cubz//vvt2hDWUZy5IngLeAF4p6lGEQkCXgLOV0od\nEpEIJ8oCQHy4lcggH1buyeH6iQOcfTuDocdxqjN3Z+Hn51f3fu/evTz33HOsW7eOoKAg5s6d22Q8\nvaenZ917Nzc3bLamS3Z7eXm12udUCQ0NJSUlhSVLlvDiiy+ycOFCXn31VZYuXcrKlStZtGgRjz/+\nOCkpKbi5uXXqvZvCaSsCpdQqoKUQneuAT5VSh+z9jzlLllpEhOmDw/lxXx6Vthpn385gMHQhRUVF\n+Pv7ExAQwJEjR1i6dGmn32Py5MksWLAAgG3btjW54nBk4sSJLF++nLy8PGw2Gx999BHTp08nJycH\npRRXXXUVjzzyCJs2baK6uprMzEzOOussnnrqKXJzcykrK+v079AUrvQRDAI8RGQF4A88p5RqbvUw\nD5gHEBMT06GbTh8UzgdrD7HxYD5nxId2aCyDwXD6MHbsWIYNG8aQIUMYMGAAkydP7vR7/OY3v+EX\nv/gFw4YNqztqzTpNERUVxaOPPsqMGTNQSnHxxRdz4YUXsmnTJm6++WaUUogITz75JDabjeuuu47i\n4mJqamr4wx/+gL+/f6d/h6Zwas1iEYkFvlJKJTXR9gKQDJwN+AA/ARcqpVJbGjM5OVl1pDBNcUUV\nYx75llumDuS+WUNOeRyDobewa9cuhg4d6moxTgtsNhs2mw1vb2/27t3Lueeey969e3F3P73ibpr6\nNxORjUqp5Kb6u1L6TCBPKVUKlIrIKmAU0KIi6Cj+3h4kxwazMjXHKAKDwdAuSkpKOPvss7HZbCil\neOWVV047JXAquPIbfAG8ICLugCcwEXi2K248fVAET36zm+yiCvoEeHfFLQ0GQw8gKCiIjRs3ulqM\nTseZ4aMfos09g0UkU0RuFpHbReR2AKXULuAbIAVYB/xLKdVsqGlnMn2QCSM1GAyGWpy2IlBKzWlD\nn/8D/s9ZMjTH0H7+RPh7sTI1h6uTo7v69gaDwXBa0WtSTDgiIkwfFM73qTnYqk0YqcFg6N30SkUA\nMGNwBEUVNrZmdt02boPBYDgd6bWKYEpCGBaBlXuMn8BgOJ2ZOXNmo81h8+fP54477mjxOqvVCsDh\nw4eZPXt2k31mzJhBa+Ho8+fPr7ex64ILLuiUPEAPP/wwTz/9dIfH6Qx6rSII9PVgTEwwK4zD2GA4\nrZkzZw4fffRRvXMfffQRc+a06oYEoH///nzyySenfP+GimDx4sUEBQWd8ninI71WEQDMGBROSmYh\nuSUnXC2KwWBohtmzZ/P111/XFaFJT0/n8OHDTJ06tS6uf+zYsYwYMYIvvvii0fXp6ekkJek9reXl\n5Vx77bUMHTqUyy+/nPLy8rp+d9xxR10K64ceegiA559/nsOHDzNz5kxmzpwJQGxsLLm5uQA888wz\nJCUlkZSUVJfCOj09naFDh3LrrbcyfPhwzj333Hr3aYotW7YwadIkRo4cyeWXX05+fn7d/WvTUtcm\nu1u5cmVdYZ4xY8ZQXFx8yr9tLd1/J0QHmD44nL9/m8rqvblcNibS1eIYDKc/S+6Do9s6d8y+I2DW\nE802h4SEMGHCBJYsWcKll17KRx99xNVXX42I4O3tzWeffUZAQAC5ublMmjSJSy65pNm6vS+//DK+\nvr7s2rWLlJSUemmkH3vsMUJCQqiurubss88mJSWFu+66i2eeeYbly5cTFhZWb6yNGzfy5ptvsnbt\nWpRSTJw4kenTpxMcHMzevXv58MMPee2117j66qtZuHBhi/UFfvGLX/CPf/yD6dOn8+CDD/KXv/yF\n+fPn88QTT3DgwAG8vLzqzFFPP/00L774IpMnT6akpARv747vherVK4Kk/oGE+nmyYo/T890ZDIYO\n4GgecjQLKaW4//77GTlyJOeccw5ZWVlkZ2c3O86qVavqHsgjR45k5MiRdW0LFixg7NixjBkzhh07\ndrSaUG716tVcfvnl+Pn5YbVaueKKK/j+++8BiIuLY/To0UDLqa5B10coKChg+vTpAPzyl79k1apV\ndTJef/31vPfee3U7mCdPnsw999zD888/T0FBQafsbO7VKwKLRZg2KJyVqTnU1CgslqZnEQaDwU4L\nM3dncumll/K73/2OTZs2UVZWxrhx4wB4//33ycnJYePGjXh4eBAbG9tk6unWOHDgAE8//TTr168n\nODiYG2644ZTGqaU2hTXoNNatmYaa4+uvv2bVqlV8+eWXPPbYY2zbto377ruPCy+8kMWLFzN58mSW\nLl3KkCEdS5fTq1cEoHcZHy+tZPvhQleLYjAYmsFqtTJz5kxuuummek7iwsJCIiIi8PDwYPny5Rw8\neLDFcaZNm8YHH3wAwPbt20lJSQF0Cms/Pz8CAwPJzs5myZIlddf4+/s3aYefOnUqn3/+OWVlZZSW\nlvLZZ58xderUdn+3wMBAgoOD61YT7777LtOnT6empoaMjAxmzpzJk08+SWFhISUlJezbt48RI0Zw\n7733Mn78eHbv3t3uezakV68IAKYmhiECK/bkMDKqZ0UCGAw9iTlz5nD55ZfXiyC6/vrrufjiixkx\nYgTJycmtzozvuOMObrzxRoYOHcrQoUPrVhajRo1izJgxDBkyhOjo6HoprOfNm8f5559P//79Wb58\ned35sWPHcsMNNzBhwgQAbrnlFsaMGdOiGag53n77bW6//XbKysoYOHAgb775JtXV1cydO5fCwkKU\nUtx1110EBQXx5z//meXLl2OxWBg+fHhdtbWO4NQ01M6go2mom+LSF1bj7mZh4R1nduq4BkNPwKSh\n7n60Nw117zENVRTBxregCcU3fVA4mw/lU1hW1fVyGQwGg4vpPYpg99fw5d2Q9p9GTdMHR1Cj4Ps0\ns7nMYDD0PnqPIki6EgKiYHXjkgejo4Pw93bnx315LhDMYDj96W4m5N7Mqfxb9R5F4O4JZ/waDv4A\nGevqNblZhCF9/dmb3fEdegZDT8Pb25u8vDyjDLoBSiny8vLavcnMaVFDIvIGcBFwrKmaxQ79xqML\n2FyrlDr1hCBtYewvYNVTsHo+zPmgXlNChD/fbD/i1NsbDN2RqKgoMjMzyckxptPugLe3N1FRUe26\nxpnho28BLwDvNNdBRNyAJ4FlTpTjJF5WmDAPVj4JOXsgfHBdU0KElfyyKvJKThBq9WphEIOhd+Hh\n4UFcXJyrxTA4EaeZhpRSq4DjrXT7DbAQ6LocDxNuA3cf+OG5eqcTInTK2r3HSrpMFIPBYDgdcJmP\nQEQigcuBl9vQd56IbBCRDR1envqFahNRygIozKo7XasI0owiMBgMvQxXOovnA/cqpVqtFamUelUp\nlayUSg4PD+/4nc/4NagaWPNS3an+gd74eboZRWAwGHodrlQEycBHIpIOzAZeEpHLuuTOwQNgxGzY\n8CaUaeuViBAfYWVfjlEEBoOhd+EyRaCUilNKxSqlYoFPgF8ppT7vMgEm3w1VpbD+9bpTCeFWsyIw\nGAy9DqcpAhH5EB0WOlhEMkXkZhG5XURud9Y920Wf4ZB4Lqx9GSp1Gbr4CCtHCisorjCpJgwGQ+/B\naeGjSqm2FRTVfW9wlhwtMuV38OYs2PI+TLi1zmG8L6eU0dEmE6nBYOgd9J6dxU0RcwZETYAfn4dq\nG4kmcshgMPRCerciENGrgoJDsOMzYkJ88XSzGEVgMBh6Fb1bEQAMOh/Ch8AP83G3CLFhvqQdMzmH\nDAZD78EoAotFRxBlb4e0/5AQYSKHDAZD78IoAoCk2WDtC5veJiHCn0PHy6ioqna1VAaDwdAlGEUA\nOkX14PNh3woSwzypUZCeV+pqqQwGg6FLMIqglsTzoLKYkbadAOzNNuYhg8HQOzCKoJa4aeDmSWTO\nKkRMCKnBYOg9GEVQi5cVYqfgvu9booN9STM5hwwGQy/BKAJHEs+DvDQmBxeyz6wIDAZDL8EoAkcG\nnQvAWW5b2J9Tiq261QzZBoPB0O0xisCRkIEQmsiIsjVUVteQkV/uaokMBoPB6RhF0JBB5xFxfCO+\nVBiHscFg6BUYRdCQxHOx1FQyxbLNKAKDwdArMIqgITFngKc/F3gbRWAwGHoHRhE0xN0T4mcyTbaQ\nll3kamkMBoPB6TizQtkbInJMRLY30369iKSIyDYR+VFERjlLlnYz6DxCqnNxz92JUsrV0hgMBoNT\nceaK4C3g/BbaDwDTlVIjgEeBV50oS/tI+BkAk2wbOFpU4WJhDAaDwbk4TREopVYBx1to/1EplW//\nuAaIcpYs7ca/DyWhIzjLbfNp6yfILqrgmWV7qK4xKxaDwdAxThcfwc3AkuYaRWSeiGwQkQ05OTld\nIpAMOo8xksahjIwW+9XUKF5btZ8N6c3qPKfw8op9PP9dGjsOF3bpfQ0GQ8/D5YpARGaiFcG9zfVR\nSr2qlEpWSiWHh4d3iVy+wy/AIgqP9O9a7PftrmweW7yL2f/8ifsWppBfWul02SqqqvlscxYA+0xO\nJIPB0EFcqghEZCTwL+BSpVSeK2VpiPQfQ74lmMhjq5rto5TixeVpxIT4Mm/aQD7emMnZz6zk4w0Z\nTnUyf7szm8LyKgD2HTN1EwwGQ8dwmSIQkRjgU+DnSqlUV8nRLBYL+wInMaJiA1Tbmuzy/d5cUjIL\nuWNGPPdfMJSv75pCXJgff/wkhWteXcPebOfUPl6wIYPIIB/iwvxOWx+GwWDoPjgzfPRD4CdgsIhk\nisjNInK7iNxu7/IgEAq8JCJbRGSDs2Q5VQoiZxJAKUVpPzTZ/sJ3afQN8OaKsZEADOkbwMe3ncGT\nV44gNbuYWc99z5Pf7Ka8svPKXmbml7E6LZfZ46JIiLAa05DBYOgw7s4aWCk1p5X2W4BbnHX/zsBz\n8DlUbXOjZNtiAgZPr9e27sBx1qUf58GLhuHl7lZ33mIRrhkfwzlD+/C3Jbt5ecU+tmUW8t4tEztF\npoUbtW9g9rgo3l97iBV7jmGrrsHdzeXuHoPB0E0xT48WiIvsx/qawfim/6dR2wvL0wj182TOhJgm\nrw21evH0VaP45RkDWJ9+vFN8BjU1io83ZjA5PozoEF/iw/2oqlYcOl7W4bENBsNpilKQuxc2vw8Z\n651yC6etCHoCkUE+fCBjObPkPSjIgKBoAFIyC1iVmsP/nD8YH0+3FseIj7BywlZDTvEJIgK8OyTP\nT/vzyMwv54/nDQYgIcIKwL6cUgaGWzs0tsFgaCMHVkFuKgy9FKxOiGI8UQxZG/VDP3MdZK6HcvuW\nq4m3Q/T4Tr+lUQQtYLEIB0ImQ8F7sHcZjL8ZgBeXpxHg7c7PJw1odYzoEF8ADh0v67Ai+Pf6DAK8\n3TlveF+Auof/vpwSfkafDo1tMBhaoaYaVvwNVv2f/rzkXkg8F0Zfr1/dPTs2/q6vYOUTkL0DlL0o\nVvgQGHIRRE+AqAkQNqhj92gGowhawbvPELIK+hCZuhTG30xqdjFLd2Rz11kJ+Ht7tHp9dLBWBBn5\nZSTHhpyyHIVlVXyz4yjXjo/G20OvQgJ9PAj39zKRQwaDsyk7Dp/eCmn/gdFzYeJtsP0T2PoR7FkM\nvqEw4moYfR30G9m+sW0nYNmfYd0rEDEMpv1RP/SjxoFPsHO+TwOMImiFhD7+LNsxmhv3LoW/ReOv\nQnjfK4DkouHwXTQE9Ncmo4EzwdLYTBQV7APAobyOVTv7YmsWlbYark6Ori9fuIkcMhicypGt8O+5\nUHwULpoP424AEf3AP+tB2PcdbHkfNrwOa1+GPiPgjF/BiKvArZXJYt4++ORGfY9Jv4JzHgZ3ry74\nUvUxiqAVEiKsPGC7jPPPHItfxTG2bk5hREApXgdXwPajgN0JPP4WuPDvja739nCjT4AXGfkdc+gu\n2JDBsH4BJEUG1jsfH+HHoi2HUUohIh26h8FgaMCWD+Cr3+kZ/43f6Fm6I27uutb5oHP1qmH7Qtjw\nJnx+Byx/HM68C8bMBU/fxmNvXwiL7tYTyGs/gCEXds13agKjCFohIcKfXAL5sc9c1qcf51OVxerb\nZkKAN1RXQUk2rHoa1r+ul4YxjcNEY0J8OxTZs+NwIduzivjLJcMbtcWHWymqsJFbUkm4f9fPJAyG\nHontBHxzH2x4A+Kmwew3wS+s5Wt8Q2DCrXpSuHcZfP8MLPkjrHwSJt2hz/sEQVW5HnvjW9oENPt1\nCGo6+rCrMOGjrTAg1Bd3i7A6LZeFmzK5Jjn6pNPXzQMCo+Dcv+rXL+8CW+NcQ9HBvmR2QBF8vCET\nTzcLl47u36itNnLI+AkMhk6i4BC8eYFWApPvhrmfta4EHBGBQefBzUvhxiXQfwx89yjMHwHLHoDX\nztZKYPLdcONilysBMIqgVTzcLMSG+fHZ5iyUgtumD2zcycsKFz4DObvhh/mNmqNDfDlSVEGlrabd\n969NMHfu8D4E+TaOSoh3iBwyGAwdZPfX8M+pkLMHrn4HfvaINv+cKgPOhLmfwG3fQ8I58NOLUHIU\nrv/EPnbrASddgTENtYGEcCtpx0q4bEwkUcFN2PpA2wiHX6FDy4ZfDmGJdU3RIb4oBVkF5cSF+bXr\n3rUJ5q4ZH91ke98Ab3w93YwiMBg6gu0EfPuQdvb2Gw1XvQkhTUz6TpV+I/WYRY9rf4F3YOvXdCFm\nRdAGhvYLwCJwx4z4ljvOehI8fODLu6Hm5Ow/xr6XIOMUzEO1CeYmxze9NLVYhIHhJvmcwXDKHN8P\nr5+rlcDEO+DmZZ2rBBwJ6HfaKQEwiqBN3Dw1jkV3TqkzwzSLNUL7Cw7+AJvfqTsdHWIPIW2nInBM\nMGexNB8RlBBuZX+OSUdtMLSb7Z/CP6dB/gG45n2Y9YRLwjddjVEEbcDq5d4obLNZxvwcYqfCsgd1\n3DHQx98bTzdLu0NIP9mYCcodXjAAACAASURBVOgEcy0RH24lq6Ccssqm02UbDD2Smmo4fgBKjmnT\nTluwVUJxtvYBfPU7HcMfMQRuXw1DL3KuvKcxxkfQ2YjoTScvn6m3oF/9NhaLEBXs027T0DfbjzIh\nNqQuTUVzxNsjh/bnlLZdYRkM3Q2ltBln/3LYv0Ln/KlwKNXq7q3NLrWHVwBUV+o8PeUF+rWqwcr5\nzLvg7AdPG6etqzCKwBmEJcD0P8J3f4U9S2DwLKJDfMk43vbdxVXVNezLKeGmKXGt9j2ZfK7EKAJD\nz0EpKD4Ch9acfPgXHNJtgdEw9BKIGq8f9hWFJ48TRfb3BeDmpcMz+43S6Rp8gvSrd5DO29PedBA9\nFKcpAhF5A7gIOKaUSmqiXYDngAuAMuAGpdQmZ8nT5Zx5t7Y/fv17iJ1CdIgPWzIK2nx5em4pVdWK\nIX39W+07INQXi8A+4zA2dEdsJ/RMPzdVp1vO3XvyfaW9yp9XgN7YdeZdOp1LaLxefRs6BWeuCN4C\nXgDeaaZ9FpBoPyYCL9tfewbunnDxczoa4b+PEhNyG4XlVRSWVxHo0/oydI+9zOWgPq0rAi93N2JC\nfNlnHMaG050TJXB0m86tU3vk7AblUMUvIFKHX4+eY5+1j4L+YzsWz29oEWdWKFslIrEtdLkUeEfp\nii1rRCRIRPoppY44S6YuJ3oCJN8E6//FkHMuAXQIaWAbzDepR4uxCK1HKtlJiLCaEFKD6zi0Fgoz\nwFahUyjYTuj3ta9Fh/VDPy+NuvxcfuE6Zn/QeRAxVD/8QxPAq/XJj6FzcaWKjQQyHD5n2s/1HEUA\nMPN+SFnA6NTngBvIzC9rkx1/T3YxsWF+dSmnWyM+3Mqq1FyqaxRuLYSaGgydSk2NTp+w+plmOoh2\n4vqG6pn9iKv0a79R4N/XmHdOE7rFWktE5gHzAGJiXJ+Xo134hcGU3xLw3aOMl0kcOj6kTZelZpe0\nyT9QS3y4lcrqGjLzyxgQ2r7dywbDKWE7AZ//SuflH3cDnPEbHYPv7n3y1c3DPOy7Aa7cR5AFOOZN\niLKfa4RS6lWlVLJSKjk83Aml4ZzNpF+Bf38e9PqAjLzWQ0jLK6tJzyttk3+glvgI/fA35iFDl1B2\nHN69XCuBcx7WIdNhCbo2hzUcvAO0n8wogW6BKxXBIuAXopkEFPYo/4Ajnr5w1p8YQRp9sr5ptXva\nsRKUgsHtXBGAST7X6ykvqB9b7wzy0+GN83Qt3Stfhym/Mw/8bo4zw0c/BGYAYSKSCTwEeAAopf4J\nLEaHjqahw0dvdJYspwWj5pC15Gkuz/sX2O5usb5peyKGagny9STM6sm+YyZyqFehlI66Sf0GUpdB\nxhoQN0g4G5KuhMGzOtf5mrURPrhG1+L4+ecQO7nzxja4DGdGDc1ppV0Bv3bW/U87LG6sjv0N16Te\nQ83617GccUezXVOzi/F0txAb2vKO4oYMDLeSZlYEPZ+qCkj/HlKX6qPQvsmq7wiYco+O0tnxmVYO\n7t46KifpSl1g3cPn5Dg1NVCaA0WZUJipI3ss7tqx6xeu/Vu+YbrgisUNdi+GhTfrths+gXDnFFI3\ndD3dwlncU6iMO5vVu4Zz5sqnYMx1zWYh3HO0mIRwK+5u7bPcJURY+TrliClb2R04UQJuni2uDOtR\nXqAf+ru/hLT/QlUZePjCwBkw9R79kA+MPNn/Z49C5jpdDnHHZ7DzC/C06jxYJ4r1w7/osN6V2yqi\nlUF5vg73vO7fOsGiocdgFEEXEhPqx99s1/F1xZ9g9Xw456Em+6VmFzNpYGi7x48Pt1JYXsXx0kpC\nrb0vg2K3oKYa1rwE/30UUNAnSVew6j9av4YPOZn3pugI7Pkadn2lVwA1NvDvB6PmwOALIHYKeHg3\nfR+LBWIm6eP8JyB9tVYKB3/UM/3IZBgWqVM1BERqJRIQBcq+SijLhVL7UZarz3kFwPT/AU8TldbT\nMIqgC4kO9mGHiuNQ5EXErHlJ1zB1nMUBheVVHCmsaJd/oJb4cP0fdF9OqVEEpyPH98Pnv4ZDP+oH\neWgCHN4M2z6GDa/rPu7eWjkAZG3Qr6EJcMadMPRivcPW0s4YD4sbDJyuj7Zg7YaReYYOYRRBFxIZ\n7IMIfNtvHjcfXQbLH4PLXqrXZ6/dUTy4b9t2FDviWL94QlxIxwU2dA5K6fq3y/6sbfCX/RNGXXsy\n0qamRufDP7zZfmzRdv6ZD+iHf/hgE5VjcCpGEXQhXu5u9A3wZkdZIEy8DX58Qe8x6HsyJ9+pRAzV\n0j/QB28PiwkhPZ0ozIJFd8K+73SytEtfgMAG9SUsFp1ELTQeRsx2jZyGXk2b1pgiEi8iXvb3M0Tk\nLhEJcq5oPZPoEF8yj5fD1N9rZ/F/6vsJUo8W4+fpRmSQTzMjNI/FIgwMsxpFcDpQXQVb/w0vnaHT\nKF/4d/j5Z42VgMFwGtDWFcFCIFlEEoBXgS+AD9D7AAztIDrYlx/ScnVO9Gl/hGV/gq//AGf9CXyC\n2X20mEF9/U856ic+wsrmQ/mdLLWhEZWlsHORjuEvy9U7bUtzoSxPf67d1BU9SZv/Qlupd20wuJC2\nKoIapZRNRC4H/qGU+oeIbHamYD2VmBBfPi2uoKKqGu8J8/QuzQ2vw45PUWc/xN6jfTg3qf8pj58Q\nbuWrlMOUV1bj49m2hHWGdpCzB9a/Dls/ghOFOgTUN0zH3vuG6Ogf31B9LmQgJF2hnbUGw2lMWxVB\nlYjMAX4JXGw/17tru50i0SE+KAVZBeU6LcSFT8PYX8DiPyJf3sUbNQPJ8PkLcGqVk+Ij/FAKDuSW\nMqx/QOcK31uxVer4/fVvwMHV+uE/7FKdYjzmDOPINXR72hqHdiNwBvCYUuqAiMQB7zpPrJ5LjL3+\ncL36xf1Gwk3fsOfMZ+gnx7l43c/hi19DSU67x6/NOdTjdhhXFLa9QHlnUXYc/vsIPDsMPrlJ59s/\n52H43U648l8w4EyjBAw9gjatCJRSO4G7AEQkGPBXSj3pTMF6KtFNKQIAEVb7nsUzJwJZP3UTvhtf\ngZ1fwsirISRO112tPbyDmn0AxYX5IT2lbGVlKez+GlL+DfuW63Oh8RAxTB997K/BsZ1rfqmpho1v\n6Tz7FYWQeB6Mvxniz25/DL/B0A1okyIQkRXAJfb+G4FjIvKDUuoeJ8rWIwm3euHlbiEjv3Eh+9Sj\nxXj7BeJ74eMw4Qb49s+w9UOobPBQ9wrQCiEkDgZMhrjpusKTCN4ebkQH+3Z95JBSsOV9+P7vOhdN\n+GAIG6x3yoYP1tEybZk9V9vgwApIWaB31FaVQmAMTL5Lx+Bn79SVrnZ+QV2lK3cfXd0qZGDjo73F\nTw6thcV/gKMpOh3DrCehz/BT+UUMhm5DW30EgUqpIhG5BV1e8iERSXGmYD0Vi0WICvbhUBN1CfZk\nF5/cPxA+SOd0UUrneCk41Pg4uh12fan7+4Xr4t5x05gUFMS2Y13ooCw7Dl/9Vj+cI8eBxQP2LIFN\nDuWqPfz0d7L21UVLPHz0LlrH19JcnRen9Jhe9Yy8GkZeA9ETG8/EK0t1xE72Tji2S78/mqJ/D8f6\ntx6+WiH0H6Pt+TGT9OeGyqH4KHz7EKR8pFMuzH4Dhl9hTD+GXkFbFYG7iPQDrgb+5ER5egUxIb5k\n5NdXBDU1ir3ZxVyVHF2/s9gTftVGpDSkIAMOrIIDK2H/Sti+kKeATBWG+le8DkMVARq8+veD6ffq\nYiId4cAq+PQ2nYvmZ4/oKlW1D+3SPMjdox/SOan6tSjLXte2AmzlJ19rbNoJO+g8/fBPPFcrjObw\n9NNKJ3Jc/fPVVdqWf3w/HD+gj9xUrSA2291afhH2PDxnQMxEnX9nxZNQfULv75j6e5NPx9CraKsi\neARYCvyglFovIgOBvc4Tq2cTHeLLhoP1Y/2zCsopraxu/47ioGgYc70+lILcvWxY8TlHU/5DmHji\n7S76PNhflX7ds0TPvifephOJNZMJtVlslTpFxg/Pabv9nG/1rNsRv1DwO1M7VVuj2qZlc+tgMJqb\nx0mzkCM1NVopHfpJb/A6tAZ2LTrZnngenP83E+9v6JW01Vn8MfCxw+f9wJXOEqqnExPiS3GFjcKy\nKgJ99YNvz9FTzzFUhwiED0KNv4U7Nw7nzSnjmTm4mXTBJcd0RMxPL+qY+LMfhDFz2+Z0zd2r89If\n2apr1Z73eMdn0G5OznZisWg/SsRQHfYJOg3zoTU67r+tCdkMhh5IW1NMRInIZyJyzH4sFJFW98qL\nyPkiskdE0kTkvibaY0RkuYhsFpEUEekVO5WjgnXk0CGHyKGO5BhqSF3ZypYih6wROu/NvOU6u+WX\nd8FrM+HgT437lh3X5ze+BUvuhVemaR/FNe/Bxc91XzNKQH+94csoAUMvp63TsDfRKSWusn+eaz/3\ns+YuEBE34EV7n0xgvYgssoei1vIAsEAp9bKIDEOXr4xt1zfohtTtJcgvY0SUNsmkZhcTGeSDv3fH\n9+mF+HnSP9Cb1Wm53DJ1YMud+4+Bm77Rueq/fRDePF9vlvIJ1jP/nD06ZUIt7t46edpFz0JAvw7L\najAYXE9bFUG4UupNh89vichvW7lmApBmNyMhIh8BlwKOikABtdtfA4HDbZSnWxMdohPK1VsRHC1m\nUJ8OmIUaMDs5mn98t5eM42V1exeaRURnvRw8C354Xtv9Pbx1+OeQCyBskD0UdJAO5TSx9AZDj6Kt\niiBPROYCH9o/zwHyWrkmEshw+JwJTGzQ52FgmYj8BvADzmlqIBGZB8wDiImJaaPIpy/+3h4E+3rU\nbSqrqq5hf04p0wd3XkGQa8dH88J3e/lg3SHuPX9I2y7y9IOZ/6udx2IxoZMGQy+hrVO7m9Cho0eB\nI8Bs4IZOuP8c4C2lVBQ6k+m7ItJIJqXUq0qpZKVUcnh4z6ieFB3iW7ciOJhXSmV1DYM7wT9QS/8g\nH84e2ocF6zOotNW072KLm1ECBkMvok2KQCl1UCl1iVIqXCkVoZS6jNajhrIAx6D4KPs5R24GFtjv\n8RPgDYS1SfJuTnSIL5n23cV7jmqnbmc4ih25fmIMeaWVfLPjaKeOazAYehYdMfa2ll5iPZAoInEi\n4glcCyxq0OcQcDaAiAxFK4L2Z1rrhkQH+5KZX0Z1jWJPdjEWOVlqsrOYlhhOTIgv76852KnjGgyG\nnkVHFEGLtgOllA24E70RbRc6OmiHiDwiIpfYu/0euFVEtqL9DzcoVbv7qWcTE+JLVbUiu6iC1KPF\nxIb64e3RuWkhLBbhuokxrD1wvK4WssFgMDSkI4qg1Qe2UmqxUmqQUipeKfWY/dyDSqlF9vc7lVKT\nlVKjlFKjlVLLOiBPt6I2cijjeBmp2cUM7tu5ZqFarhoXhaebhffXHnLK+AaDofvToiIQkWIRKWri\nKAZOvYyWgWj7prLUYyWk55V2un+gllCrF7NG9GXhpkzKKm1OuYfBYOjetKgIlFL+SqmAJg5/pZST\ncwL0bPoH+WARWLH7GDUKp60IAK6fOIDiChtfbm19m0Z5ZTXPfJtKVkHjNNkGg6FnYnYGuQhPdwv9\nAn1YnaZ37TprRQAwPjaYQX2srZqHbNU1/ObDzTz/373M/zbVafJ0NesOHKequp0htAZDL8IoAhcS\nHeLDCVsNnm4WYkNb2f3bAUSE6ycOICWzkJTMgib7KKX48xc7+M+ubOLC/Pgq5QhFFVVOk6mrSM8t\n5epXfuKzzQ0jlw0GQy1GEbiQWj9BfIQVdzfn/lNcPjYSHw833l/T9Krghe/S+HDdIe6YEc/8a0ZT\nXlXNF1u6f8aP2kptu4+YqCmDoTmMInAhtcnnBndijqHmCPD24LIx/fliaxaF5fVn+gs2ZPD3b1O5\nYkwk/3PeYEZGBTKsXwAfrj1Ed4/mTbdXgtt7zCgCg6E5jCJwIbXJ4AY50VHsyPUTB1BRVcNnmzLr\nzi3fc4z//XQbUxPDeOLKkYgIIsKcCdHsPFLEtqzCLpHNWRzMKwUgraWU3AZDL8coAhdS6yAeHRXU\nJfdLigxkVHQQ79ln+lszCvjVe5sY0tefl+eOw9P95J/DpWMi8faw8OG6jBZGbEzJidMrRLV2RXCk\nsILiHuDzMBicgVEELmRY/wBW/GEGZyZ0XXql6yfGkHashI83ZHLTW+sJtXry5o3jsXrVjwYO8Pbg\nopH9WbQlq80P92+2H2XMI8v4bne2M0Q/JQ7mleJv/25mVWAwNI1RBC4mNqxrq3tdPLI/Ad7u/M/C\nFGqU4u2bJhDh791k3zkTYiitrG7T/oOSEzYeXrSDqmrFE0t2U13jet9CVXUNmfnldem99xpFYDA0\niVEEvQwfTzeunzQAbw8L//rl+Lqylk0xNiaIwX38+Whd6+kpnv/vXo4WVTBv2kBSs0v41MEP4Sqy\n8suprlFMTQzD091iVgQGQzMYRdAL+eO5g1l7/zmMGxDcYj8R4doJ0WzNLGTH4eadxqnZxbyx+gDX\nJEfzv7OGMCo6iGe/TaWiqrqzRW8X6XZH8cBwKwPD/IwiMBiawSiCXojFIgT6tK028uVjIvFyt/BR\nM05jpRQPfL4dq7c7984agohw7/mDOVxYwbs/uTb99UG7o3hAqC+JffxNCKnB0AxGERhaJMjXkwtG\n9OPzzVlNJq37bHMW6w4c597zhxDi5wnAmfFhTBsUzosr0hrtWehK0vNK8fV0I9zqRWKElcz8cpN4\nz2BoAqMIDK0yZ0IMxSdsfJ1ypN75wvIqHl+8i9HRQVyTHF2v7X/OG0xBWRWvrNzXlaLW42BeGQNC\n/RAREiOsKAX7c0pdJo/BcLriVEUgIueLyB4RSROR+5rpc7WI7BSRHSLygTPlMZwa42ODiQ/348MG\nTuNnlu3heGklf70sCYulfp2ipMhALh3dnzd+OEB2UUVXiltHel5pXQ6nRPvubWMeMhga4zRFICJu\nwIvALGAYMEdEhjXokwj8LzBZKTUc+K2z5DGcOnqncQybDhWw56h+kG7PKuTdNQf5+aQBJEUGNnnd\n7382mOoaxXP/3duV4gJQXaPIOK5XBAADQv1wtwh7s43D2GBoiDNXBBOANKXUfqVUJfARcGmDPrcC\nLyql8gGUUsecKI+hA1wxVlc6+3DdIWpqtIM4xM+Le84d3Ow1MaG+XDchhn+vz6hL/tZVHC4op6pa\n1a0IPNwsxIb5mb0EBkMTOFMRRAKOoSaZ9nOODAIGicgPIrJGRM53ojyGDhDi58l5SX35dFMmb/+U\nzpaMAv504ZBWo4/uPCsRL3cLf1+2p2sEtXMyYujkhr3ECCv7jCIwGBrhamexO5AIzADmAK+JSKPE\nOyIyT0Q2iMiGnJycLhbRUMuc8dEUVdh45KudTIwL4bLRDfV6Y8L9vbh16kAWbzvKloymayE4g9o9\nBLFhJ+s8JEZYSc8r5YTNtfsbDIbTDWcqgizAMZQkyn7OkUxgkVKqSil1AEhFK4Z6KKVeVUolK6WS\nw8PDnSawoWUmDQwlNtQXNxEevSwJEWn9IuDWaQMJ9fPkySW7uyyt9cG8UrzcLfRxSJ+R0MefGgUH\nck3kkMHgiDMVwXogUUTiRMQTuBZY1KDP5+jVACIShjYV7XeiTIYOYLEIT80exfNzxrSrtKbVy507\nz0rgp/15rNqb60QJT5KeV0ZMiG+9aKbECHvkkHEYGwz1cJoiUErZgDuBpcAuYIFSaoeIPCIil9i7\nLQXyRGQnsBz4o1Iqz1kyGTrOhLgQLhjRr93XXTcxhsggH15anuYEqRpzMK+0nn8AIC7MD4uY5HMG\nQ0PcW+9y6iilFgOLG5x70OG9Au6xH4YejJe7G9eMj+aZb1PJzC8jKth5NZprahQH88qYlljfjOjt\n4caAUD/SzF4Cg6EernYWG3oRl4/RzmVn10LOLq7ghK2GAU2k+E6IsBrTkMHQAKMIDF1GdIgvE2JD\nWLgp06lO4/RcHTpau4fAkYQIKwdyS6mqrnHa/Q2G7oZRBIYu5YqxkezPKSUl03m1kGvrFMeGNl4R\nJEZYsdlNR67k+7055JdWulQGg6EWowgMXcqsEf3wdLfw2eaGkcSdR3peGR5uQr/AxpXXEiN0tJMr\n/QQFZZX84o11PP9d16feMBiawigCQ5cS6OPBz4b1YdHWw04zzxzMKyU62Bd3t8Z/3vERepXgSj/B\njsNFKAXfd1EorcHQGkYRGLqcK8ZEcry0kpV7nLNLXKefbjoqydfTnahgH5eGkG7P0maxtGMlHC4o\nd5kcBkMtRhEYupxpg8IJ9fPk082dX9dYKdXkHgJHEiOsrlUEh4vwctf/9VabVYHhNMAoAkOX4+Fm\n4eJR/fnPrmOdXsEst6SS0srqJiOGakns48++nBKqa7om3UVDdhwuZNqgcCL8vfg+zSgCg+sxisDg\nEq4YG0mlrYbF24603rkd1EYMNbWHoJaECCuVthoyjnd95FDJCRsHcksZERnI1MRwVu/NocZFCslg\nqMUoAoNLGBEZSEKElU83tW4eKj1h47cfbW6TGSU9r3YPQcuKALSNvqvZdUQ7ipMiA5g2KIz8sip2\nHC7qcjkMBkeMIjC4BBHh8jGRrE/Pb3FmXlOj+N2/t/D5lsO8sLz1cMuDeaW4WYTIIJ9m+9QqAlf4\nCWodxUn9A5mcEAbAqr0mtbrBtRhFYHAZl42JRIQW9xQ8tXQPy3ZmM6SvP2sPHOdYccv1j9PzyogM\n8sHTvfk/7QBvD/oGeLukfvH2rCLCrF5EBHgTZvVieP8AvjeKwOBijCIwuIzIIB8mxYXyaTMpJz7e\nkME/V+7juokxPD9nDErB0u1HWxxTRwy1ntAusY/VJaahHYcLSYoMqPs8NTGcjQfzKT1h63JZDIZa\njCIwuJQrxkaSnlfG5gbVy9YdOM79n21jckIof7lkOIP6+JMQYeXrFpzLSikO5Ja26B+oJSFCK4Ku\ndNRWVFWz91gJSf0D685NSwyjqlqx9oDJvm5wHUYRGFzKrBH98Paw1HMaH8or47Z3NxAd7MtL143D\nw75D+MIR/Vh34Dg5xSeaHKugrIriClvbVgQR/pRVVnO4sOUNXZ0ZYrrnaDHVNareimBcbDDeHhZW\npZowUoPrMIrA4FKsXu6cN7wvX6UcodJWQ1FFFTe9vZ4aBa/fMJ5AX4+6vheO7EeNgm92NG0eSm8h\n2VxDEvu07jD+YksWYx5Z1mmbvrYf1o7i4Q4rAi93NybGhRo/gcGlOFURiMj5IrJHRNJE5L4W+l0p\nIkpEkp0pj+H05PIxkRSUVfGfXdnc+cFm0nNLeXnuWOIa7AWoMw+lNF3PoDajqGPB+uZICNeKYF8z\nimBrRgF//CSFogobv1uwhdySplch7WF7VhGBPh5EBdePaJqaGMa+nFKyTLoJg4twmiIQETfgRWAW\nMAyYIyLDmujnD9wNrHWWLIbTmykJYYRZvfjjx1tZlZrDo5clcWZ8WJN9L2jBPJSeV4oIbap+Fuzn\nSZjVs8nkc8eKKpj37gbCrV58cOtEisqr+P2CrR32J+w4XMjw/gGISL3z0wbpSmqrzarA4CKcuSKY\nAKQppfYrpSqBj4BLm+j3KPAk0HJcoKHH4u5m4bLR/SmtrObmKXHMmRDTbN8LRzRvHjqYV0b/QB+8\nPdzadN+ECGujENKKqmrmvbuR4gob//plMmfGh/Hni4axMjWHf63e374v5kBVdQ27jxaTFBnYqC0x\nwkqfAC9WmbxDBhfhTEUQCWQ4fM60n6tDRMYC0Uqpr1saSETmicgGEdmQk2NmTT2R35yVyP/NHsn9\nFwxtsd+gPlYSIqwsTmkcPZTextDRWhIj/Nl7rKQudFUpxZ8+286WjAKeuXoUQ/tpp+71E2OYldSX\np77Zw9YG0U1tJe1YCZW2Gob3D2jUJiJMTQznh7Rcl+U/MvRuXOYsFhEL8Azw+9b6KqVeVUolK6WS\nw8PDW+tu6IYE+npwVXI0bhZpsZ+IcMGIfqw9kNfIPKTTT7fuKK4lsY+V4gobx+zjvL76AAs3ZfLb\ncxI5P6lfvXs+ccVI+gR485sPN1Nc0f5EeXU7iptYEYD2ExSUVdX1Mxi6Emcqgiwg2uFzlP1cLf5A\nErBCRNKBScAi4zA2tEZT5qHC8iqOl1a2mHW0IXWpJrJLWJmaw+OLdzErqS93nZXYqG+grwfPzxlN\nVkE593+2vd01l3ccLsLP0424ZhTVFHu6CRM9ZHAFzlQE64FEEYkTEU/gWmBRbaNSqlApFaaUilVK\nxQJrgEuUUhucKJOhBzCoj5X4cL965qFD9oih9pqGAJbtPMqdH2xiUB9/nr5qFJZmViXjBoRwz88G\n8eXWw3y8oX21FLZnFTKsf0CzY4davUiKDDB+AoNLcJoiUErZgDuBpcAuYIFSaoeIPCIilzjrvoae\nj4hwod08VBvWWbuHoD2moTCrJ0G+Hrzz00E83Cy89otk/LzcW7zm9unxTE4I5aFFO9pc97i6RrHz\nSFG9/QNNMTUxnE0H8ykx6SYMXYxTfQRKqcVKqUFKqXil1GP2cw8qpRY10XeGWQ0Y2sqFI/tr85A9\n91BdHYJ2rAhEhMQIK+4W4aXrxxId0vq1bhbh2atH4+vpxp0fbKaiqrrVa9LzSimrrG7SUezI1MQw\nbDWKNftMuglD12J2Fhu6JXXmIXvuofS8MiL8vfD1bHlG35A/XTiMN24Yz6SBoW2+JiLAm6evHsXu\no8U8821qq/1bcxTXMm5AMD4ebsZPYOhyjCIwdEtqzUNr9mvz0MG8tiWba8jo6KC6DV3tYebgCK4c\nG8U7P6WT18qu4x2Hi/B0t9Q5p5vDy92NSQND+N74CQxdjFEEhm7LBbW5h7YfJT2vrF1moc7gjhnx\nnLDV8OYP6S32255VyNC+/nXJ81piSmI4+3NLXVJG09B7MYrA0G0Z3MefgeF+fLwxk5ziE8S2UKfY\nGSREWDl/eF/e/im92b0FSim2ZxUyvBWzUC3TEnUY6WpT1N7QhRhFYOi2iAgXjehXt9u3q1cEAL+a\nkUBxhY331hxqsj0zYwtFDwAAF+1JREFUv5yiClurjuJaEiKs9A3wNn4COy98t5c3Vh9wtRg9HqMI\nDN2aC0ae3AF8Kj6CjjIiKpCpiWG8vvpAkxFEOw6frFHcFkSEKYlh/JCW16VFc05HbNU1/HPlfl77\nfn+7N/AZ2odRBIZuTa15CCDGBSsC0KuC3JITfLwho1Hb9qwi3CzC4L7+bR5vQmwIheVV7Mvp+lKa\npxNbMwspOWHjSGEFGcdNim5nYhSBoVsjIvzyjFgmxIUQ4O3R+gVOYNLAEMbGBPHPlfupqq6p17b9\ncCGJEdY2Z0QFXbUMYOPB/E6Vc19OCY8v3kVZZffYsPajg59kjSnl6VSMIjB0e355ZiwLbjvDZfcX\nEX41I4GsgnK+3HqyaE6to7i1/QMNGRjmR7CvBxs6UREcyC1lzqtreHXVfj7Z2L70GK5idVouw/oF\nEOzrwdr9x10tTo/GKAKDoRM4a0gEQ/r689KKfXW2/WPFJ8gtqSSpjY7iWkSEcQOC2dRJiuBQXhnX\nvbaG6hrFwHA/3ltz8LS3uZdV2th8qICpiWFMjAtlrVkROBWjCAyGTsBiEe6YEU/asRK+3ZUNtH1H\ncVOMGxDC/tzSVjertUZmfhlzXltDeVU1790ykdunx5OaXcK6A6f3DHt9ej6V1TVMTghj4sAQMvPL\nycw3eyuchVEEBkMnceGIfsSE+PLS8jSUUuw4XIQIdQVu2sO4AR33ExwuKGfOa2sorqjivZsnMrRf\nABeP7E+gjwfvrjl4yuN2BT+m5eLpZmF8bAgT43T6D2Mech5GERgMnYS7m4Xbp8ezNbOQH/flsT2r\nkLgwv1YzmjbFyKhAPNyEjYdOTRFkF1Vw3WtrKCit4t2bJ9atSnw83bhqXBTfbD/KseLTtzrs6rRc\nxg4IwsfTjSF9/Qn08TDmISdiFIHB0IlcOS6SCH8vXlyexo7DRW3eP9AQbw83kiID2ZjefkVwrLiC\nOa+tIaf4BG/dNIFR0UH12q+fNABbjWLB+sbhrqcDx0sr2XmkiMnxepe1xSKMjw1h7WluzurOGEVg\nMHQiXu5u3Dp1ID/uyyOroJykyPabhWoZFxNMSlbh/7d359FRVPkCx7+/LCQQAklIoqxZWAQUDRAg\nCI4wLrigiE9HeaDihgvq6FNn0HlnFp3n6LiMczw6M7igjgoiozOoKCLixpIQFoOygyyBsCSERJZs\n5Pf+qGpsm4R0liZp+vc5x5PuquqqmyvpX9176/4u5VV1p7r2KDpQzvgXsynYX8a0Gwcf7WLylpYY\nwzk9E3krextVPo+7tgSLNxWhCsPcdBvgPKK7tegQu0pabismmFkgMKaJjRvSjfatnTkNDW0RAGSm\nxlNRVc23O0r9/swD73zD9uJDvDJxEIPTEmo9bkJWCjtLyvhs7Z4Gly9QFm4qJDYqgjO9BtmPjhME\nuHtIVVm0sZD73l7J2l3+13uwC2ggEJGLRGSdiGwUkSk17P8fEVktInkiMl9EUgJZHmNOhLZREdwy\nPI3oyLA6VyU7ngFHB4z96xIpPFDOF+v3cvPwNIZ2P/76Cuf1TqZj++gmGTTOy99P5h8/ZczzC5n6\n5aZGP92zcGMhQ9ITiPDK1tq3UztioyJYEqABY1Vl/prdXPm3Rfz3S9m8t2IHf/FjrYmTRf1Hsfwk\nIuHA88AFQD6wVERmq+pqr8NWAJmqekhE7gD+DFwTqDIZc6JMHtmDawZ1pX2bhs92To6NpltCG7+f\nHJr73S6qFS7t16nOYyPCwxg3uBvPzFvP94UHSWtg5tYV24q5/pUc2kVHoqo8Nmctj81ZS0bXOEaf\n2ZGL+3Wkc1xrv8+3fd8hthYdYuLZqT/ZHh4mDEpLIHtz07YIjlQrH31bwPMLNrGmoJTOca159Ioz\n2FJ4kGkLv2fH/sP1Kn+wCmSLYDCwUVU3q2oFMAMY432Aqi5QVc/twxKgSwDLY8wJExYmJLeLbvR5\nMlPiWba12K8JYHNWFZCWGEOfjv7lNbp2UFciwoS3shvWKsjdso/rXs4hIaYV79w+lNl3DefLB0fy\n64t6U1VdzR8/XMOwxz9j7AsL+SBvZ90nBBZtctJKDOuReMy+IWnO3Io9pY0fJ6g8Us3M3O1c8MwX\n3PXWCsqrjvDU1Wfx+YMjuC4rhZuGpwHwZgt/zLapBDIQdAa8H0vId7fV5mbgo5p2iMgkEckVkdy9\ney09rwkdA1LiKTxQwbY6FqopPFDO4k1FXNqvIyLi17mT20Uz6oxTmZmb79fay96yNxdx/Ss5JMdG\n8fakoXRy75q7dWjDHSO688Hd5/D5AyN4cNRplB6u5N4ZK9lSeLDO8y7cWERSbBQ9a1jNbUi6Z5yg\ncd1De0rLuHbqEn41K4/oyHBeGD+Aefedy1UDuxxdPKhzXGsu6HsKM5Zur3fdBKMWMVgsIhOATODJ\nmvar6lRVzVTVzKSk+i8raEywynQT0OXW8Ripp1vokn4dj3ucrwlDUig5XPmTHEl1WbSxkBum5dAp\nrjUzJmVxavuaWz6piTFMHtmD6ZOyiAwP46lP1h33vKrKok2FDOveocZgdkandsS0Cm/UgPGyrcWM\nfu5rVu8s5dlrMvjwnuFc0q8j4WHHXu+GoansO1jBB3kFDb5esAhkINgBdPV638Xd9hMicj7wG+By\nVW3cfHpjTjI9k2OJjYqoc2LZnFUFpNejW8gjKz2BHsltecPPLpAv1+/lxleXkpIQw/Rbs/zq/kqO\njebm4Wl8kFfAqvySWo9bt/sHCg9U1NgtBM64xsDUhAbPMH4rexvXTl1MdGQ4700+myv6dz5u62lo\n9w70SG7La4u2tPjcTI0VyECwFOgpImki0gq4FpjtfYCI9Af+gRMEWt5zbMY0s/AwoX9K/HEnlnm6\nhS6pR7eQh4hwXVYK3+SXkJe//7jHLli7h1tezyUtMYa3bh1CUmyU39eZdG468W0i+fPctbUe8/WG\n2scHPLLSE9iw5wCF9cjBVF51hIfezePh91YxtHsis+8aRu9T657f4aQ4T2HVjhJWbj9+3QS7gAUC\nVa0C7gLmAmuAmar6nYg8IiKXu4c9CbQF3hGRlSIyu5bTGROyMlPiWb/nB0oO17wu8tGnhc6sX7eQ\nx9gBnWnTKrzGVsGhiioWrNvDI++vZtI/c+l1Slum35pFh7b+BwGAdtGRTB7Zg682FB79wve1aFMR\n6YkxR8cbauKZT+Bv0rzd7njA9Jzt3DmiO9MmDiKuTSu/yz12QBfaRkXw+uKTe9A4YI+PAqjqHGCO\nz7bfer0+P5DXN+ZkMDAlHlXnUc0RpyUfs9/TLdS7HqugeWsXHcmYjM68uzyfhy7uw479h/lqQyFf\nbdhLrpsFtFVEGCNPS+bJq85q8COxE7JSmLZwC098vJazuw8jzKtfvvJINdmbixg74HjPkzg5mFpH\nhpO9uajO8ZDcLfu4483lHCyv4oXxA+o9fgLOnJCrBnbhrextPHxJn3q1gprantIykmKj6t3q80eL\nGCw2xtQuo2scYVJzJtLGdAt5m5DVjfKqaoY+Pp/Rz33NEx+vZd/BCm44O4XXbxpM3u8uZOr1mY2a\nFxEdGc59F/Ri1Y4S5nz70wHYb7bv52DFkaP5hWoTGR7GwJT4Op8cWlNQyoSXs4lpFc6/Jw9rUBDw\nuG5oChVHqnl76bYGn6OxSg5VMvq5r3n849q71hojoC0CY0zjxURF0KdjuxoDQWO7hTxO79SeG4el\nUnywgp/1SmJ4j8QmmQfha2z/zrz45WaemruOUaefevRxzYUbixChzhnR4MwneHreeooPVhAfc2w3\nT8nhSm5/YxntoiOZeftQkmMb93t0T2rLOT0TeWPJNm47t/vRMp9Ij364mqKDFVx2Zt2TBRvCWgTG\nBIHMlHhWbt9/TJK4D/Ma1y3k7XeXnc6z1/bnygFdAhIEwBn8fnDUaWwpOsTbXtlPF24s5IxO7f3q\nv/fMJ8jZcmyroLpauX/mSnYUH+aF8QMaHQQ8bhiayq7SMuat3l3nsUUHypt07sGCdXuYtSyf289N\nb9AiR/6wFoExQWBgagKvLd7KmoIf6NfF+TIoPFDOks1FTB7ZIyD9xoFyXp9kMlPi+ev8DVw5oLMz\n/rG9mJuHp/v1+bO6ticqIowlm4sYdfqpP9n3ty828emaPfz+sr5kptaedK++RvZOpkt8a15btKXW\nbqbSskoefX8177hrQp/SLoou8W3oGt/a+Zng/MzoGuf3GhWlZZU8/O4qeia35Z7zejbZ7+PLAoEx\nQWCgVwI6TyBo6CSy5iYiTLm4N1f9fTHTFm6hb6d2VB5RhvWou1sInFTfA7rFHzOf4Mv1e3nqk3WM\nyejEDT65ihorPMx5zPZPH61l7a7SYx4//WL9Xqb8K4/dpWXcNCyN9q0jyS8+RH7xYXK3FvN+XgFH\n3LWs0xNjmD4pi1P8aHX9ac4adpeW8bc7hxEVEd6kv5M3CwTGBIHOca3p2D6a3K3FTBzm5MH5MK+A\n9KSm6RY60TJTEzi/TzJ//3wTo844lVYRzrKU/hqSnsBf52+g5FAl7ds4X7q/nLGCXsmx/OnKfgFp\nIf0isyvPzFvP64u38tjYfgD8UFbJY3PWMD1nOz2S2/LencOOWQgIoOpINQUlZazaUcKD73zDuKlL\nmDHp+BPyvt5QyPSc7dz2s3QyajhnU7IxAmOCxICUeJa7A8aebqH65BZqaR4c1ZsDFVXMWpbPwG7x\nREf6f8c7JK0DqrB0yz7KKo9w55vLqTqi/P26gbRpFZj72/iYVozJ6MR7y3dQcriSrzcUctGzX/H2\n0u3cdm46H9w9vMYgAM6s6K4JbbikX0devWmwM7/hxSW1JtA7UF7Fr/+VR3piDPdd0Csgv483CwTG\nBInMlHh2lpSxc//hoO0W8nbaqbFc2d9JODy85/EfG/XVv1scrcLDyP6+iD+8v5q8/BKe/sVZDU6n\n7a/rh6ZyuPII419awoSXs4mKDGPWHWfz0MV9/A5kg1ITePWmwewqqT0YPPHRWnaWHObJq8+sV4Bs\nKAsExgSJzBSn6yR3a3FQdwt5u//CXgzr0YHR9Xz8NToynIyucUzP2c70nG3cOaI7F/oMHAfCGZ3b\nMyg1nu92lnLrOWnMueccBnQ7djnQugxKTeA1NxiM8wkGizcV8c8lW7nx7DQGpjTdgPfxWCAwJkj0\n7hhL68hwPvluV9B3C3l0imvNm7dkkdKh/nfyQ9ITOFBexfAeidx/4WkBKF3Nnh8/gLn3/ozfXNq3\nUXfrg1ITePXGwRR4gsEPZRyqcLqEUjq04cFRJ+53ssFiY4JEZHgYGV3jjqZFbuwksmA3tn9nthQd\n4veX9a0xjXSgJMdGN9n8hMFpTjCYOC2HcVOX0L9bPNv2HWLGpCxatwp8l5CHtQiMCSKex0jTk2I4\n7ZTg7hZqrPSktjw3rn+9E+C1NIPTEpg2cRAFJWXMWpbP9UNTyEr371HapmItAmOCyEB3oZqToVvI\n/GhIegdev2kws5bl8+uLep/w61sgMCaInN29A7cMT+O6rJTmLoppYpmpCU06G7o+LBAYE0SiIsL5\n39F9m7sY5iQT0DECEblIRNaJyEYRmVLD/igRedvdny0iqYEsjzHGmGMFLBCISDjwPHAx0BcYJyK+\ntzI3A8Wq2gP4C/BEoMpjjDGmZoFsEQwGNqrqZlWtAGYAY3yOGQO85r6eBZwnNgJmjDEnVCADQWdg\nu9f7fHdbjce4axyXAMc8NyUik0QkV0Ry9+7dG6DiGmNMaAqKeQSqOlVVM1U1MykpqbmLY4wxJ5VA\nBoIdQFev913cbTUeIyIRQHugKIBlMsYY4yOQgWAp0FNE0kSkFXAtMNvnmNnADe7rq4DPVFUDWCZj\njDE+AjaPQFWrROQuYC4QDryiqt+JyCNArqrOBl4G/ikiG4F9OMHCGGPMCSTBdgMuInuBrQ38eCJQ\n2ITFOZlZXfnH6sk/Vk/+CWQ9pahqjYOsQRcIGkNEclU1s7nLEQysrvxj9eQfqyf/NFc9BcVTQ8YY\nYwLHAoExxoS4UAsEU5u7AEHE6so/Vk/+sXryT7PUU0iNERhjjDlWqLUIjDHG+LBAYIwxIS5kAkFd\nayOEKhF5RUT2iMi3XtsSRGSeiGxwf8Y3ZxlbAhHpKiILRGS1iHwnIr90t1td+RCRaBHJEZFv3Lr6\ng7s9zV13ZKO7Dkmr5i5rSyAi4SKyQkQ+cN+f8HoKiUDg59oIoepV4CKfbVOA+araE5jvvg91VcD9\nqtoXyAImu/+GrK6OVQ78XFXPAjKAi0QkC2e9kb+4648U46xHYuCXwBqv9ye8nkIiEODf2gghSVW/\nxEnv4c17nYjXgCtOaKFaIFUtUNXl7usfcP5wO2N1dQx1HHDfRrr/KfBznHVHwOoKABHpAlwKvOS+\nF5qhnkIlEPizNoL50SmqWuC+3gWc0pyFaWncJVX7A9lYXdXI7e5YCewB5gGbgP3uuiNgf4MezwK/\nAqrd9x1ohnoKlUBgGsjNBmvPGLtEpC3wL+BeVS313md19SNVPaKqGTjp5wcDvZu5SC2OiIwG9qjq\nsuYuS8Cyj7Yw/qyNYH60W0Q6qmqBiHTEuasLeSISiRME3lTVd93NVlfHoar7RWQBMBSIE5EI927X\n/gZhGHC5iFwCRAPtgL/SDPUUKi0Cf9ZGMD/yXifiBuA/zViWFsHtu30ZWKOqz3jtsrryISJJIhLn\nvm4NXIAzprIAZ90RsLpCVR9S1S6qmorznfSZqo6nGeopZGYWu1H3WX5cG+H/mrlILYKITAdG4KS/\n3Q38Dvg3MBPohpPy+xeq6jugHFJEZDjwFbCKH/tzH8YZJ7C68iIiZ+IMcobj3GzOVNVHRCQd50GN\nBGAFMEFVy5uvpC2HiIwAHlDV0c1RTyETCIwxxtQsVLqGjDHG1MICgTHGhDgLBMYYE+IsEBhjTIiz\nQGCMMSHOAoFpUUREReRpr/cPiMjvm+jcr4rIVXUf2ejrXC0ia9yJVN7bO4nILPd1hvtIc1NdM05E\n7qzpWsbUxQKBaWnKgStFJLG5C+JNROozC/9m4FZVHem9UVV3qqonEGUA9QoEdZQhDjgaCHyuZcxx\nWSAwLU0Vzrqt9/nu8L2jF5ED7s8RIvKFiPxHRDaLyOMiMt7Nib9KRLp7neZ8EckVkfVurhdPgrQn\nRWSpiOSJyG1e5/1KRGYDq2sozzj3/N+KyBPutt8Cw4GXReRJn+NT3WNbAY8A14jIShG5RkRixFkb\nIsfNTT/G/cxEEZktIp8B80WkrYjMF5Hl7rU9WXQfB7q753vScy33HNEiMs09foWIjPQ697si8rE4\n6yn8ud7/t8xJIVRyDZng8jyQV88vprOAPjgptTcDL6nqYHEWkLkbuNc9LhUnCVp3YIGI9ACuB0pU\ndZCIRAELReQT9/gBwBmq+r33xUSkE07e+IE4OeM/EZEr3Bm0P8eZJZpbU0FVtcINGJmqepd7vsdw\nUgzc5KZnyBGRT73KcKaq7nNbBWNVtdRtNS1xA9UUt5wZ7vlSvS452bms9hOR3m5Ze7n7MnAyqZYD\n60TkOVX1ztRrQoC1CEyL42b1fB24px4fW+quGVCOk/LY80W+CufL32Omqlar6gacgNEbuBC43k2b\nnI2TCrine3yObxBwDQI+V9W9bnKwN4Gf1aO8vi4Eprhl+BwnCVk3d988r7QVAjwmInnApzgpiutK\nfT0ceANAVdfipMLwBIL5qlqiqmU4rZ6URvwOJkhZi8C0VM8Cy4FpXtuqcG9eRCQM8F7CzzsXS7XX\n+2p++u/cN6eK4ny53q2qc713uPlfDjas+PUmwH+p6jqfMgzxKcN4IAkYqKqVIrIFJ2g0lHe9HcG+\nE0KStQhMi+TeAc/kp8v0bcHpigG4HGflq/q6WkTC3HGDdGAdMBe4Q5w004hILxGJqeM8OcC5IpIo\nzlKo44Av6lGOH4BYr/dzgbvdLKeISP9aPtceJ4d9pdvX77mD9z2ft69wAghul1A3nN/bGMACgWnZ\nnsbJiurxIs6X7zc4+e0bcre+DedL/CPgdrdL5CWcbpHl7gDrP6jjzthdlWwKTsrgb4BlqlqfdMEL\ngL6ewWLgUZzAlici37nva/ImkCkiq3DGNta65SnCGdv41neQGngBCHM/8zYw0bJ+Gm+WfdQYY0Kc\ntQiMMSbEWSAwxpgQZ4HAGGNCnAUCY4wJcRYIjDEmxFkgMMaYEGeBwBhjQtz/A8qRA7x5olIYAAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean time per epoch 45.474914494809504 seconds\n",
            "Best accuracy: 74.72  Best training loss: 0.04415479674935341  Best validation loss: 0.8063691279292109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3773421c-5054-41b8-92e6-b92c304d36d2",
        "id": "LjnpLeVyyXS2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(str(iteration_list))\n",
        "print(str(loss_list))\n",
        "print(str(loss_list_test))\n",
        "print(str(accuracy_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]\n",
            "[1.6536296606063843, 1.4398102760314941, 1.064573049545288, 1.1902273893356323, 1.1529728174209595, 0.6907373070716858, 1.04155695438385, 1.1986956596374512, 0.8147517442703247, 0.6668054461479187, 0.7346854209899902, 0.5823341608047485, 0.6819490194320679, 0.5692170858383179, 0.4035908877849579, 0.47556304931640625, 0.29393941164016724, 0.7242833971977234, 0.3548170328140259, 0.4286886155605316, 0.34619057178497314, 0.21889831125736237, 0.5294685959815979, 0.3082839548587799, 0.04415479674935341, 0.22535362839698792, 0.2871718108654022, 0.14088304340839386, 0.15735384821891785, 0.2165956050157547, 0.19633236527442932, 0.08345698565244675, 0.3127736747264862, 0.06781768053770065, 0.145253986120224, 0.19343358278274536, 0.2224409282207489, 0.07933913916349411, 0.11991449445486069, 0.11226620525121689, 0.05171310529112816, 0.14326344430446625]\n",
            "[1.4487174665927887, 1.319182298183441, 1.1126837414503097, 1.1061175251007074, 1.010921832919121, 0.9006039363145828, 0.8648271256685255, 0.8572568905353544, 0.841548652648926, 0.8413213419914245, 0.8063691279292109, 0.8159981152415277, 0.8590839308500289, 0.8492450281977654, 0.8471501499414444, 0.8688637721538547, 0.8583878186345103, 0.8498844695091253, 0.835724220573902, 0.8714640641212457, 0.8961994382739069, 0.9010201692581178, 0.9421867689490314, 0.9299696439504627, 0.9491137641668321, 0.9669340848922728, 0.9461888754367828, 0.9531012353301047, 0.9480566823482517, 1.0007599699497216, 1.0026775765419003, 1.014471134543419, 1.0238916563987732, 1.040102934837341, 1.0363413953781129, 1.0405507487058636, 1.0854925903677943, 1.1328451561927795, 1.1101237395405767, 1.0815439224243164, 1.126463493704796, 1.1399510735273364]\n",
            "[49.54, 54.26, 61.1, 61.96, 65.74, 69.08, 70.52, 71.14, 71.16, 71.84, 72.78, 72.9, 72.04, 72.62, 72.98, 72.82, 73.32, 73.78, 73.44, 73.24, 73.38, 73.0, 72.96, 73.04, 72.76, 73.92, 74.08, 73.84, 73.8, 73.76, 74.5, 74.46, 74.28, 74.72, 74.22, 74.48, 74.06, 73.82, 74.08, 74.3, 74.46, 73.66]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}